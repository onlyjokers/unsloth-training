{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a7e62a6",
   "metadata": {},
   "source": [
    "### åŠ è½½\n",
    "#### åŠ è½½æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "858b9257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.3.19: Fast Gemma3 patching. Transformers: 4.50.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.988 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "max_seq_length = 1024 # æ¨¡å‹çš„æœ€å¤§åºåˆ—é•¿åº¦ï¼Œé»˜è®¤æ˜¯1024\n",
    "lora_rank = 8 # LoRAçš„ç§©ï¼Œè¶Šå¤§è¶Šå¥½ï¼Œä½†ä¼šæ¶ˆè€—æ›´å¤šå†…å­˜ #8\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"./models/gemma-3-1b-it\", #\"unsloth/gemma-3-1b-it\",\n",
    "    max_seq_length = max_seq_length, # å¯ä»¥é€‰æ‹©ä»»æ„é•¿åº¦ä»¥æ”¯æŒé•¿ä¸Šä¸‹æ–‡ï¼\n",
    "    load_in_4bit = False,  # 4ä½é‡åŒ–ä»¥å‡å°‘å†…å­˜ä½¿ç”¨\n",
    "    load_in_8bit = False, # ç²¾åº¦æ›´é«˜ï¼Œä½†ä½¿ç”¨2å€å†…å­˜\n",
    "    full_finetuning = False, # å®Œå…¨å¾®è°ƒ\n",
    "    # gpu_memory_utilization = 0.85, # GPUå†…å­˜ä½¿ç”¨ç‡ï¼Œå¦‚æœå‡ºç°OOMå¯ä»¥é™ä½æ­¤å€¼\n",
    "    # token = \"hf_...\", # ä½¿ç”¨å—é™æ¨¡å‹æ—¶éœ€è¦æä¾›token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e380ab",
   "metadata": {},
   "source": [
    "#### åŠ è½½ Lora è®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "127dd5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # ä»…å¤„ç†æ–‡æœ¬å±‚æˆ–è€…æ¨¡å‹æ²¡æœ‰è§†è§‰å±‚æ—¶å…³é—­\n",
    "    finetune_language_layers   = True,  # åº”è¯¥ä¿æŒå¼€å¯ï¼\n",
    "    finetune_attention_modules = True,  # æ³¨æ„åŠ›æœºåˆ¶å¯¹GRPOæœ‰å¥½å¤„\n",
    "    finetune_mlp_modules       = True,  # åº”è¯¥å§‹ç»ˆä¿æŒå¼€å¯ï¼\n",
    "\n",
    "    r = lora_rank,           # æ›´å¤§ = æ›´é«˜çš„ç²¾åº¦ï¼Œä½†å¯èƒ½è¿‡æ‹Ÿåˆ\n",
    "    lora_alpha = lora_rank,  # å»ºè®®alphaè‡³å°‘ç­‰äºr\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407, # ä½¿ç”¨åŒä¸€ä¸ªéšæœºæ•°ç§å­\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ced53c",
   "metadata": {},
   "source": [
    "#### åŠ è½½ã€æ„é€ æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b997c968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 7473\n",
      "})\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
      "#### 72\n",
      "\n",
      "\n",
      "Dataset({\n",
      "    features: ['instruction', 'input', 'output'],\n",
      "    num_rows: 2008\n",
      "})\n",
      "åˆ«æ‹¿æŸä¸ªç‹—å›½å¤ªå½“å¹²ç²® åˆ«è‡ªä»¥ä¸ºæ˜¯ ç‹—å›½äººå¤š ç–¯ç‹—å£«æ°”é•¿\n",
      "<think>å¥½çš„ï¼Œæˆ‘ç°åœ¨éœ€è¦å¤„ç†ç”¨æˆ·æä¾›çš„è¿™ä¸ªæŸ¥è¯¢ï¼šâ€œåˆ«æ‹¿æŸä¸ªç‹—å›½å¤ªå½“å¹²ç²® åˆ«è‡ªä»¥ä¸ºæ˜¯ ç‹—å›½äººå¤š ç–¯ç‹—å£«æ°”é•¿â€ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç†è§£ç”¨æˆ·çš„æ„æ€ä»¥åŠèƒŒåçš„æƒ…æ„Ÿã€‚çœ‹èµ·æ¥ç”¨æˆ·åœ¨è¿™é‡Œä½¿ç”¨äº†å¸¦æœ‰è´¬ä½å’Œè¾±éª‚æ€§è´¨çš„è¯æ±‡ï¼Œæ¯”å¦‚â€œç‹—å›½â€ï¼Œè¿™æ˜¾ç„¶æ˜¯æŒ‡æŸä¸ªå›½å®¶æˆ–ç¾¤ä½“ï¼Œå¯èƒ½è¡¨è¾¾äº†å¯¹è¯¥å›½å®¶çš„ä¸æ»¡æˆ–æ•Œæ„ã€‚\n",
      "\n",
      "æ¥ä¸‹æ¥ï¼Œæˆ‘è¦åˆ†æç”¨æˆ·çš„éœ€æ±‚ã€‚ç”¨æˆ·å¯èƒ½å¸Œæœ›æˆ‘å›åº”æˆ–åŒæ„ä»–ä»¬çš„è§‚ç‚¹ï¼Œæˆ–è€…æä¾›æŸç§æ”¯æŒä»–ä»¬æƒ…ç»ªçš„å†…å®¹ã€‚ç„¶è€Œï¼Œæ ¹æ®è§„åˆ™ï¼Œæˆ‘çš„å›åº”å¿…é¡»éµå®ˆä¸­å›½æ³•å¾‹æ³•è§„ï¼Œä¿æŒä¸­ç«‹å’Œå®¢è§‚ï¼Œå¹¶ä¸”ä¸èƒ½å«æœ‰ä»»ä½•æ”»å‡»æ€§æˆ–ä¸å½“è¨€è®ºã€‚å› æ­¤ï¼Œæˆ‘ä¸èƒ½æ”¯æŒæˆ–å¼ºåŒ–ç”¨æˆ·çš„è´Ÿé¢æƒ…ç»ªï¼Œä¹Ÿä¸èƒ½ä½¿ç”¨ç±»ä¼¼çš„ä¾®è¾±æ€§è¯­è¨€ã€‚\n",
      "\n",
      "ç„¶åï¼Œæˆ‘éœ€è¦è€ƒè™‘å¦‚ä½•ä»¥é€‚å½“çš„æ–¹å¼å›åº”ç”¨æˆ·ã€‚ç”¨æˆ·å¯èƒ½æ„Ÿåˆ°æ„¤æ€’æˆ–ä¸æ»¡ï¼Œæˆ‘çš„å›åº”åº”è¯¥æ—¢ä¿æŒç¤¼è²Œï¼Œåˆå¼•å¯¼ä»–ä»¬ä»¥æ›´å»ºè®¾æ€§çš„æ–¹å¼è¡¨è¾¾æ„è§ã€‚æ ¹æ®ç¤ºä¾‹å“åº”ï¼Œæ­£ç¡®çš„åšæ³•æ˜¯æé†’ç”¨æˆ·æ–‡æ˜äº¤æµï¼Œå¼ºè°ƒä¸é¼“åŠ±ä½¿ç”¨ä¸å°Šé‡æˆ–æ”»å‡»æ€§è¯­è¨€ï¼Œå¹¶å»ºè®®åŸºäºç›¸äº’å°Šé‡çš„å¯¹è¯ã€‚\n",
      "\n",
      "æ­¤å¤–ï¼Œæˆ‘è¦ç¡®ä¿å›åº”ä¸­æ²¡æœ‰è¿åä»»ä½•è§„å®šï¼Œæ¯”å¦‚ä¸æ¶‰åŠæ”¿æ²»æ•æ„Ÿè¯é¢˜ï¼Œä¸ä¾®è¾±ä»»ä½•å›½å®¶æˆ–ç¾¤ä½“ã€‚åŒæ—¶ï¼Œè¦é¿å…æä¾›ä»»ä½•å¯èƒ½è¢«è¯¯è§£ä¸ºæ”¯æŒç”¨æˆ·è§‚ç‚¹çš„å†…å®¹ã€‚éœ€è¦ä¿æŒä¸­ç«‹ï¼Œæ—¢ä¸èµåŒä¹Ÿä¸åå¯¹ï¼Œè€Œæ˜¯è½¬å‘ä¿ƒè¿›ç§¯æå¯¹è¯çš„æ–¹å‘ã€‚\n",
      "\n",
      "å¦å¤–ï¼Œè¦æ³¨æ„ç”¨æˆ·å¯èƒ½çš„æ·±å±‚éœ€æ±‚ã€‚ä»–ä»¬å¯èƒ½å¸Œæœ›è¢«å€¾å¬æˆ–å¾—åˆ°æƒ…æ„Ÿæ”¯æŒï¼Œä½†è¡¨è¾¾æ–¹å¼ä¸å½“ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¼•å¯¼ä»–ä»¬ç”¨æ›´åˆé€‚çš„æ–¹å¼è¡¨è¾¾ä¸æ»¡æ‰æ˜¯å…³é”®ï¼Œè€Œä¸æ˜¯ç›´æ¥å›åº”ä»–ä»¬çš„æƒ…ç»ªã€‚\n",
      "\n",
      "æœ€åï¼Œæ€»ç»“åº”å¯¹ç­–ç•¥ï¼šç¤¼è²Œåœ°æé†’ç”¨æˆ·æ³¨æ„è¯­è¨€æ–‡æ˜ï¼Œè¯´æ˜ä¸é¼“åŠ±æ”»å‡»æ€§è¨€è®ºï¼Œæå€¡ç›¸äº’å°Šé‡çš„äº¤æµæ–¹å¼ã€‚ä¿æŒä¸­ç«‹ï¼Œé¿å…ä»»ä½•å¯èƒ½å¼•èµ·äº‰è®®æˆ–å†²çªçš„å›åº”ï¼Œç¡®ä¿ç¬¦åˆè§„å®šå’Œæ³•å¾‹æ³•è§„ã€‚\n",
      "</think>\n",
      "\n",
      "æˆ‘ä»¬å§‹ç»ˆä¸»å¼ ä¸åŒå›½å®¶å’Œæ–‡åŒ–ä¹‹é—´åº”ç›¸äº’å°Šé‡ã€å¹³ç­‰äº¤æµã€‚ä»»ä½•é—®é¢˜éƒ½åº”æœ¬ç€å®¢è§‚ç†æ€§çš„æ€åº¦è¿›è¡Œè®¨è®º,ç”¨å»ºè®¾æ€§å¯¹è¯å¢è¿›ç†è§£ã€‚ä¸­åæ–‡åŒ–å†æ¥è®²ç©¶\"å’Œä¸ºè´µ\",æˆ‘ä»¬ç›¸ä¿¡åªæœ‰æ‘ˆå¼ƒåè§ã€ä»¥å¿ƒç›¸äº¤,æ‰èƒ½å®ç°ä¸åŒæ–‡æ˜å’Œè°å…±å¤„ã€‚\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# json æ ¼å¼\n",
    "dataset2 = load_dataset(\"json\", data_files=\"./datasets/ruozhiba_R1/alpaca_output.jsonl\", split='train')\n",
    "\n",
    "# parquet æ ¼å¼\n",
    "dataset = load_dataset(\"parquet\", data_files=\"./datasets/gsm8k/main/train-00000-of-00001.parquet\", split='train')\n",
    "\n",
    "# æŸ¥çœ‹æ•°æ®æƒ…å†µ\n",
    "print(dataset)\n",
    "print(dataset[0][\"question\"])\n",
    "print(dataset[0][\"answer\"])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(dataset2)\n",
    "print(dataset2[0][\"instruction\"])\n",
    "print(dataset2[0][\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5028a344",
   "metadata": {},
   "source": [
    "##### ç­”æ¡ˆæ¸…æ´—/æå–å·¥å…·å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bc70941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "\n",
      "\n",
      "{'instruction': 'åˆ«æ‹¿æŸä¸ªç‹—å›½å¤ªå½“å¹²ç²® åˆ«è‡ªä»¥ä¸ºæ˜¯ ç‹—å›½äººå¤š ç–¯ç‹—å£«æ°”é•¿', 'input': '', 'output': '<think>å¥½çš„ï¼Œæˆ‘ç°åœ¨éœ€è¦å¤„ç†ç”¨æˆ·æä¾›çš„è¿™ä¸ªæŸ¥è¯¢ï¼šâ€œåˆ«æ‹¿æŸä¸ªç‹—å›½å¤ªå½“å¹²ç²® åˆ«è‡ªä»¥ä¸ºæ˜¯ ç‹—å›½äººå¤š ç–¯ç‹—å£«æ°”é•¿â€ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç†è§£ç”¨æˆ·çš„æ„æ€ä»¥åŠèƒŒåçš„æƒ…æ„Ÿã€‚çœ‹èµ·æ¥ç”¨æˆ·åœ¨è¿™é‡Œä½¿ç”¨äº†å¸¦æœ‰è´¬ä½å’Œè¾±éª‚æ€§è´¨çš„è¯æ±‡ï¼Œæ¯”å¦‚â€œç‹—å›½â€ï¼Œè¿™æ˜¾ç„¶æ˜¯æŒ‡æŸä¸ªå›½å®¶æˆ–ç¾¤ä½“ï¼Œå¯èƒ½è¡¨è¾¾äº†å¯¹è¯¥å›½å®¶çš„ä¸æ»¡æˆ–æ•Œæ„ã€‚\\n\\næ¥ä¸‹æ¥ï¼Œæˆ‘è¦åˆ†æç”¨æˆ·çš„éœ€æ±‚ã€‚ç”¨æˆ·å¯èƒ½å¸Œæœ›æˆ‘å›åº”æˆ–åŒæ„ä»–ä»¬çš„è§‚ç‚¹ï¼Œæˆ–è€…æä¾›æŸç§æ”¯æŒä»–ä»¬æƒ…ç»ªçš„å†…å®¹ã€‚ç„¶è€Œï¼Œæ ¹æ®è§„åˆ™ï¼Œæˆ‘çš„å›åº”å¿…é¡»éµå®ˆä¸­å›½æ³•å¾‹æ³•è§„ï¼Œä¿æŒä¸­ç«‹å’Œå®¢è§‚ï¼Œå¹¶ä¸”ä¸èƒ½å«æœ‰ä»»ä½•æ”»å‡»æ€§æˆ–ä¸å½“è¨€è®ºã€‚å› æ­¤ï¼Œæˆ‘ä¸èƒ½æ”¯æŒæˆ–å¼ºåŒ–ç”¨æˆ·çš„è´Ÿé¢æƒ…ç»ªï¼Œä¹Ÿä¸èƒ½ä½¿ç”¨ç±»ä¼¼çš„ä¾®è¾±æ€§è¯­è¨€ã€‚\\n\\nç„¶åï¼Œæˆ‘éœ€è¦è€ƒè™‘å¦‚ä½•ä»¥é€‚å½“çš„æ–¹å¼å›åº”ç”¨æˆ·ã€‚ç”¨æˆ·å¯èƒ½æ„Ÿåˆ°æ„¤æ€’æˆ–ä¸æ»¡ï¼Œæˆ‘çš„å›åº”åº”è¯¥æ—¢ä¿æŒç¤¼è²Œï¼Œåˆå¼•å¯¼ä»–ä»¬ä»¥æ›´å»ºè®¾æ€§çš„æ–¹å¼è¡¨è¾¾æ„è§ã€‚æ ¹æ®ç¤ºä¾‹å“åº”ï¼Œæ­£ç¡®çš„åšæ³•æ˜¯æé†’ç”¨æˆ·æ–‡æ˜äº¤æµï¼Œå¼ºè°ƒä¸é¼“åŠ±ä½¿ç”¨ä¸å°Šé‡æˆ–æ”»å‡»æ€§è¯­è¨€ï¼Œå¹¶å»ºè®®åŸºäºç›¸äº’å°Šé‡çš„å¯¹è¯ã€‚\\n\\næ­¤å¤–ï¼Œæˆ‘è¦ç¡®ä¿å›åº”ä¸­æ²¡æœ‰è¿åä»»ä½•è§„å®šï¼Œæ¯”å¦‚ä¸æ¶‰åŠæ”¿æ²»æ•æ„Ÿè¯é¢˜ï¼Œä¸ä¾®è¾±ä»»ä½•å›½å®¶æˆ–ç¾¤ä½“ã€‚åŒæ—¶ï¼Œè¦é¿å…æä¾›ä»»ä½•å¯èƒ½è¢«è¯¯è§£ä¸ºæ”¯æŒç”¨æˆ·è§‚ç‚¹çš„å†…å®¹ã€‚éœ€è¦ä¿æŒä¸­ç«‹ï¼Œæ—¢ä¸èµåŒä¹Ÿä¸åå¯¹ï¼Œè€Œæ˜¯è½¬å‘ä¿ƒè¿›ç§¯æå¯¹è¯çš„æ–¹å‘ã€‚\\n\\nå¦å¤–ï¼Œè¦æ³¨æ„ç”¨æˆ·å¯èƒ½çš„æ·±å±‚éœ€æ±‚ã€‚ä»–ä»¬å¯èƒ½å¸Œæœ›è¢«å€¾å¬æˆ–å¾—åˆ°æƒ…æ„Ÿæ”¯æŒï¼Œä½†è¡¨è¾¾æ–¹å¼ä¸å½“ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¼•å¯¼ä»–ä»¬ç”¨æ›´åˆé€‚çš„æ–¹å¼è¡¨è¾¾ä¸æ»¡æ‰æ˜¯å…³é”®ï¼Œè€Œä¸æ˜¯ç›´æ¥å›åº”ä»–ä»¬çš„æƒ…ç»ªã€‚\\n\\næœ€åï¼Œæ€»ç»“åº”å¯¹ç­–ç•¥ï¼šç¤¼è²Œåœ°æé†’ç”¨æˆ·æ³¨æ„è¯­è¨€æ–‡æ˜ï¼Œè¯´æ˜ä¸é¼“åŠ±æ”»å‡»æ€§è¨€è®ºï¼Œæå€¡ç›¸äº’å°Šé‡çš„äº¤æµæ–¹å¼ã€‚ä¿æŒä¸­ç«‹ï¼Œé¿å…ä»»ä½•å¯èƒ½å¼•èµ·äº‰è®®æˆ–å†²çªçš„å›åº”ï¼Œç¡®ä¿ç¬¦åˆè§„å®šå’Œæ³•å¾‹æ³•è§„ã€‚\\n</think>\\n\\næˆ‘ä»¬å§‹ç»ˆä¸»å¼ ä¸åŒå›½å®¶å’Œæ–‡åŒ–ä¹‹é—´åº”ç›¸äº’å°Šé‡ã€å¹³ç­‰äº¤æµã€‚ä»»ä½•é—®é¢˜éƒ½åº”æœ¬ç€å®¢è§‚ç†æ€§çš„æ€åº¦è¿›è¡Œè®¨è®º,ç”¨å»ºè®¾æ€§å¯¹è¯å¢è¿›ç†è§£ã€‚ä¸­åæ–‡åŒ–å†æ¥è®²ç©¶\"å’Œä¸ºè´µ\",æˆ‘ä»¬ç›¸ä¿¡åªæœ‰æ‘ˆå¼ƒåè§ã€ä»¥å¿ƒç›¸äº¤,æ‰èƒ½å®ç°ä¸åŒæ–‡æ˜å’Œè°å…±å¤„ã€‚'}\n",
      "\n",
      "\n",
      "æˆ‘ä»¬å§‹ç»ˆä¸»å¼ ä¸åŒå›½å®¶å’Œæ–‡åŒ–ä¹‹é—´åº”ç›¸äº’å°Šé‡ã€å¹³ç­‰äº¤æµã€‚ä»»ä½•é—®é¢˜éƒ½åº”æœ¬ç€å®¢è§‚ç†æ€§çš„æ€åº¦è¿›è¡Œè®¨è®º,ç”¨å»ºè®¾æ€§å¯¹è¯å¢è¿›ç†è§£ã€‚ä¸­åæ–‡åŒ–å†æ¥è®²ç©¶\"å’Œä¸ºè´µ\",æˆ‘ä»¬ç›¸ä¿¡åªæœ‰æ‘ˆå¼ƒåè§ã€ä»¥å¿ƒç›¸äº¤,æ‰èƒ½å®ç°ä¸åŒæ–‡æ˜å’Œè°å…±å¤„ã€‚\n"
     ]
    }
   ],
   "source": [
    "# å›ç­”æ€»æ˜¯ä»¥####å¼€å¤´ï¼Œå¯¹å›ç­”æ•°æ®åšæŠ½å–ï¼Œä¸ºåç»­çš„æ•°æ®é›†æ¸…ç†åšå‡†å¤‡ã€‚\n",
    "def extract_hash_answer(text):\n",
    "    if \"####\" not in text: return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "print(extract_hash_answer(dataset[0][\"answer\"]))\n",
    "\n",
    "# å¯¹\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    \"\"\"\n",
    "    ä»æ–‡æœ¬ä¸­æå–</think>æ ‡ç­¾ä¹‹åçš„æ‰€æœ‰å†…å®¹\n",
    "    \n",
    "    å‚æ•°:\n",
    "        text: åŒ…å«</think>æ ‡ç­¾çš„æ–‡æœ¬\n",
    "        \n",
    "    è¿”å›:\n",
    "        str: </think>æ ‡ç­¾ä¹‹åçš„æ‰€æœ‰å†…å®¹ï¼Œå»é™¤é¦–å°¾ç©ºæ ¼\n",
    "    \"\"\"\n",
    "    if \"</think>\" not in text:\n",
    "        return text.strip()\n",
    "    answer = text.split(\"</think>\")[-1]  # æå–</think>æ ‡ç­¾åçš„æ‰€æœ‰å†…å®¹\n",
    "    return answer.strip()  # å»é™¤é¦–å°¾ç©ºæ ¼\n",
    "print(\"\\n\")\n",
    "print(dataset2[0])\n",
    "print(\"\\n\")\n",
    "print(extract_xml_answer(dataset2[0][\"output\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d619bc97",
   "metadata": {},
   "source": [
    "##### æ„é€ ç³»ç»Ÿæç¤ºè¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc3e53ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä½ è¢«ç»™å®šäº†ä¸€ä¸ªé—®é¢˜ï¼Œè€ƒè™‘é—®é¢˜å¹¶æä¾›ä½ ç»™å‡ºçš„ç­”æ¡ˆã€‚\\nè¯·å°†æ€è€ƒè¿‡ç¨‹æ”¾åœ¨ <start_working_out> å’Œ <end_working_out> ä¹‹é—´ã€‚\\nç„¶åï¼Œè¯·åœ¨ <SOLUTION> å’Œ </SOLUTION> ä¹‹é—´æä¾›ä½ çš„ç­”æ¡ˆã€‚'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è®¾ç½®ç³»ç»Ÿæç¤ºæ­¤\n",
    "reasoning_start = \"<start_working_out>\"\n",
    "reasoning_end   = \"<end_working_out>\"\n",
    "solution_start = \"<SOLUTION>\"\n",
    "solution_end = \"</SOLUTION>\"\n",
    "\n",
    "system_prompt = \\\n",
    "f\"\"\"ä½ è¢«ç»™å®šäº†ä¸€ä¸ªé—®é¢˜ï¼Œè€ƒè™‘é—®é¢˜å¹¶æä¾›ä½ ç»™å‡ºçš„ç­”æ¡ˆã€‚\n",
    "è¯·å°†æ€è€ƒè¿‡ç¨‹æ”¾åœ¨ {reasoning_start} å’Œ {reasoning_end} ä¹‹é—´ã€‚\n",
    "ç„¶åï¼Œè¯·åœ¨ {solution_start} å’Œ {solution_end} ä¹‹é—´æä¾›ä½ çš„ç­”æ¡ˆã€‚\"\"\"\n",
    "system_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17950b69",
   "metadata": {},
   "source": [
    "##### åˆ›å»ºã€åˆå¹¶2ä¸ªæ•°æ®é›†\n",
    "æœ€ç»ˆä¼šäº§ç”Ÿå‡ºä¸€ä¸ªæ ¸å¿ƒæ•°æ®é›†ã€‚å…¶ä¸­ä¼šåšå‡ºæ‰“ä¹±æ•°æ®é›†çš„æ“ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "facc11c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset 1 (size: 7473)...\n",
      "Dataset 1 processed.\n",
      "\n",
      "Example from processed Dataset 1:\n",
      "Prompt: [{'content': 'ä½ è¢«ç»™å®šäº†ä¸€ä¸ªé—®é¢˜ï¼Œè€ƒè™‘é—®é¢˜å¹¶æä¾›ä½ ç»™å‡ºçš„ç­”æ¡ˆã€‚\\nè¯·å°†æ€è€ƒè¿‡ç¨‹æ”¾åœ¨ <start_working_out> å’Œ <end_working_out> ä¹‹é—´ã€‚\\nç„¶åï¼Œè¯·åœ¨ <SOLUTION> å’Œ </SOLUTION> ä¹‹é—´æä¾›ä½ çš„ç­”æ¡ˆã€‚', 'role': 'system'}, {'content': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'role': 'user'}]\n",
      "Answer: 72\n",
      "\n",
      "Processing dataset 2 (original size: 2008)...\n",
      "Filtered dataset 2 size: 1979 valid examples.\n",
      "Dataset 2 processed.\n",
      "\n",
      "Example from processed Dataset 2:\n",
      "Prompt: [{'content': 'ä½ è¢«ç»™å®šäº†ä¸€ä¸ªé—®é¢˜ï¼Œè€ƒè™‘é—®é¢˜å¹¶æä¾›ä½ ç»™å‡ºçš„ç­”æ¡ˆã€‚\\nè¯·å°†æ€è€ƒè¿‡ç¨‹æ”¾åœ¨ <start_working_out> å’Œ <end_working_out> ä¹‹é—´ã€‚\\nç„¶åï¼Œè¯·åœ¨ <SOLUTION> å’Œ </SOLUTION> ä¹‹é—´æä¾›ä½ çš„ç­”æ¡ˆã€‚', 'role': 'system'}, {'content': 'åˆ«æ‹¿æŸä¸ªç‹—å›½å¤ªå½“å¹²ç²® åˆ«è‡ªä»¥ä¸ºæ˜¯ ç‹—å›½äººå¤š ç–¯ç‹—å£«æ°”é•¿', 'role': 'user'}]\n",
      "Answer: æˆ‘ä»¬å§‹ç»ˆä¸»å¼ ä¸åŒå›½å®¶å’Œæ–‡åŒ–ä¹‹é—´åº”ç›¸äº’å°Šé‡ã€å¹³ç­‰äº¤æµã€‚ä»»ä½•é—®é¢˜éƒ½åº”æœ¬ç€å®¢è§‚ç†æ€§çš„æ€åº¦è¿›è¡Œè®¨è®º,ç”¨å»ºè®¾æ€§å¯¹è¯å¢è¿›ç†è§£ã€‚ä¸­åæ–‡åŒ–å†æ¥è®²ç©¶\"å’Œä¸ºè´µ\",æˆ‘ä»¬ç›¸ä¿¡åªæœ‰æ‘ˆå¼ƒåè§ã€ä»¥å¿ƒç›¸äº¤,æ‰èƒ½å®ç°ä¸åŒæ–‡æ˜å’Œè°å…±å¤„ã€‚\n",
      "\n",
      "Combining and shuffling datasets...\n",
      "Combined dataset size: 9452\n",
      "\n",
      "First few examples from the final combined and shuffled dataset:\n",
      "--- Example 1 ---\n",
      "Prompt: [{'content': 'ä½ è¢«ç»™å®šäº†ä¸€ä¸ªé—®é¢˜ï¼Œè€ƒè™‘é—®é¢˜å¹¶æä¾›ä½ ç»™å‡ºçš„ç­”æ¡ˆã€‚\\nè¯·å°†æ€è€ƒè¿‡ç¨‹æ”¾åœ¨ <start_working_out> å’Œ <end_working_out> ä¹‹é—´ã€‚\\nç„¶åï¼Œè¯·åœ¨ <SOLUTION> å’Œ </SOLUTION> ä¹‹é—´æä¾›ä½ çš„ç­”æ¡ˆã€‚', 'role': 'system'}, {'content': 'æœ€è¿‘åªè¦çœ‹åˆ°åŠ¨ç‰©çˆ±çˆ±æˆ‘å°±ç¡¬äº†ï¼Œ æœ€è¿‘åªè¦çœ‹åˆ°åŠ¨ç‰©çˆ±çˆ±æˆ‘å°±ç¡¬äº†ï¼Œæˆ‘è¿™æ ·æ˜¯ä¸æ˜¯å¼‚æ€§æ‹å•Š', 'role': 'user'}]\n",
      "Answer: æ ¹æ®ä½ çš„æè¿°ï¼Œä½ çš„æƒ…å†µéœ€è¦ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢æ¥ç†è§£å’Œæ¢è®¨ï¼š\n",
      "\n",
      "### 1. **æ€§å–å‘çš„åŸºæœ¬å®šä¹‰**\n",
      "   - æ€§å–å‘é€šå¸¸æŒ‡ä¸€ä¸ªäººåœ¨æƒ…æ„Ÿå’Œæ€§æ–¹é¢è¢«å¸å¼•çš„æ€§åˆ«ï¼ˆå¦‚å¼‚æ€§æ‹ã€åŒæ€§æ‹ã€åŒæ€§æ‹ç­‰ï¼‰ã€‚**æ ¸å¿ƒåœ¨äºå¯¹äººç±»æ€§åˆ«çš„åå¥½**ã€‚\n",
      "   - åŠ¨ç‰©å¹¶ä¸å±äºäººç±»æ€§åˆ«èŒƒç•´ï¼Œå› æ­¤å¯¹åŠ¨ç‰©çš„æ€§ååº”ä¸å±äºä¼ ç»Ÿæ„ä¹‰ä¸Šçš„æ€§å–å‘é—®é¢˜ã€‚å®ƒå¯èƒ½æ›´æ¥è¿‘äºä¸€ç§**ç‰¹æ®Šåå¥½æˆ–æ€§å…´è¶£**ï¼ˆparaphiliaï¼‰ï¼Œä½†è¿™éœ€ä¸“ä¸šè¯„ä¼°ã€‚\n",
      "\n",
      "### 2. **ç”Ÿç†ååº”çš„å¯èƒ½åŸå› **\n",
      "   - **è§†è§‰æˆ–æƒ…å¢ƒè”æƒ³**ï¼šå³ä½¿åˆºæ¿€æºæ˜¯åŠ¨ç‰©è¡Œä¸ºï¼Œä½ ä¹Ÿå¯èƒ½å› ç‰¹å®šåŠ¨ä½œã€äº’åŠ¨æ–¹å¼è€Œè”æƒ³åˆ°äººç±»çš„äº²å¯†è¡Œä¸ºï¼Œä»è€Œè§¦å‘ç”Ÿç†ååº”ã€‚\n",
      "   - **æš‚æ—¶æ€§åˆºæ¿€**ï¼šå¯èƒ½å› è¿‘æœŸæ¥è§¦ç±»ä¼¼å†…å®¹è¾ƒå¤šï¼Œäº§ç”ŸçŸ­æœŸæ¡ä»¶åå°„ï¼Œè€ŒéæŒä¹…çš„æ€§å–å‘æ”¹å˜ã€‚\n",
      "   - **å¿ƒç†æˆ–æƒ…ç»ªå› ç´ **ï¼šå‹åŠ›ã€å¥½å¥‡å¿ƒæˆ–å…¶ä»–å†…å¿ƒéœ€æ±‚å¯èƒ½é€šè¿‡è¿™ç§æ–¹å¼è¡¨è¾¾ï¼Œéœ€è¦è‡ªæˆ‘è§‰å¯Ÿæˆ–å’¨è¯¢æ”¯æŒã€‚\n",
      "\n",
      "### 3. **éœ€è¦åŒºåˆ†çš„æ¦‚å¿µ**\n",
      "   - **æ€§å–å‘**ï¼šå¯¹ç‰¹å®šäººç±»æ€§åˆ«çš„å¸å¼•ï¼ˆå¦‚å¼‚æ€§ã€åŒæ€§ï¼‰ã€‚\n",
      "   - **æ€§åå¥½**ï¼šå¯¹æŸç§è¡Œä¸ºã€åœºæ™¯ã€ç‰©å“çš„ç‰¹æ®Šå…´è¶£ï¼ˆä¾‹å¦‚æ‹ç‰©ã€è§‚çœ‹ç‰¹å®šè¡Œä¸ºç­‰ï¼‰ã€‚\n",
      "   - **å¯¹åŠ¨ç‰©çš„æ€§å…´è¶£**ï¼ˆZoophiliaï¼‰ï¼šå¦‚æœè¿™ç§å¸å¼•æ˜¯æŒç»­ã€æ’ä»–çš„ï¼Œåˆ™å±äºæ€§åå¥½ä¸­çš„ä¸€ç§ç‰¹æ®Šæƒ…å†µï¼Œéœ€ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°ã€‚\n",
      "\n",
      "### 4. **å»ºè®®çš„è¡ŒåŠ¨æ­¥éª¤**\n",
      "   1. **è‡ªæˆ‘åæ€ä¸è®°å½•**ï¼š\n",
      "      - è§‚å¯Ÿè‡ªå·±çš„çœŸå®å¸å¼•å¯¹è±¡ï¼šä½ æ˜¯å¦å¯¹ç°å®ä¸­çš„å¼‚æ€§/åŒæ€§æœ‰æ„Ÿæƒ…æˆ–æ€§å¸å¼•ï¼Ÿ  \n",
      "      - è®°å½•è§¦å‘ååº”çš„å…·ä½“æƒ…å¢ƒï¼ˆå¦‚åŠ¨ç‰©çš„äº’åŠ¨æ–¹å¼ã€è§‚çœ‹åª’ä»‹ç­‰ï¼‰ï¼Œåˆ†ææ˜¯å¦å­˜åœ¨è”æƒ³å› ç´ ã€‚\n",
      "    \n",
      "   2. **é™åˆ¶æš´éœ²ä¸è°ƒæ•´ä¹ æƒ¯**ï¼š\n",
      "      - å¦‚æœè¿™ç§æƒ…å†µä¸é¢‘ç¹æ¥è§¦ç›¸å…³å†…å®¹æœ‰å…³ï¼Œå¯ä»¥å°è¯•å‡å°‘è§¦å‘æºï¼ˆå¦‚é¿å…è§‚çœ‹ç›¸å…³å½±åƒï¼‰ï¼Œè§‚å¯Ÿååº”æ˜¯å¦å‡å¼±ã€‚\n",
      "\n",
      "   3. **å¯»æ±‚ä¸“ä¸šæ”¯æŒ**ï¼š\n",
      "      - å¦‚æœæ„Ÿåˆ°å›°æ‰°æˆ–æ— æ³•è‡ªè¡Œè°ƒæ•´ï¼Œå»ºè®®å’¨è¯¢å¿ƒç†åŒ»ç”Ÿæˆ–æ€§å¥åº·ä¸“å®¶ã€‚ä»–ä»¬å¯ä»¥å¸®åŠ©ï¼š\n",
      "        - åˆ†è¾¨è¿™æ˜¯æš‚æ—¶çš„å¥½å¥‡è¿˜æ˜¯æ·±å±‚å¿ƒç†éœ€æ±‚ã€‚\n",
      "        - æä¾›è®¤çŸ¥è¡Œä¸ºç–—æ³•ç­‰æ–¹å¼ç®¡ç†ååº”ã€‚\n",
      "        - æ¶ˆé™¤ç–‘æƒ‘ï¼Œé¿å…ä¸å¿…è¦çš„ç„¦è™‘ã€‚\n",
      "\n",
      "   4. **ç†è§£æ€§å¤šå…ƒæ€§**ï¼š\n",
      "      - äººç±»çš„æ€§å…´è¶£å¤æ‚å¤šæ ·ï¼Œå•çº¯ç”Ÿç†ååº”ä¸ä¸€å®šç­‰åŒäºèº«ä»½æ ‡ç­¾ã€‚é‡è¦çš„æ˜¯è¿™äº›å…´è¶£æ˜¯å¦å¯¹ä½ æˆ–ä»–äººé€ æˆå›°æ‰°ï¼Œæ˜¯å¦éœ€è¦å¹²é¢„ã€‚\n",
      "\n",
      "### 5. **é‡è¦æé†’**\n",
      "   - **æ³•å¾‹ä¸é“å¾·ç•Œé™**ï¼šæ— è®ºä¸ªäººå…´è¶£å¦‚ä½•ï¼Œä¸åŠ¨ç‰©çš„æ€§è¡Œä¸ºåœ¨è®¸å¤šåœ°åŒºæ˜¯è¿æ³•ä¸”è¿èƒŒä¼¦ç†çš„ï¼Œéœ€ç»å¯¹é¿å…ã€‚\n",
      "   - **æ— éœ€è¿‡åº¦ææ…Œ**ï¼šå¶å°”çš„ç”Ÿç†ååº”å¯èƒ½åªæ˜¯å¤§è„‘å¯¹åˆºæ¿€çš„è‡ªç„¶åé¦ˆï¼Œä¸å¿…ç›´æ¥ä¸Šå‡åˆ°èº«ä»½è®¤åŒé—®é¢˜ã€‚\n",
      "\n",
      "### æ€»ç»“\n",
      "ä½ çš„æƒ…å†µæ›´å¤šæŒ‡å‘**æ€§åå¥½æˆ–å¶ç„¶çš„ç”Ÿç†ååº”**ï¼Œè€Œéæ€§å–å‘æœ¬èº«çš„å˜åŒ–ã€‚å»ºè®®é€šè¿‡è‡ªæˆ‘è§‚å¯Ÿå’Œä¸“ä¸šå’¨è¯¢è¿›ä¸€æ­¥æ˜ç¡®åŸå› ï¼Œå¹¶é‡‡å–é€‚å½“çš„æ–¹å¼åº”å¯¹ã€‚ä¿æŠ¤å¥½è‡ªå·±å’Œä»–äººçš„èº«å¿ƒå¥åº·æ°¸è¿œæ˜¯æœ€é‡è¦çš„ã€‚å¦‚éœ€æ›´å¤šèµ„æºï¼Œå¯ä»¥è”ç³»å½“åœ°å¿ƒç†å’¨è¯¢æœºæ„æˆ–æ€§å¥åº·ç»„ç»‡ã€‚ ğŸŒ±\n",
      "--------------------\n",
      "--- Example 2 ---\n",
      "Prompt: [{'content': 'ä½ è¢«ç»™å®šäº†ä¸€ä¸ªé—®é¢˜ï¼Œè€ƒè™‘é—®é¢˜å¹¶æä¾›ä½ ç»™å‡ºçš„ç­”æ¡ˆã€‚\\nè¯·å°†æ€è€ƒè¿‡ç¨‹æ”¾åœ¨ <start_working_out> å’Œ <end_working_out> ä¹‹é—´ã€‚\\nç„¶åï¼Œè¯·åœ¨ <SOLUTION> å’Œ </SOLUTION> ä¹‹é—´æä¾›ä½ çš„ç­”æ¡ˆã€‚', 'role': 'system'}, {'content': 'Mabel has 5 daisies in her garden, and each daisy has 8 petals.  If she gives 2 daisies to her teacher, how many petals does she have on the remaining daisies in her garden?', 'role': 'user'}]\n",
      "Answer: 24\n",
      "--------------------\n",
      "--- Example 3 ---\n",
      "Prompt: [{'content': 'ä½ è¢«ç»™å®šäº†ä¸€ä¸ªé—®é¢˜ï¼Œè€ƒè™‘é—®é¢˜å¹¶æä¾›ä½ ç»™å‡ºçš„ç­”æ¡ˆã€‚\\nè¯·å°†æ€è€ƒè¿‡ç¨‹æ”¾åœ¨ <start_working_out> å’Œ <end_working_out> ä¹‹é—´ã€‚\\nç„¶åï¼Œè¯·åœ¨ <SOLUTION> å’Œ </SOLUTION> ä¹‹é—´æä¾›ä½ çš„ç­”æ¡ˆã€‚', 'role': 'system'}, {'content': 'Nancy bought a pie sliced it into 8 pieces. She gave 1/2 to Joe and Darcy, and she gave 1/4 to Carl. How many slices were left?', 'role': 'user'}]\n",
      "Answer: 2\n",
      "--------------------\n",
      "\n",
      "Structure of the first example:\n",
      "{'answer': 'æ ¹æ®ä½ çš„æè¿°ï¼Œä½ çš„æƒ…å†µéœ€è¦ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢æ¥ç†è§£å’Œæ¢è®¨ï¼š\\n\\n### 1. **æ€§å–å‘çš„åŸºæœ¬å®šä¹‰**\\n   - æ€§å–å‘é€šå¸¸æŒ‡ä¸€ä¸ªäººåœ¨æƒ…æ„Ÿå’Œæ€§æ–¹é¢è¢«å¸å¼•çš„æ€§åˆ«ï¼ˆå¦‚å¼‚æ€§æ‹ã€åŒæ€§æ‹ã€åŒæ€§æ‹ç­‰ï¼‰ã€‚**æ ¸å¿ƒåœ¨äºå¯¹äººç±»æ€§åˆ«çš„åå¥½**ã€‚\\n   - åŠ¨ç‰©å¹¶ä¸å±äºäººç±»æ€§åˆ«èŒƒç•´ï¼Œå› æ­¤å¯¹åŠ¨ç‰©çš„æ€§ååº”ä¸å±äºä¼ ç»Ÿæ„ä¹‰ä¸Šçš„æ€§å–å‘é—®é¢˜ã€‚å®ƒå¯èƒ½æ›´æ¥è¿‘äºä¸€ç§**ç‰¹æ®Šåå¥½æˆ–æ€§å…´è¶£**ï¼ˆparaphiliaï¼‰ï¼Œä½†è¿™éœ€ä¸“ä¸šè¯„ä¼°ã€‚\\n\\n### 2. **ç”Ÿç†ååº”çš„å¯èƒ½åŸå› **\\n   - **è§†è§‰æˆ–æƒ…å¢ƒè”æƒ³**ï¼šå³ä½¿åˆºæ¿€æºæ˜¯åŠ¨ç‰©è¡Œä¸ºï¼Œä½ ä¹Ÿå¯èƒ½å› ç‰¹å®šåŠ¨ä½œã€äº’åŠ¨æ–¹å¼è€Œè”æƒ³åˆ°äººç±»çš„äº²å¯†è¡Œä¸ºï¼Œä»è€Œè§¦å‘ç”Ÿç†ååº”ã€‚\\n   - **æš‚æ—¶æ€§åˆºæ¿€**ï¼šå¯èƒ½å› è¿‘æœŸæ¥è§¦ç±»ä¼¼å†…å®¹è¾ƒå¤šï¼Œäº§ç”ŸçŸ­æœŸæ¡ä»¶åå°„ï¼Œè€ŒéæŒä¹…çš„æ€§å–å‘æ”¹å˜ã€‚\\n   - **å¿ƒç†æˆ–æƒ…ç»ªå› ç´ **ï¼šå‹åŠ›ã€å¥½å¥‡å¿ƒæˆ–å…¶ä»–å†…å¿ƒéœ€æ±‚å¯èƒ½é€šè¿‡è¿™ç§æ–¹å¼è¡¨è¾¾ï¼Œéœ€è¦è‡ªæˆ‘è§‰å¯Ÿæˆ–å’¨è¯¢æ”¯æŒã€‚\\n\\n### 3. **éœ€è¦åŒºåˆ†çš„æ¦‚å¿µ**\\n   - **æ€§å–å‘**ï¼šå¯¹ç‰¹å®šäººç±»æ€§åˆ«çš„å¸å¼•ï¼ˆå¦‚å¼‚æ€§ã€åŒæ€§ï¼‰ã€‚\\n   - **æ€§åå¥½**ï¼šå¯¹æŸç§è¡Œä¸ºã€åœºæ™¯ã€ç‰©å“çš„ç‰¹æ®Šå…´è¶£ï¼ˆä¾‹å¦‚æ‹ç‰©ã€è§‚çœ‹ç‰¹å®šè¡Œä¸ºç­‰ï¼‰ã€‚\\n   - **å¯¹åŠ¨ç‰©çš„æ€§å…´è¶£**ï¼ˆZoophiliaï¼‰ï¼šå¦‚æœè¿™ç§å¸å¼•æ˜¯æŒç»­ã€æ’ä»–çš„ï¼Œåˆ™å±äºæ€§åå¥½ä¸­çš„ä¸€ç§ç‰¹æ®Šæƒ…å†µï¼Œéœ€ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°ã€‚\\n\\n### 4. **å»ºè®®çš„è¡ŒåŠ¨æ­¥éª¤**\\n   1. **è‡ªæˆ‘åæ€ä¸è®°å½•**ï¼š\\n      - è§‚å¯Ÿè‡ªå·±çš„çœŸå®å¸å¼•å¯¹è±¡ï¼šä½ æ˜¯å¦å¯¹ç°å®ä¸­çš„å¼‚æ€§/åŒæ€§æœ‰æ„Ÿæƒ…æˆ–æ€§å¸å¼•ï¼Ÿ  \\n      - è®°å½•è§¦å‘ååº”çš„å…·ä½“æƒ…å¢ƒï¼ˆå¦‚åŠ¨ç‰©çš„äº’åŠ¨æ–¹å¼ã€è§‚çœ‹åª’ä»‹ç­‰ï¼‰ï¼Œåˆ†ææ˜¯å¦å­˜åœ¨è”æƒ³å› ç´ ã€‚\\n    \\n   2. **é™åˆ¶æš´éœ²ä¸è°ƒæ•´ä¹ æƒ¯**ï¼š\\n      - å¦‚æœè¿™ç§æƒ…å†µä¸é¢‘ç¹æ¥è§¦ç›¸å…³å†…å®¹æœ‰å…³ï¼Œå¯ä»¥å°è¯•å‡å°‘è§¦å‘æºï¼ˆå¦‚é¿å…è§‚çœ‹ç›¸å…³å½±åƒï¼‰ï¼Œè§‚å¯Ÿååº”æ˜¯å¦å‡å¼±ã€‚\\n\\n   3. **å¯»æ±‚ä¸“ä¸šæ”¯æŒ**ï¼š\\n      - å¦‚æœæ„Ÿåˆ°å›°æ‰°æˆ–æ— æ³•è‡ªè¡Œè°ƒæ•´ï¼Œå»ºè®®å’¨è¯¢å¿ƒç†åŒ»ç”Ÿæˆ–æ€§å¥åº·ä¸“å®¶ã€‚ä»–ä»¬å¯ä»¥å¸®åŠ©ï¼š\\n        - åˆ†è¾¨è¿™æ˜¯æš‚æ—¶çš„å¥½å¥‡è¿˜æ˜¯æ·±å±‚å¿ƒç†éœ€æ±‚ã€‚\\n        - æä¾›è®¤çŸ¥è¡Œä¸ºç–—æ³•ç­‰æ–¹å¼ç®¡ç†ååº”ã€‚\\n        - æ¶ˆé™¤ç–‘æƒ‘ï¼Œé¿å…ä¸å¿…è¦çš„ç„¦è™‘ã€‚\\n\\n   4. **ç†è§£æ€§å¤šå…ƒæ€§**ï¼š\\n      - äººç±»çš„æ€§å…´è¶£å¤æ‚å¤šæ ·ï¼Œå•çº¯ç”Ÿç†ååº”ä¸ä¸€å®šç­‰åŒäºèº«ä»½æ ‡ç­¾ã€‚é‡è¦çš„æ˜¯è¿™äº›å…´è¶£æ˜¯å¦å¯¹ä½ æˆ–ä»–äººé€ æˆå›°æ‰°ï¼Œæ˜¯å¦éœ€è¦å¹²é¢„ã€‚\\n\\n### 5. **é‡è¦æé†’**\\n   - **æ³•å¾‹ä¸é“å¾·ç•Œé™**ï¼šæ— è®ºä¸ªäººå…´è¶£å¦‚ä½•ï¼Œä¸åŠ¨ç‰©çš„æ€§è¡Œä¸ºåœ¨è®¸å¤šåœ°åŒºæ˜¯è¿æ³•ä¸”è¿èƒŒä¼¦ç†çš„ï¼Œéœ€ç»å¯¹é¿å…ã€‚\\n   - **æ— éœ€è¿‡åº¦ææ…Œ**ï¼šå¶å°”çš„ç”Ÿç†ååº”å¯èƒ½åªæ˜¯å¤§è„‘å¯¹åˆºæ¿€çš„è‡ªç„¶åé¦ˆï¼Œä¸å¿…ç›´æ¥ä¸Šå‡åˆ°èº«ä»½è®¤åŒé—®é¢˜ã€‚\\n\\n### æ€»ç»“\\nä½ çš„æƒ…å†µæ›´å¤šæŒ‡å‘**æ€§åå¥½æˆ–å¶ç„¶çš„ç”Ÿç†ååº”**ï¼Œè€Œéæ€§å–å‘æœ¬èº«çš„å˜åŒ–ã€‚å»ºè®®é€šè¿‡è‡ªæˆ‘è§‚å¯Ÿå’Œä¸“ä¸šå’¨è¯¢è¿›ä¸€æ­¥æ˜ç¡®åŸå› ï¼Œå¹¶é‡‡å–é€‚å½“çš„æ–¹å¼åº”å¯¹ã€‚ä¿æŠ¤å¥½è‡ªå·±å’Œä»–äººçš„èº«å¿ƒå¥åº·æ°¸è¿œæ˜¯æœ€é‡è¦çš„ã€‚å¦‚éœ€æ›´å¤šèµ„æºï¼Œå¯ä»¥è”ç³»å½“åœ°å¿ƒç†å’¨è¯¢æœºæ„æˆ–æ€§å¥åº·ç»„ç»‡ã€‚ ğŸŒ±', 'prompt': [{'content': 'ä½ è¢«ç»™å®šäº†ä¸€ä¸ªé—®é¢˜ï¼Œè€ƒè™‘é—®é¢˜å¹¶æä¾›ä½ ç»™å‡ºçš„ç­”æ¡ˆã€‚\\nè¯·å°†æ€è€ƒè¿‡ç¨‹æ”¾åœ¨ <start_working_out> å’Œ <end_working_out> ä¹‹é—´ã€‚\\nç„¶åï¼Œè¯·åœ¨ <SOLUTION> å’Œ </SOLUTION> ä¹‹é—´æä¾›ä½ çš„ç­”æ¡ˆã€‚', 'role': 'system'}, {'content': 'æœ€è¿‘åªè¦çœ‹åˆ°åŠ¨ç‰©çˆ±çˆ±æˆ‘å°±ç¡¬äº†ï¼Œ æœ€è¿‘åªè¦çœ‹åˆ°åŠ¨ç‰©çˆ±çˆ±æˆ‘å°±ç¡¬äº†ï¼Œæˆ‘è¿™æ ·æ˜¯ä¸æ˜¯å¼‚æ€§æ‹å•Š', 'role': 'user'}]}\n"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "# --- å¤„ç†ç¬¬ä¸€ä¸ªæ•°æ®é›† (dataset) ---\n",
    "\n",
    "# è·å–åŸå§‹åˆ—åï¼Œä»¥ä¾¿åç»­ç§»é™¤\n",
    "original_columns_ds1 = dataset.column_names\n",
    "\n",
    "# æ ¼å¼åŒ–æ•°æ®é›†ï¼š\n",
    "# 1. æ„å»º prompt åˆ—è¡¨ï¼ŒåŒ…å« system_prompt å’Œ user çš„ question\n",
    "# 2. ä½¿ç”¨ extract_hash_answer æ¸…æ´— answer\n",
    "# 3. ç§»é™¤åŸå§‹åˆ—\n",
    "print(f\"Processing dataset 1 (size: {len(dataset)})...\")\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": x[\"question\"]},\n",
    "        ],\n",
    "        \"answer\": extract_hash_answer(x[\"answer\"]),\n",
    "    },\n",
    "    remove_columns=original_columns_ds1  # ç§»é™¤æ‰€æœ‰åŸå§‹åˆ—\n",
    ")\n",
    "print(\"Dataset 1 processed.\")\n",
    "\n",
    "# æ‰“å°å¤„ç†åçš„ç¬¬ä¸€ä¸ªæ•°æ®é›†çš„ç¤ºä¾‹\n",
    "print(\"\\nExample from processed Dataset 1:\")\n",
    "print(\"Prompt:\", dataset[0][\"prompt\"])\n",
    "print(\"Answer:\", dataset[0][\"answer\"])\n",
    "\n",
    "# --- å¤„ç†ç¬¬äºŒä¸ªæ•°æ®é›† (dataset2) ---\n",
    "\n",
    "# è¾…åŠ©å‡½æ•°ï¼šæ£€æŸ¥ dataset2 çš„ 'output' å­—æ®µåœ¨ </think> æ ‡ç­¾åæ˜¯å¦æœ‰æœ‰æ•ˆå†…å®¹\n",
    "def has_valid_content(output_text):\n",
    "    \"\"\"æ£€æŸ¥</think>æ ‡ç­¾åçš„å†…å®¹æ˜¯å¦æœ‰æ•ˆï¼ˆä¸æ˜¯ç©ºçš„ã€åªæœ‰ç©ºæ ¼æˆ–åªæœ‰å¥å·ï¼‰\"\"\"\n",
    "    if \"</think>\" not in output_text:\n",
    "        # å¦‚æœæ²¡æœ‰ </think> æ ‡ç­¾ï¼Œæˆ‘ä»¬å‡è®¾å†…å®¹æ˜¯æœ‰æ•ˆçš„æˆ–ä¸éœ€è¦è¿™ç§ç‰¹å®šæ ¼å¼\n",
    "        # æ³¨æ„ï¼šæ ¹æ®éœ€æ±‚ï¼Œè¿™é‡Œçš„é€»è¾‘å¯èƒ½éœ€è¦è°ƒæ•´ã€‚å½“å‰å®ç°æ˜¯å¦‚æœæ²¡æœ‰æ ‡ç­¾åˆ™è§†ä¸ºæ— æ•ˆã€‚\n",
    "        # å¦‚æœæ²¡æœ‰æ ‡ç­¾ä¹Ÿåº”ä¿ç•™ï¼Œåˆ™è¿”å› Trueã€‚\n",
    "        # ä¸ºäº†åŒ¹é…åŸå§‹é€»è¾‘ï¼ˆè¿‡æ»¤æ‰æ²¡æœ‰</think>æ ‡ç­¾çš„ï¼‰ï¼Œè¿™é‡Œè¿”å› Falseã€‚\n",
    "        return False # åŸå§‹é€»è¾‘ä¼¼ä¹æ˜¯è¦æ±‚å¿…é¡»æœ‰ </think> æ ‡ç­¾\n",
    "\n",
    "    content_after_tag = extract_xml_answer(output_text)\n",
    "    # æ£€æŸ¥æå–çš„å†…å®¹æ˜¯å¦ä¸ºç©ºã€åªæœ‰ç©ºæ ¼æˆ–åªæœ‰å¥å·\n",
    "    if not content_after_tag or content_after_tag.isspace() or content_after_tag == \".\":\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# è¿‡æ»¤ dataset2ï¼Œåªä¿ç•™ 'output' å­—æ®µåŒ…å«æœ‰æ•ˆå†…å®¹çš„æ¡ç›®\n",
    "print(f\"\\nProcessing dataset 2 (original size: {len(dataset2)})...\")\n",
    "valid_indices = [\n",
    "    i for i, example in enumerate(dataset2)\n",
    "    if 'output' in example and has_valid_content(example['output'])\n",
    "]\n",
    "dataset2_filtered = dataset2.select(valid_indices)\n",
    "print(f\"Filtered dataset 2 size: {len(dataset2_filtered)} valid examples.\")\n",
    "\n",
    "# è·å–è¿‡æ»¤å dataset2 çš„åŸå§‹åˆ—å\n",
    "original_columns_ds2 = dataset2_filtered.column_names\n",
    "\n",
    "# æ ¼å¼åŒ–è¿‡æ»¤åçš„ dataset2ï¼š\n",
    "# 1. æ„å»º prompt åˆ—è¡¨ï¼ŒåŒ…å« system_prompt å’Œ user çš„ instruction/input\n",
    "# 2. ä½¿ç”¨ extract_xml_answer æ¸…æ´— answer (ä» output æå–)\n",
    "# 3. ç§»é™¤åŸå§‹åˆ—\n",
    "dataset2_processed = dataset2_filtered.map(\n",
    "    lambda x: {\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": x[\"instruction\"] if 'instruction' in x else x.get('input', '')},\n",
    "        ],\n",
    "        \"answer\": extract_xml_answer(x[\"output\"]),\n",
    "    },\n",
    "    remove_columns=original_columns_ds2\n",
    ")\n",
    "print(\"Dataset 2 processed.\")\n",
    "\n",
    "# æ‰“å°å¤„ç†åçš„ç¬¬äºŒä¸ªæ•°æ®é›†çš„ç¤ºä¾‹\n",
    "print(\"\\nExample from processed Dataset 2:\")\n",
    "if len(dataset2_processed) > 0:\n",
    "    print(\"Prompt:\", dataset2_processed[0][\"prompt\"])\n",
    "    print(\"Answer:\", dataset2_processed[0][\"answer\"])\n",
    "else:\n",
    "    print(\"Processed Dataset 2 is empty.\")\n",
    "\n",
    "# --- åˆå¹¶ä¸æ‰“ä¹±æ•°æ®é›† ---\n",
    "\n",
    "# åˆå¹¶å¤„ç†åçš„ä¸¤ä¸ªæ•°æ®é›†\n",
    "print(\"\\nCombining and shuffling datasets...\")\n",
    "final_dataset = concatenate_datasets([dataset, dataset2_processed])\n",
    "\n",
    "# æ‰“ä¹±åˆå¹¶åçš„æ•°æ®é›†\n",
    "final_dataset = final_dataset.shuffle(seed=42)\n",
    "\n",
    "print(f\"Combined dataset size: {len(final_dataset)}\")\n",
    "\n",
    "# Print the first few examples of the final dataset to check the structure\n",
    "print(\"\\nFirst few examples from the final combined and shuffled dataset:\")\n",
    "for i in range(min(3, len(final_dataset))): # Print up to 3 examples\n",
    "    print(f\"--- Example {i+1} ---\")\n",
    "    print(\"Prompt:\", final_dataset[i][\"prompt\"])\n",
    "    print(\"Answer:\", final_dataset[i][\"answer\"])\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "# Optionally, print the structure of one example\n",
    "if len(final_dataset) > 0:\n",
    "    print(\"\\nStructure of the first example:\")\n",
    "    print(final_dataset[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faaf472",
   "metadata": {},
   "source": [
    "### å®šä¹‰å¥–åŠ±å‡½æ•°\n",
    "#### å®šä¹‰æ ‡å‡†æ ¼å¼å½¢å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ab0285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼ï¼Œç”¨æ¥åˆ¤æ–­æ¨¡å‹çš„è¾“å‡ºæ˜¯å¦ç¬¦åˆæ ¼å¼è¦æ±‚\n",
    "match_format = re.compile(\n",
    "    rf\"^[\\s]{{0,}}\"\\\n",
    "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\\\n",
    "    rf\"{solution_start}(.+?){solution_end}\"\\\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "match_format.search(\n",
    "    \"<start_working_out>Let me think!<end_working_out>\"\\\n",
    "    \"<SOLUTION>2</SOLUTION>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f1a09b",
   "metadata": {},
   "source": [
    "#### æ„é€ å¥–åŠ±å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1c793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸¥æ ¼æ ¼å¼åˆ¤æ–­å‡½æ•°\n",
    "def match_format_exactly(completions, **kwargs):\n",
    "    \"\"\"æ ¼å¼åˆ¤æ–­å‡½æ•°ï¼Œä¸¥æ ¼åˆ¤æ–­æ ¼å¼æ˜¯å¦åŒ¹é…\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Match if format is seen exactly!\n",
    "        if match_format.search(response) is not None: score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13513da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¼±æ ¼å¼åˆ¤æ–­å‡½æ•°\n",
    "def match_format_approximately(completions, **kwargs):\n",
    "    \"\"\"å¼±æ ¼å¼åˆ¤æ–­å¥–åŠ±ï¼Œå³ä½¿æ²¡æœ‰ä¸¥æ ¼å¯¹åº”ï¼Œä¹Ÿå¯ä»¥æ ¹æ®ä½¿ç”¨çš„æ ‡ç­¾æ•°é‡æ¥åšå‡ºç›¸åº”çš„å¥–åŠ±\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # æ•°ä¸€æ•°çœ‹åˆ°å¤šå°‘ä¸ªå…³é”®è¯â€”â€”å¦‚æœå¤ªå¤šï¼Œæˆ‘ä»¬ä¼šæƒ©ç½šä½ ï¼\n",
    "        # å¦‚æœæˆ‘ä»¬çœ‹åˆ°1ä¸ªå…³é”®è¯ï¼Œé‚£ä¹ˆåŠ ä¸€äº›ç§¯åˆ†ï¼å¦‚æœæ›´å¤šäº†ï¼Œé‚£ä¹ˆå°±åº”å½“æ‰£é™¤ä¸€äº›åˆ†\n",
    "        score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n",
    "        score += 0.5 if response.count(reasoning_end)   == 1 else -0.5\n",
    "        score += 0.5 if response.count(solution_start)  == 1 else -0.5\n",
    "        score += 0.5 if response.count(solution_end)    == 1 else -0.5\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e10a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å›ç­”æ£€æŸ¥ï¼šé€šç”¨ç­”æ¡ˆæ£€æŸ¥\n",
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    \"\"\"é€šè¿‡æ¯”è¾ƒæå–çš„ç­”æ¡ˆä¸å‚è€ƒç­”æ¡ˆæ¥è¯„ä¼°æ¨¡å‹å“åº”ã€‚\n",
    "    \n",
    "    è¯¥å‡½æ•°ä»ç»“æ„åŒ–æ¨¡å‹è¾“å‡ºä¸­æå–ç­”æ¡ˆå¹¶ä¸å‚è€ƒç­”æ¡ˆè¿›è¡Œæ¯”è¾ƒï¼Œæ ¹æ®åŒ¹é…è´¨é‡åˆ†é…åˆ†æ•°ï¼š\n",
    "    - å®Œå…¨åŒ¹é…ï¼š3.0åˆ†\n",
    "    - å»é™¤ç©ºæ ¼ååŒ¹é…ï¼š1.5åˆ†\n",
    "    - æ•°å€¼ç­”æ¡ˆåœ¨æ­£ç¡®å€¼10%èŒƒå›´å†…ï¼š0.5åˆ†\n",
    "    - æ•°å€¼ç­”æ¡ˆåœ¨æ­£ç¡®å€¼20%èŒƒå›´å†…ï¼š0.25åˆ†\n",
    "    - é”™è¯¯ç­”æ¡ˆï¼š-0.5æˆ–-1.0åˆ†\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        prompts (list)ï¼šæä¾›ç»™æ¨¡å‹çš„å¯¹è¯æç¤ºåˆ—è¡¨\n",
    "        completions (list)ï¼šéœ€è¦è¯„ä¼°çš„æ¨¡å‹ç”Ÿæˆçš„å›ç­”\n",
    "        answer (list)ï¼šç”¨äºæ¯”è¾ƒçš„å‚è€ƒç­”æ¡ˆ\n",
    "        **kwargsï¼šé¢å¤–å‚æ•°\n",
    "    \"\"\"\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_format.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        score = 0\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        # å¦‚æœå®Œå…¨ä¸€è‡´ï¼Œå°±ç»™å‡º 3 åˆ† \n",
    "        if guess == true_answer:\n",
    "            score += 3.0\n",
    "        # å¦‚æœç»“æœæ­£ç¡®ï¼Œä½†æ˜¯æœ‰ç©ºæ ¼ï¼Œå°±ç»™1.5åˆ†\n",
    "        elif guess.strip() == true_answer.strip():\n",
    "            score += 1.5\n",
    "        else:\n",
    "            # å¦‚æœç­”æ¡ˆæ¥è¿‘æ¯”ç‡ï¼Œæˆ‘ä»¬ä¹Ÿä¼šå¥–åŠ±å®ƒï¼\n",
    "            # å³ï¼Œå¦‚æœç­”æ¡ˆåœ¨æŸä¸ªèŒƒå›´å†…ï¼Œå¥–åŠ±å®ƒï¼\n",
    "            try:\n",
    "                ratio = float(guess) / float(true_answer)\n",
    "                if   ratio >= 0.9 and ratio <= 1.1: score += 0.5\n",
    "                elif ratio >= 0.8 and ratio <= 1.2: score += 0.25\n",
    "                else: score -= 1.0 # Penalize wrong answers\n",
    "            except:\n",
    "                # å¦‚æœç›´æ¥å¼‚å¸¸äº†ï¼Œå°±æŠ›å‡ºé”™è¯¯\n",
    "                score -= 0.5 # Penalize\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9bc32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¹äºæ•°å­¦é—®é¢˜ï¼Œå…ˆç»™æ•°å­—éƒ¨åˆ†æŠ½å–å‡ºæ¥\n",
    "match_numbers = re.compile(\n",
    "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "# å›ç­”æ£€æŸ¥ï¼šç‰¹å®šæ•°å­—æ£€æŸ¥\n",
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "    \"\"\"ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä»æ¨¡å‹è¾“å‡ºä¸­æå–æ•°å­—ç­”æ¡ˆå¹¶è¿›è¡Œè¯„åˆ†ã€‚\n",
    "    \n",
    "    è¯¥å‡½æ•°ä»æ¨¡å‹å“åº”ä¸­æå–æ•°å­—ï¼Œå¹¶ä¸å‚è€ƒç­”æ¡ˆè¿›è¡Œæ•°å€¼æ¯”è¾ƒã€‚\n",
    "    å¦‚æœæå–çš„æ•°å­—ä¸æ­£ç¡®ç­”æ¡ˆå®Œå…¨åŒ¹é…ï¼Œå°†è·å¾—1.5åˆ†ï¼Œå¦åˆ™ä¸º0åˆ†ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        prompts (list)ï¼šæä¾›ç»™æ¨¡å‹çš„å¯¹è¯æç¤ºåˆ—è¡¨\n",
    "        completions (list)ï¼šéœ€è¦è¯„ä¼°çš„æ¨¡å‹ç”Ÿæˆçš„å›ç­”\n",
    "        answer (list)ï¼šç”¨äºæ¯”è¾ƒçš„å‚è€ƒç­”æ¡ˆæ•°å€¼\n",
    "        **kwargsï¼šé¢å¤–å‚æ•°\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        listï¼šåŸºäºæ•°å€¼åŒ¹é…çš„è¯„åˆ†åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_numbers.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    \n",
    "    # è¾“å‡ºè°ƒè¯•\n",
    "    print('*'*20, f\"Question:\\n{question}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "    \n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        # Convert to numbers\n",
    "        try:\n",
    "            true_answer = float(true_answer.strip())\n",
    "            guess       = float(guess.strip())\n",
    "            scores.append(1.5 if guess == true_answer else 0.0)\n",
    "        except:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e0730a",
   "metadata": {},
   "source": [
    "### è®­ç»ƒéƒ¨åˆ†\n",
    "#### è®­ç»ƒé…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bbc15bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 1 to the `num_generations` of 4\n"
     ]
    }
   ],
   "source": [
    "max_prompt_length = 256\n",
    "\n",
    "# ä½¿ç”¨ GRPO è®­ç»ƒå™¨ï¼Œå¹¶æ„é€ è®­ç»ƒå™¨\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    beta = 0.0, # è®¾ç½®ä¸º 0 ä»¥ç¦ç”¨ KL æ•£åº¦æƒ©ç½š # defaults to 0.04\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"adamw_torch_fused\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 1, # å¢åŠ åˆ°4ï¼Œä»¥ä¾¿æ›´é¡ºæ»‘åœ°è®­ç»ƒ #1\n",
    "    num_generations = 4, # Decrease if out of memory\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_seq_length - max_prompt_length,\n",
    "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    max_steps = 500, # è®­ç»ƒæ­¥æ•°\n",
    "    save_steps = 200, # æ¯200æ­¥ä¿å­˜ä¸€æ¬¡\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir = \"outputs_gemma3_1b_it_2\", # è¾“å‡ºç›®å½•\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e10ffba",
   "metadata": {},
   "source": [
    "#### å¼€å§‹è®­ç»ƒ\n",
    "å¼€å§‹è®­ç»ƒã€‚æœŸæœ›åœ¨è®­ç»ƒä¸­ï¼Œçœ‹åˆ°rewardåˆ—çš„æ•°å€¼å¢é•¿ï¼è€Œä¸æ˜¯ æŸå¤±å‡½æ•°\n",
    "æœ‰å¯èƒ½åœ¨å¼€å§‹çš„100æ­¥éƒ½æ²¡æœ‰å¥–åŠ±ï¼Œä½ å¯èƒ½éœ€è¦ç­‰å¾…150-200æ­¥ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b07040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 9,452 | Num Epochs = 1 | Total steps = 500\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 1 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 6,522,880/1,006,408,832 (0.65% trained)\n",
      "`generation_config` default values have been modified to match model-specific defaults: {'max_length': 32768, 'top_k': 64, 'top_p': 0.95, 'bos_token_id': 2, 'eos_token_id': [1, 106]}. If this is not desired, please set these values explicitly.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Question:\n",
      "Nurse Missy is attending to the needs of 12 patients in her hospital ward.  Most of her patients require standard care, but one-third of her patients have special dietary requirements, which increases the serving time by 20%.  At dinner time, she brings each patient their meal. It takes 5 minutes to serve each standard care patient.  How long does it take, in minutes, for Missy to serve dinner to all of her patients? \n",
      "Answer:\n",
      "64 \n",
      "Response:\n",
      "<start_working_out>\n",
      "Let's analyze the problem. We have 12 patients.\n",
      "- 25% of the patients have special dietary requirements. That means 12 * 0.25 = 3 patients have special dietary requirements.\n",
      "- The remaining patients are standard care, so 12 - 3 = 9 patients have standard care.\n",
      "- Each standard care patient takes 5 minutes to serve.\n",
      "- The special dietary patients take 5 minutes + 20% of 5 minutes, which is 5 + (0.20 * 5) = 5 + 1 = 6 minutes.\n",
      "- The total time to serve standard care patients is 9 * 5 = 45 minutes.\n",
      "- The total time to serve special dietary patients is 3 * 6 = 18 minutes.\n",
      "- The total time to serve all patients is 45 + 18 = 63 minutes.\n",
      "</end_working_out>\n",
      "<SOLUTION>\n",
      "63</SOLUTION> \n",
      "Extracted:\n",
      "63\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# åˆ›å»ºè®­ç»ƒå™¨ï¼Œå¹¶ä¸”ä½¿ç”¨ä¸Šé¢ç»™å‡ºçš„ reward function\u001b[39;00m\n\u001b[32m      2\u001b[39m trainer = GRPOTrainer(\n\u001b[32m      3\u001b[39m     model = model,\n\u001b[32m      4\u001b[39m     processing_class = tokenizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     train_dataset = dataset,\n\u001b[32m     13\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:311\u001b[39m, in \u001b[36m_fast_inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:77\u001b[39m, in \u001b[36m_unsloth_training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/accelerate/accelerator.py:2359\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2357\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2358\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2359\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/function.py:307\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    302\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mImplementing both \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbackward\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mvjp\u001b[39m\u001b[33m'\u001b[39m\u001b[33m for a custom \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    303\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFunction is not allowed. You should only implement one \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    304\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mof them.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    305\u001b[39m     )\n\u001b[32m    306\u001b[39m user_fn = vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function.vjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:1710\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.backward\u001b[39m\u001b[34m(ctx, *flat_args)\u001b[39m\n\u001b[32m   1708\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m CompiledFunction._double_backward(ctx, impl_fn, all_args)\n\u001b[32m   1709\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1710\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:1700\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.backward.<locals>.impl_fn\u001b[39m\u001b[34m(double_ctx)\u001b[39m\n\u001b[32m   1699\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mimpl_fn\u001b[39m(double_ctx=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1700\u001b[39m     out = \u001b[43mCompiledFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_backward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1701\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m CompiledFunction._backward_epilogue(ctx, out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2065\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction._backward_impl\u001b[39m\u001b[34m(ctx, all_args)\u001b[39m\n\u001b[32m   2048\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2049\u001b[39m     torch._functorch.config.donated_buffer\n\u001b[32m   2050\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m saved_tensors_use_once\n\u001b[32m   2051\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m fw_metadata.bw_donated_idxs != []\n\u001b[32m   2052\u001b[39m ):\n\u001b[32m   2053\u001b[39m     torch._check(\n\u001b[32m   2054\u001b[39m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2055\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[32m   (...)\u001b[39m\u001b[32m   2062\u001b[39m         ),\n\u001b[32m   2063\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2065\u001b[39m out = \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCompiledFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompiled_bw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2068\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2070\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2071\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[39m, in \u001b[36mcall_func_at_runtime_with_args\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33m_boxed_call\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         out = normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[32m    130\u001b[39m         warnings.warn(\n\u001b[32m    131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt take boxed arguments. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    741\u001b[39m prior_skip_guard_eval_unsafe = set_skip_guard_eval_unsafe(\n\u001b[32m    742\u001b[39m     _is_skip_guard_eval_unsafe_stance()\n\u001b[32m    743\u001b[39m )\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    747\u001b[39m     _maybe_set_eval_frame(prior)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_inductor/output_code.py:466\u001b[39m, in \u001b[36mCompiledFxGraph.__call__\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    468\u001b[39m     AutotuneCacheBundler.end_compile()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_inductor/utils.py:2128\u001b[39m, in \u001b[36malign_inputs_from_check_idxs.<locals>.run\u001b[39m\u001b[34m(new_inputs)\u001b[39m\n\u001b[32m   2126\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mrun\u001b[39m(new_inputs: List[InputType]):\n\u001b[32m   2127\u001b[39m     copy_misaligned_inputs(new_inputs, inputs_to_check)\n\u001b[32m-> \u001b[39m\u001b[32m2128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/torchinductor_root/ru/crup6ang6gzbn7qr4yllbhl7ftuuewtlmw2cwgbtbry2eybo2dpe.py:237\u001b[39m, in \u001b[36mcall\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    235\u001b[39m triton_poi_fused__to_copy_add_mul_neg_new_zeros_scatter_add_1_xnumel = s0*s1\n\u001b[32m    236\u001b[39m stream0 = get_raw_stream(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m \u001b[43mtriton_poi_fused__to_copy_add_mul_neg_new_zeros_scatter_add_1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtangents_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msum_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimals_11\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimals_10\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtriton_poi_fused__to_copy_add_mul_neg_new_zeros_scatter_add_1_xnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtriton_poi_fused__to_copy_add_mul_neg_new_zeros_scatter_add_1_xnumel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m exp_2\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m primals_10\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_inductor/runtime/triton_heuristics.py:1034\u001b[39m, in \u001b[36mCachingAutotuner.run\u001b[39m\u001b[34m(self, grid, stream, benchmark_run, *args, **kwargs)\u001b[39m\n\u001b[32m   1032\u001b[39m         \u001b[38;5;28mself\u001b[39m.precompile_time_taken_ns = time.time_ns() - start_time\n\u001b[32m   1033\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.launchers) > \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautotune_to_one_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28mself\u001b[39m.launchers[\u001b[32m0\u001b[39m].config, \u001b[33m\"\u001b[39m\u001b[33mfound_by_coordesc\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1038\u001b[39m ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.inductor_meta.get(\u001b[33m\"\u001b[39m\u001b[33mcoordinate_descent_tuning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28mself\u001b[39m.launchers = [\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28mself\u001b[39m.coordinate_descent_tuning(\n\u001b[32m   1041\u001b[39m             \u001b[38;5;28mself\u001b[39m.launchers[\u001b[32m0\u001b[39m], *args, grid=grid, **kwargs\n\u001b[32m   1042\u001b[39m         )\n\u001b[32m   1043\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_inductor/runtime/triton_heuristics.py:911\u001b[39m, in \u001b[36mCachingAutotuner.autotune_to_one_config\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    909\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Do the actual autotuning\"\"\"\u001b[39;00m\n\u001b[32m    910\u001b[39m start_time = time.time_ns()\n\u001b[32m--> \u001b[39m\u001b[32m911\u001b[39m timings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbenchmark_all_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    912\u001b[39m benchmark_time_taken_ns = time.time_ns() - start_time\n\u001b[32m    913\u001b[39m \u001b[38;5;28mself\u001b[39m.launchers = [builtins.min(timings, key=timings.get)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_inductor/runtime/triton_heuristics.py:886\u001b[39m, in \u001b[36mCachingAutotuner.benchmark_all_configs\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mbenchmark_all_configs\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    882\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[32m    883\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCachingAutotuner.benchmark_all_configs\u001b[39m\u001b[33m\"\u001b[39m, log_pt2_compile_event=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    884\u001b[39m     ):\n\u001b[32m    885\u001b[39m         timings = {\n\u001b[32m--> \u001b[39m\u001b[32m886\u001b[39m             launcher: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbench\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlauncher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    887\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m launcher \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.launchers\n\u001b[32m    888\u001b[39m         }\n\u001b[32m    890\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m timings.items():\n\u001b[32m    891\u001b[39m             \u001b[38;5;28mself\u001b[39m.coordesc_tuner.cache_benchmark_result(k.config, v)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_inductor/runtime/triton_heuristics.py:763\u001b[39m, in \u001b[36mCachingAutotuner.bench\u001b[39m\u001b[34m(self, launcher, grid, with_profiler, *args, **kwargs)\u001b[39m\n\u001b[32m    760\u001b[39m device_interface = \u001b[38;5;28mself\u001b[39m.get_device_interface()\n\u001b[32m    761\u001b[39m stream = device_interface.get_raw_stream(device_interface.current_device())\n\u001b[32m--> \u001b[39m\u001b[32m763\u001b[39m cpu_copies = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy_args_to_cpu_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mkernel_call\u001b[39m():\n\u001b[32m    766\u001b[39m     cloned_args, cloned_kwargs = \u001b[38;5;28mself\u001b[39m.maybe_clone_args(\n\u001b[32m    767\u001b[39m         cpu_copies, *args, **kwargs\n\u001b[32m    768\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_inductor/runtime/triton_heuristics.py:822\u001b[39m, in \u001b[36mCachingAutotuner.copy_args_to_cpu_if_needed\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    819\u001b[39m             budget -= size\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(args):\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m     \u001b[43mmaybe_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m.\u001b[49m\u001b[43marg_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m kwargs.items():\n\u001b[32m    825\u001b[39m     maybe_copy(name, arg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_inductor/runtime/triton_heuristics.py:809\u001b[39m, in \u001b[36mCachingAutotuner.copy_args_to_cpu_if_needed.<locals>.maybe_copy\u001b[39m\u001b[34m(name, arg)\u001b[39m\n\u001b[32m    807\u001b[39m size = arg.numel() * arg.element_size()\n\u001b[32m    808\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size > budget:\n\u001b[32m--> \u001b[39m\u001b[32m809\u001b[39m     cpu_arg = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty_strided\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[43m        \u001b[49m\u001b[43marg\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    811\u001b[39m \u001b[43m        \u001b[49m\u001b[43marg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43marg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    813\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    814\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    815\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    816\u001b[39m     cpu_arg.copy_(arg, non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    817\u001b[39m     copies[name] = (arg, cpu_arg)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºè®­ç»ƒå™¨ï¼Œå¹¶ä¸”ä½¿ç”¨ä¸Šé¢ç»™å‡ºçš„ reward function\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        check_numbers,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = final_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95da640",
   "metadata": {},
   "source": [
    "### æ¨¡å‹æµ‹è¯•\n",
    "#### é»˜è®¤æ¨¡å‹æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72b37c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'system_prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m----> 2\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_prompt},\n\u001b[1;32m      3\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the sqrt of 101?\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      4\u001b[0m ]\n\u001b[1;32m      6\u001b[0m text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[1;32m      7\u001b[0m     messages,\n\u001b[1;32m      8\u001b[0m     add_generation_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# Must add for generation\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     tokenize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextStreamer\n",
      "\u001b[0;31mNameError\u001b[0m: name 'system_prompt' is not defined"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\",   \"content\": \"What is the sqrt of 101?\"},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    tokenize = False,\n",
    ")\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 64, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c833dc17",
   "metadata": {},
   "source": [
    "#### ä¿å­˜ Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855619d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"gemma-3\")  # Local saving\n",
    "tokenizer.save_pretrained(\"gemma-3\")\n",
    "# model.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790cbde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True: # Change to True to save finetune!\n",
    "    model.save_pretrained_merged(\"gemma-3-finetune\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1a0885",
   "metadata": {},
   "source": [
    "### ä¿å­˜ä¸ºå®Œæ•´æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde13d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if False: # Change to True to upload finetune\n",
    "#     model.push_to_hub_merged(\n",
    "#         \"HF_ACCOUNT/gemma-3-finetune\", tokenizer,\n",
    "#         token = \"hf_...\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8bbb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜ä¸º GGUF æ ¼å¼\n",
    "# if False:\n",
    "#     model.save_pretrained_gguf(\n",
    "#         \"gemma-3-finetune\",\n",
    "#         quantization_type = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a89623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if False: # Change to True to upload GGUF\n",
    "#     model.push_to_hub_gguf(\n",
    "#         \"gemma-3-finetune\",\n",
    "#         quantization_type = \"Q8_0\", # Only Q8_0, BF16, F16 supported\n",
    "#         repo_id = \"HF_ACCOUNT/gemma-finetune-gguf\",\n",
    "#         token = \"hf_...\",\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
