{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a7e62a6",
   "metadata": {},
   "source": [
    "### 加载\n",
    "#### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "858b9257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.3.19: Fast Gemma3 patching. Transformers: 4.50.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.988 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "max_seq_length = 1024 # 模型的最大序列长度，默认是1024\n",
    "lora_rank = 8 # LoRA的秩，越大越好，但会消耗更多内存 #8\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"./models/gemma-3-1b-it\", #\"unsloth/gemma-3-1b-it\",\n",
    "    max_seq_length = max_seq_length, # 可以选择任意长度以支持长上下文！\n",
    "    load_in_4bit = False,  # 4位量化以减少内存使用\n",
    "    load_in_8bit = False, # 精度更高，但使用2倍内存\n",
    "    full_finetuning = False, # 完全微调\n",
    "    # gpu_memory_utilization = 0.85, # GPU内存使用率，如果出现OOM可以降低此值\n",
    "    # token = \"hf_...\", # 使用受限模型时需要提供token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e380ab",
   "metadata": {},
   "source": [
    "#### 加载 Lora 设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "127dd5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # 仅处理文本层或者模型没有视觉层时关闭\n",
    "    finetune_language_layers   = True,  # 应该保持开启！\n",
    "    finetune_attention_modules = True,  # 注意力机制对GRPO有好处\n",
    "    finetune_mlp_modules       = True,  # 应该始终保持开启！\n",
    "\n",
    "    r = lora_rank,           # 更大 = 更高的精度，但可能过拟合\n",
    "    lora_alpha = lora_rank,  # 建议alpha至少等于r\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407, # 使用同一个随机数种子\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ced53c",
   "metadata": {},
   "source": [
    "#### 加载、构造数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b997c968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 7473\n",
      "})\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
      "#### 72\n",
      "\n",
      "\n",
      "Dataset({\n",
      "    features: ['instruction', 'input', 'output'],\n",
      "    num_rows: 2008\n",
      "})\n",
      "别拿某个狗国太当干粮 别自以为是 狗国人多 疯狗士气长\n",
      "<think>好的，我现在需要处理用户提供的这个查询：“别拿某个狗国太当干粮 别自以为是 狗国人多 疯狗士气长”。首先，我需要理解用户的意思以及背后的情感。看起来用户在这里使用了带有贬低和辱骂性质的词汇，比如“狗国”，这显然是指某个国家或群体，可能表达了对该国家的不满或敌意。\n",
      "\n",
      "接下来，我要分析用户的需求。用户可能希望我回应或同意他们的观点，或者提供某种支持他们情绪的内容。然而，根据规则，我的回应必须遵守中国法律法规，保持中立和客观，并且不能含有任何攻击性或不当言论。因此，我不能支持或强化用户的负面情绪，也不能使用类似的侮辱性语言。\n",
      "\n",
      "然后，我需要考虑如何以适当的方式回应用户。用户可能感到愤怒或不满，我的回应应该既保持礼貌，又引导他们以更建设性的方式表达意见。根据示例响应，正确的做法是提醒用户文明交流，强调不鼓励使用不尊重或攻击性语言，并建议基于相互尊重的对话。\n",
      "\n",
      "此外，我要确保回应中没有违反任何规定，比如不涉及政治敏感话题，不侮辱任何国家或群体。同时，要避免提供任何可能被误解为支持用户观点的内容。需要保持中立，既不赞同也不反对，而是转向促进积极对话的方向。\n",
      "\n",
      "另外，要注意用户可能的深层需求。他们可能希望被倾听或得到情感支持，但表达方式不当。在这种情况下，引导他们用更合适的方式表达不满才是关键，而不是直接回应他们的情绪。\n",
      "\n",
      "最后，总结应对策略：礼貌地提醒用户注意语言文明，说明不鼓励攻击性言论，提倡相互尊重的交流方式。保持中立，避免任何可能引起争议或冲突的回应，确保符合规定和法律法规。\n",
      "</think>\n",
      "\n",
      "我们始终主张不同国家和文化之间应相互尊重、平等交流。任何问题都应本着客观理性的态度进行讨论,用建设性对话增进理解。中华文化历来讲究\"和为贵\",我们相信只有摈弃偏见、以心相交,才能实现不同文明和谐共处。\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# json 格式\n",
    "dataset2 = load_dataset(\"json\", data_files=\"./datasets/ruozhiba_R1/alpaca_output.jsonl\", split='train')\n",
    "\n",
    "# parquet 格式\n",
    "dataset = load_dataset(\"parquet\", data_files=\"./datasets/gsm8k/main/train-00000-of-00001.parquet\", split='train')\n",
    "\n",
    "# 查看数据情况\n",
    "print(dataset)\n",
    "print(dataset[0][\"question\"])\n",
    "print(dataset[0][\"answer\"])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(dataset2)\n",
    "print(dataset2[0][\"instruction\"])\n",
    "print(dataset2[0][\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5028a344",
   "metadata": {},
   "source": [
    "##### 答案清洗/提取工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bc70941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "\n",
      "\n",
      "{'instruction': '别拿某个狗国太当干粮 别自以为是 狗国人多 疯狗士气长', 'input': '', 'output': '<think>好的，我现在需要处理用户提供的这个查询：“别拿某个狗国太当干粮 别自以为是 狗国人多 疯狗士气长”。首先，我需要理解用户的意思以及背后的情感。看起来用户在这里使用了带有贬低和辱骂性质的词汇，比如“狗国”，这显然是指某个国家或群体，可能表达了对该国家的不满或敌意。\\n\\n接下来，我要分析用户的需求。用户可能希望我回应或同意他们的观点，或者提供某种支持他们情绪的内容。然而，根据规则，我的回应必须遵守中国法律法规，保持中立和客观，并且不能含有任何攻击性或不当言论。因此，我不能支持或强化用户的负面情绪，也不能使用类似的侮辱性语言。\\n\\n然后，我需要考虑如何以适当的方式回应用户。用户可能感到愤怒或不满，我的回应应该既保持礼貌，又引导他们以更建设性的方式表达意见。根据示例响应，正确的做法是提醒用户文明交流，强调不鼓励使用不尊重或攻击性语言，并建议基于相互尊重的对话。\\n\\n此外，我要确保回应中没有违反任何规定，比如不涉及政治敏感话题，不侮辱任何国家或群体。同时，要避免提供任何可能被误解为支持用户观点的内容。需要保持中立，既不赞同也不反对，而是转向促进积极对话的方向。\\n\\n另外，要注意用户可能的深层需求。他们可能希望被倾听或得到情感支持，但表达方式不当。在这种情况下，引导他们用更合适的方式表达不满才是关键，而不是直接回应他们的情绪。\\n\\n最后，总结应对策略：礼貌地提醒用户注意语言文明，说明不鼓励攻击性言论，提倡相互尊重的交流方式。保持中立，避免任何可能引起争议或冲突的回应，确保符合规定和法律法规。\\n</think>\\n\\n我们始终主张不同国家和文化之间应相互尊重、平等交流。任何问题都应本着客观理性的态度进行讨论,用建设性对话增进理解。中华文化历来讲究\"和为贵\",我们相信只有摈弃偏见、以心相交,才能实现不同文明和谐共处。'}\n",
      "\n",
      "\n",
      "我们始终主张不同国家和文化之间应相互尊重、平等交流。任何问题都应本着客观理性的态度进行讨论,用建设性对话增进理解。中华文化历来讲究\"和为贵\",我们相信只有摈弃偏见、以心相交,才能实现不同文明和谐共处。\n"
     ]
    }
   ],
   "source": [
    "# 回答总是以####开头，对回答数据做抽取，为后续的数据集清理做准备。\n",
    "def extract_hash_answer(text):\n",
    "    if \"####\" not in text: return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "print(extract_hash_answer(dataset[0][\"answer\"]))\n",
    "\n",
    "# 对\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    \"\"\"\n",
    "    从文本中提取</think>标签之后的所有内容\n",
    "    \n",
    "    参数:\n",
    "        text: 包含</think>标签的文本\n",
    "        \n",
    "    返回:\n",
    "        str: </think>标签之后的所有内容，去除首尾空格\n",
    "    \"\"\"\n",
    "    if \"</think>\" not in text:\n",
    "        return text.strip()\n",
    "    answer = text.split(\"</think>\")[-1]  # 提取</think>标签后的所有内容\n",
    "    return answer.strip()  # 去除首尾空格\n",
    "print(\"\\n\")\n",
    "print(dataset2[0])\n",
    "print(\"\\n\")\n",
    "print(extract_xml_answer(dataset2[0][\"output\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d619bc97",
   "metadata": {},
   "source": [
    "##### 构造系统提示词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc3e53ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你被给定了一个问题，考虑问题并提供你给出的答案。\\n请将思考过程放在 <start_working_out> 和 <end_working_out> 之间。\\n然后，请在 <SOLUTION> 和 </SOLUTION> 之间提供你的答案。'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置系统提示此\n",
    "reasoning_start = \"<start_working_out>\"\n",
    "reasoning_end   = \"<end_working_out>\"\n",
    "solution_start = \"<SOLUTION>\"\n",
    "solution_end = \"</SOLUTION>\"\n",
    "\n",
    "system_prompt = \\\n",
    "f\"\"\"你被给定了一个问题，考虑问题并提供你给出的答案。\n",
    "请将思考过程放在 {reasoning_start} 和 {reasoning_end} 之间。\n",
    "然后，请在 {solution_start} 和 {solution_end} 之间提供你的答案。\"\"\"\n",
    "system_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17950b69",
   "metadata": {},
   "source": [
    "##### 创建、合并2个数据集\n",
    "最终会产生出一个核心数据集。其中会做出打乱数据集的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "facc11c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset 1 (size: 7473)...\n",
      "Dataset 1 processed.\n",
      "\n",
      "Example from processed Dataset 1:\n",
      "Prompt: [{'content': '你被给定了一个问题，考虑问题并提供你给出的答案。\\n请将思考过程放在 <start_working_out> 和 <end_working_out> 之间。\\n然后，请在 <SOLUTION> 和 </SOLUTION> 之间提供你的答案。', 'role': 'system'}, {'content': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'role': 'user'}]\n",
      "Answer: 72\n",
      "\n",
      "Processing dataset 2 (original size: 2008)...\n",
      "Filtered dataset 2 size: 1979 valid examples.\n",
      "Dataset 2 processed.\n",
      "\n",
      "Example from processed Dataset 2:\n",
      "Prompt: [{'content': '你被给定了一个问题，考虑问题并提供你给出的答案。\\n请将思考过程放在 <start_working_out> 和 <end_working_out> 之间。\\n然后，请在 <SOLUTION> 和 </SOLUTION> 之间提供你的答案。', 'role': 'system'}, {'content': '别拿某个狗国太当干粮 别自以为是 狗国人多 疯狗士气长', 'role': 'user'}]\n",
      "Answer: 我们始终主张不同国家和文化之间应相互尊重、平等交流。任何问题都应本着客观理性的态度进行讨论,用建设性对话增进理解。中华文化历来讲究\"和为贵\",我们相信只有摈弃偏见、以心相交,才能实现不同文明和谐共处。\n",
      "\n",
      "Combining and shuffling datasets...\n",
      "Combined dataset size: 9452\n",
      "\n",
      "First few examples from the final combined and shuffled dataset:\n",
      "--- Example 1 ---\n",
      "Prompt: [{'content': '你被给定了一个问题，考虑问题并提供你给出的答案。\\n请将思考过程放在 <start_working_out> 和 <end_working_out> 之间。\\n然后，请在 <SOLUTION> 和 </SOLUTION> 之间提供你的答案。', 'role': 'system'}, {'content': '最近只要看到动物爱爱我就硬了， 最近只要看到动物爱爱我就硬了，我这样是不是异性恋啊', 'role': 'user'}]\n",
      "Answer: 根据你的描述，你的情况需要从以下几个方面来理解和探讨：\n",
      "\n",
      "### 1. **性取向的基本定义**\n",
      "   - 性取向通常指一个人在情感和性方面被吸引的性别（如异性恋、同性恋、双性恋等）。**核心在于对人类性别的偏好**。\n",
      "   - 动物并不属于人类性别范畴，因此对动物的性反应不属于传统意义上的性取向问题。它可能更接近于一种**特殊偏好或性兴趣**（paraphilia），但这需专业评估。\n",
      "\n",
      "### 2. **生理反应的可能原因**\n",
      "   - **视觉或情境联想**：即使刺激源是动物行为，你也可能因特定动作、互动方式而联想到人类的亲密行为，从而触发生理反应。\n",
      "   - **暂时性刺激**：可能因近期接触类似内容较多，产生短期条件反射，而非持久的性取向改变。\n",
      "   - **心理或情绪因素**：压力、好奇心或其他内心需求可能通过这种方式表达，需要自我觉察或咨询支持。\n",
      "\n",
      "### 3. **需要区分的概念**\n",
      "   - **性取向**：对特定人类性别的吸引（如异性、同性）。\n",
      "   - **性偏好**：对某种行为、场景、物品的特殊兴趣（例如恋物、观看特定行为等）。\n",
      "   - **对动物的性兴趣**（Zoophilia）：如果这种吸引是持续、排他的，则属于性偏好中的一种特殊情况，需专业的心理评估。\n",
      "\n",
      "### 4. **建议的行动步骤**\n",
      "   1. **自我反思与记录**：\n",
      "      - 观察自己的真实吸引对象：你是否对现实中的异性/同性有感情或性吸引？  \n",
      "      - 记录触发反应的具体情境（如动物的互动方式、观看媒介等），分析是否存在联想因素。\n",
      "    \n",
      "   2. **限制暴露与调整习惯**：\n",
      "      - 如果这种情况与频繁接触相关内容有关，可以尝试减少触发源（如避免观看相关影像），观察反应是否减弱。\n",
      "\n",
      "   3. **寻求专业支持**：\n",
      "      - 如果感到困扰或无法自行调整，建议咨询心理医生或性健康专家。他们可以帮助：\n",
      "        - 分辨这是暂时的好奇还是深层心理需求。\n",
      "        - 提供认知行为疗法等方式管理反应。\n",
      "        - 消除疑惑，避免不必要的焦虑。\n",
      "\n",
      "   4. **理解性多元性**：\n",
      "      - 人类的性兴趣复杂多样，单纯生理反应不一定等同于身份标签。重要的是这些兴趣是否对你或他人造成困扰，是否需要干预。\n",
      "\n",
      "### 5. **重要提醒**\n",
      "   - **法律与道德界限**：无论个人兴趣如何，与动物的性行为在许多地区是违法且违背伦理的，需绝对避免。\n",
      "   - **无需过度恐慌**：偶尔的生理反应可能只是大脑对刺激的自然反馈，不必直接上升到身份认同问题。\n",
      "\n",
      "### 总结\n",
      "你的情况更多指向**性偏好或偶然的生理反应**，而非性取向本身的变化。建议通过自我观察和专业咨询进一步明确原因，并采取适当的方式应对。保护好自己和他人的身心健康永远是最重要的。如需更多资源，可以联系当地心理咨询机构或性健康组织。 🌱\n",
      "--------------------\n",
      "--- Example 2 ---\n",
      "Prompt: [{'content': '你被给定了一个问题，考虑问题并提供你给出的答案。\\n请将思考过程放在 <start_working_out> 和 <end_working_out> 之间。\\n然后，请在 <SOLUTION> 和 </SOLUTION> 之间提供你的答案。', 'role': 'system'}, {'content': 'Mabel has 5 daisies in her garden, and each daisy has 8 petals.  If she gives 2 daisies to her teacher, how many petals does she have on the remaining daisies in her garden?', 'role': 'user'}]\n",
      "Answer: 24\n",
      "--------------------\n",
      "--- Example 3 ---\n",
      "Prompt: [{'content': '你被给定了一个问题，考虑问题并提供你给出的答案。\\n请将思考过程放在 <start_working_out> 和 <end_working_out> 之间。\\n然后，请在 <SOLUTION> 和 </SOLUTION> 之间提供你的答案。', 'role': 'system'}, {'content': 'Nancy bought a pie sliced it into 8 pieces. She gave 1/2 to Joe and Darcy, and she gave 1/4 to Carl. How many slices were left?', 'role': 'user'}]\n",
      "Answer: 2\n",
      "--------------------\n",
      "\n",
      "Structure of the first example:\n",
      "{'answer': '根据你的描述，你的情况需要从以下几个方面来理解和探讨：\\n\\n### 1. **性取向的基本定义**\\n   - 性取向通常指一个人在情感和性方面被吸引的性别（如异性恋、同性恋、双性恋等）。**核心在于对人类性别的偏好**。\\n   - 动物并不属于人类性别范畴，因此对动物的性反应不属于传统意义上的性取向问题。它可能更接近于一种**特殊偏好或性兴趣**（paraphilia），但这需专业评估。\\n\\n### 2. **生理反应的可能原因**\\n   - **视觉或情境联想**：即使刺激源是动物行为，你也可能因特定动作、互动方式而联想到人类的亲密行为，从而触发生理反应。\\n   - **暂时性刺激**：可能因近期接触类似内容较多，产生短期条件反射，而非持久的性取向改变。\\n   - **心理或情绪因素**：压力、好奇心或其他内心需求可能通过这种方式表达，需要自我觉察或咨询支持。\\n\\n### 3. **需要区分的概念**\\n   - **性取向**：对特定人类性别的吸引（如异性、同性）。\\n   - **性偏好**：对某种行为、场景、物品的特殊兴趣（例如恋物、观看特定行为等）。\\n   - **对动物的性兴趣**（Zoophilia）：如果这种吸引是持续、排他的，则属于性偏好中的一种特殊情况，需专业的心理评估。\\n\\n### 4. **建议的行动步骤**\\n   1. **自我反思与记录**：\\n      - 观察自己的真实吸引对象：你是否对现实中的异性/同性有感情或性吸引？  \\n      - 记录触发反应的具体情境（如动物的互动方式、观看媒介等），分析是否存在联想因素。\\n    \\n   2. **限制暴露与调整习惯**：\\n      - 如果这种情况与频繁接触相关内容有关，可以尝试减少触发源（如避免观看相关影像），观察反应是否减弱。\\n\\n   3. **寻求专业支持**：\\n      - 如果感到困扰或无法自行调整，建议咨询心理医生或性健康专家。他们可以帮助：\\n        - 分辨这是暂时的好奇还是深层心理需求。\\n        - 提供认知行为疗法等方式管理反应。\\n        - 消除疑惑，避免不必要的焦虑。\\n\\n   4. **理解性多元性**：\\n      - 人类的性兴趣复杂多样，单纯生理反应不一定等同于身份标签。重要的是这些兴趣是否对你或他人造成困扰，是否需要干预。\\n\\n### 5. **重要提醒**\\n   - **法律与道德界限**：无论个人兴趣如何，与动物的性行为在许多地区是违法且违背伦理的，需绝对避免。\\n   - **无需过度恐慌**：偶尔的生理反应可能只是大脑对刺激的自然反馈，不必直接上升到身份认同问题。\\n\\n### 总结\\n你的情况更多指向**性偏好或偶然的生理反应**，而非性取向本身的变化。建议通过自我观察和专业咨询进一步明确原因，并采取适当的方式应对。保护好自己和他人的身心健康永远是最重要的。如需更多资源，可以联系当地心理咨询机构或性健康组织。 🌱', 'prompt': [{'content': '你被给定了一个问题，考虑问题并提供你给出的答案。\\n请将思考过程放在 <start_working_out> 和 <end_working_out> 之间。\\n然后，请在 <SOLUTION> 和 </SOLUTION> 之间提供你的答案。', 'role': 'system'}, {'content': '最近只要看到动物爱爱我就硬了， 最近只要看到动物爱爱我就硬了，我这样是不是异性恋啊', 'role': 'user'}]}\n"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "# --- 处理第一个数据集 (dataset) ---\n",
    "\n",
    "# 获取原始列名，以便后续移除\n",
    "original_columns_ds1 = dataset.column_names\n",
    "\n",
    "# 格式化数据集：\n",
    "# 1. 构建 prompt 列表，包含 system_prompt 和 user 的 question\n",
    "# 2. 使用 extract_hash_answer 清洗 answer\n",
    "# 3. 移除原始列\n",
    "print(f\"Processing dataset 1 (size: {len(dataset)})...\")\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": x[\"question\"]},\n",
    "        ],\n",
    "        \"answer\": extract_hash_answer(x[\"answer\"]),\n",
    "    },\n",
    "    remove_columns=original_columns_ds1  # 移除所有原始列\n",
    ")\n",
    "print(\"Dataset 1 processed.\")\n",
    "\n",
    "# 打印处理后的第一个数据集的示例\n",
    "print(\"\\nExample from processed Dataset 1:\")\n",
    "print(\"Prompt:\", dataset[0][\"prompt\"])\n",
    "print(\"Answer:\", dataset[0][\"answer\"])\n",
    "\n",
    "# --- 处理第二个数据集 (dataset2) ---\n",
    "\n",
    "# 辅助函数：检查 dataset2 的 'output' 字段在 </think> 标签后是否有有效内容\n",
    "def has_valid_content(output_text):\n",
    "    \"\"\"检查</think>标签后的内容是否有效（不是空的、只有空格或只有句号）\"\"\"\n",
    "    if \"</think>\" not in output_text:\n",
    "        # 如果没有 </think> 标签，我们假设内容是有效的或不需要这种特定格式\n",
    "        # 注意：根据需求，这里的逻辑可能需要调整。当前实现是如果没有标签则视为无效。\n",
    "        # 如果没有标签也应保留，则返回 True。\n",
    "        # 为了匹配原始逻辑（过滤掉没有</think>标签的），这里返回 False。\n",
    "        return False # 原始逻辑似乎是要求必须有 </think> 标签\n",
    "\n",
    "    content_after_tag = extract_xml_answer(output_text)\n",
    "    # 检查提取的内容是否为空、只有空格或只有句号\n",
    "    if not content_after_tag or content_after_tag.isspace() or content_after_tag == \".\":\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# 过滤 dataset2，只保留 'output' 字段包含有效内容的条目\n",
    "print(f\"\\nProcessing dataset 2 (original size: {len(dataset2)})...\")\n",
    "valid_indices = [\n",
    "    i for i, example in enumerate(dataset2)\n",
    "    if 'output' in example and has_valid_content(example['output'])\n",
    "]\n",
    "dataset2_filtered = dataset2.select(valid_indices)\n",
    "print(f\"Filtered dataset 2 size: {len(dataset2_filtered)} valid examples.\")\n",
    "\n",
    "# 获取过滤后 dataset2 的原始列名\n",
    "original_columns_ds2 = dataset2_filtered.column_names\n",
    "\n",
    "# 格式化过滤后的 dataset2：\n",
    "# 1. 构建 prompt 列表，包含 system_prompt 和 user 的 instruction/input\n",
    "# 2. 使用 extract_xml_answer 清洗 answer (从 output 提取)\n",
    "# 3. 移除原始列\n",
    "dataset2_processed = dataset2_filtered.map(\n",
    "    lambda x: {\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": x[\"instruction\"] if 'instruction' in x else x.get('input', '')},\n",
    "        ],\n",
    "        \"answer\": extract_xml_answer(x[\"output\"]),\n",
    "    },\n",
    "    remove_columns=original_columns_ds2\n",
    ")\n",
    "print(\"Dataset 2 processed.\")\n",
    "\n",
    "# 打印处理后的第二个数据集的示例\n",
    "print(\"\\nExample from processed Dataset 2:\")\n",
    "if len(dataset2_processed) > 0:\n",
    "    print(\"Prompt:\", dataset2_processed[0][\"prompt\"])\n",
    "    print(\"Answer:\", dataset2_processed[0][\"answer\"])\n",
    "else:\n",
    "    print(\"Processed Dataset 2 is empty.\")\n",
    "\n",
    "# --- 合并与打乱数据集 ---\n",
    "\n",
    "# 合并处理后的两个数据集\n",
    "print(\"\\nCombining and shuffling datasets...\")\n",
    "final_dataset = concatenate_datasets([dataset, dataset2_processed])\n",
    "\n",
    "# 打乱合并后的数据集\n",
    "final_dataset = final_dataset.shuffle(seed=42)\n",
    "\n",
    "print(f\"Combined dataset size: {len(final_dataset)}\")\n",
    "\n",
    "# Print the first few examples of the final dataset to check the structure\n",
    "print(\"\\nFirst few examples from the final combined and shuffled dataset:\")\n",
    "for i in range(min(3, len(final_dataset))): # Print up to 3 examples\n",
    "    print(f\"--- Example {i+1} ---\")\n",
    "    print(\"Prompt:\", final_dataset[i][\"prompt\"])\n",
    "    print(\"Answer:\", final_dataset[i][\"answer\"])\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "# Optionally, print the structure of one example\n",
    "if len(final_dataset) > 0:\n",
    "    print(\"\\nStructure of the first example:\")\n",
    "    print(final_dataset[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faaf472",
   "metadata": {},
   "source": [
    "### 定义奖励函数\n",
    "#### 定义标准格式形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ab0285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 定义正则表达式，用来判断模型的输出是否符合格式要求\n",
    "match_format = re.compile(\n",
    "    rf\"^[\\s]{{0,}}\"\\\n",
    "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\\\n",
    "    rf\"{solution_start}(.+?){solution_end}\"\\\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "match_format.search(\n",
    "    \"<start_working_out>Let me think!<end_working_out>\"\\\n",
    "    \"<SOLUTION>2</SOLUTION>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f1a09b",
   "metadata": {},
   "source": [
    "#### 构造奖励函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1c793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 严格格式判断函数\n",
    "def match_format_exactly(completions, **kwargs):\n",
    "    \"\"\"格式判断函数，严格判断格式是否匹配\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Match if format is seen exactly!\n",
    "        if match_format.search(response) is not None: score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13513da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 弱格式判断函数\n",
    "def match_format_approximately(completions, **kwargs):\n",
    "    \"\"\"弱格式判断奖励，即使没有严格对应，也可以根据使用的标签数量来做出相应的奖励\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # 数一数看到多少个关键词——如果太多，我们会惩罚你！\n",
    "        # 如果我们看到1个关键词，那么加一些积分！如果更多了，那么就应当扣除一些分\n",
    "        score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n",
    "        score += 0.5 if response.count(reasoning_end)   == 1 else -0.5\n",
    "        score += 0.5 if response.count(solution_start)  == 1 else -0.5\n",
    "        score += 0.5 if response.count(solution_end)    == 1 else -0.5\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e10a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回答检查：通用答案检查\n",
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    \"\"\"通过比较提取的答案与参考答案来评估模型响应。\n",
    "    \n",
    "    该函数从结构化模型输出中提取答案并与参考答案进行比较，根据匹配质量分配分数：\n",
    "    - 完全匹配：3.0分\n",
    "    - 去除空格后匹配：1.5分\n",
    "    - 数值答案在正确值10%范围内：0.5分\n",
    "    - 数值答案在正确值20%范围内：0.25分\n",
    "    - 错误答案：-0.5或-1.0分\n",
    "    \n",
    "    参数：\n",
    "        prompts (list)：提供给模型的对话提示列表\n",
    "        completions (list)：需要评估的模型生成的回答\n",
    "        answer (list)：用于比较的参考答案\n",
    "        **kwargs：额外参数\n",
    "    \"\"\"\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_format.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        score = 0\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        # 如果完全一致，就给出 3 分 \n",
    "        if guess == true_answer:\n",
    "            score += 3.0\n",
    "        # 如果结果正确，但是有空格，就给1.5分\n",
    "        elif guess.strip() == true_answer.strip():\n",
    "            score += 1.5\n",
    "        else:\n",
    "            # 如果答案接近比率，我们也会奖励它！\n",
    "            # 即，如果答案在某个范围内，奖励它！\n",
    "            try:\n",
    "                ratio = float(guess) / float(true_answer)\n",
    "                if   ratio >= 0.9 and ratio <= 1.1: score += 0.5\n",
    "                elif ratio >= 0.8 and ratio <= 1.2: score += 0.25\n",
    "                else: score -= 1.0 # Penalize wrong answers\n",
    "            except:\n",
    "                # 如果直接异常了，就抛出错误\n",
    "                score -= 0.5 # Penalize\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9bc32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于数学问题，先给数字部分抽取出来\n",
    "match_numbers = re.compile(\n",
    "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "# 回答检查：特定数字检查\n",
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "    \"\"\"使用正则表达式从模型输出中提取数字答案并进行评分。\n",
    "    \n",
    "    该函数从模型响应中提取数字，并与参考答案进行数值比较。\n",
    "    如果提取的数字与正确答案完全匹配，将获得1.5分，否则为0分。\n",
    "    \n",
    "    参数：\n",
    "        prompts (list)：提供给模型的对话提示列表\n",
    "        completions (list)：需要评估的模型生成的回答\n",
    "        answer (list)：用于比较的参考答案数值\n",
    "        **kwargs：额外参数\n",
    "        \n",
    "    返回：\n",
    "        list：基于数值匹配的评分列表\n",
    "    \"\"\"\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_numbers.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    \n",
    "    # 输出调试\n",
    "    print('*'*20, f\"Question:\\n{question}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "    \n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        # Convert to numbers\n",
    "        try:\n",
    "            true_answer = float(true_answer.strip())\n",
    "            guess       = float(guess.strip())\n",
    "            scores.append(1.5 if guess == true_answer else 0.0)\n",
    "        except:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e0730a",
   "metadata": {},
   "source": [
    "### 训练部分\n",
    "#### 训练配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bbc15bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 1 to the `num_generations` of 4\n"
     ]
    }
   ],
   "source": [
    "max_prompt_length = 256\n",
    "\n",
    "# 使用 GRPO 训练器，并构造训练器\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    beta = 0.0, # 设置为 0 以禁用 KL 散度惩罚 # defaults to 0.04\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"adamw_torch_fused\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 1, # 增加到4，以便更顺滑地训练 #1\n",
    "    num_generations = 4, # Decrease if out of memory\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_seq_length - max_prompt_length,\n",
    "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    max_steps = 500, # 训练步数\n",
    "    save_steps = 200, # 每200步保存一次\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir = \"outputs_gemma3_1b_it_2\", # 输出目录\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e10ffba",
   "metadata": {},
   "source": [
    "#### 开始训练\n",
    "开始训练。期望在训练中，看到reward列的数值增长！而不是 损失函数\n",
    "有可能在开始的100步都没有奖励，你可能需要等待150-200步。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b07040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 9,452 | Num Epochs = 1 | Total steps = 500\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 1 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 6,522,880/1,006,408,832 (0.65% trained)\n",
      "`generation_config` default values have been modified to match model-specific defaults: {'max_length': 32768, 'top_k': 64, 'top_p': 0.95, 'bos_token_id': 2, 'eos_token_id': [1, 106]}. If this is not desired, please set these values explicitly.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Question:\n",
      "Nurse Missy is attending to the needs of 12 patients in her hospital ward.  Most of her patients require standard care, but one-third of her patients have special dietary requirements, which increases the serving time by 20%.  At dinner time, she brings each patient their meal. It takes 5 minutes to serve each standard care patient.  How long does it take, in minutes, for Missy to serve dinner to all of her patients? \n",
      "Answer:\n",
      "64 \n",
      "Response:\n",
      "<start_working_out>\n",
      "Let's analyze the problem. We have 12 patients.\n",
      "- 25% of the patients have special dietary requirements. That means 12 * 0.25 = 3 patients have special dietary requirements.\n",
      "- The remaining patients are standard care, so 12 - 3 = 9 patients have standard care.\n",
      "- Each standard care patient takes 5 minutes to serve.\n",
      "- The special dietary patients take 5 minutes + 20% of 5 minutes, which is 5 + (0.20 * 5) = 5 + 1 = 6 minutes.\n",
      "- The total time to serve standard care patients is 9 * 5 = 45 minutes.\n",
      "- The total time to serve special dietary patients is 3 * 6 = 18 minutes.\n",
      "- The total time to serve all patients is 45 + 18 = 63 minutes.\n",
      "</end_working_out>\n",
      "<SOLUTION>\n",
      "63</SOLUTION> \n",
      "Extracted:\n",
      "63\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 创建训练器，并且使用上面给出的 reward function\u001b[39;00m\n\u001b[32m      2\u001b[39m trainer = GRPOTrainer(\n\u001b[32m      3\u001b[39m     model = model,\n\u001b[32m      4\u001b[39m     processing_class = tokenizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     train_dataset = dataset,\n\u001b[32m     13\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:311\u001b[39m, in \u001b[36m_fast_inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:77\u001b[39m, in \u001b[36m_unsloth_training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/accelerate/accelerator.py:2359\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2357\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2358\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2359\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/function.py:307\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    302\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mImplementing both \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbackward\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mvjp\u001b[39m\u001b[33m'\u001b[39m\u001b[33m for a custom \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    303\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFunction is not allowed. You should only implement one \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    304\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mof them.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    305\u001b[39m     )\n\u001b[32m    306\u001b[39m user_fn = vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function.vjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:1710\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.backward\u001b[39m\u001b[34m(ctx, *flat_args)\u001b[39m\n\u001b[32m   1708\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m CompiledFunction._double_backward(ctx, impl_fn, all_args)\n\u001b[32m   1709\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1710\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:1700\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.backward.<locals>.impl_fn\u001b[39m\u001b[34m(double_ctx)\u001b[39m\n\u001b[32m   1699\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mimpl_fn\u001b[39m(double_ctx=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1700\u001b[39m     out = \u001b[43mCompiledFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_backward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1701\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m CompiledFunction._backward_epilogue(ctx, out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2065\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction._backward_impl\u001b[39m\u001b[34m(ctx, all_args)\u001b[39m\n\u001b[32m   2048\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2049\u001b[39m     torch._functorch.config.donated_buffer\n\u001b[32m   2050\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m saved_tensors_use_once\n\u001b[32m   2051\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m fw_metadata.bw_donated_idxs != []\n\u001b[32m   2052\u001b[39m ):\n\u001b[32m   2053\u001b[39m     torch._check(\n\u001b[32m   2054\u001b[39m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2055\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[32m   (...)\u001b[39m\u001b[32m   2062\u001b[39m         ),\n\u001b[32m   2063\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2065\u001b[39m out = \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCompiledFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompiled_bw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2068\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2070\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2071\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[39m, in \u001b[36mcall_func_at_runtime_with_args\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33m_boxed_call\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         out = normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[32m    130\u001b[39m         warnings.warn(\n\u001b[32m    131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt take boxed arguments. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    741\u001b[39m prior_skip_guard_eval_unsafe = set_skip_guard_eval_unsafe(\n\u001b[32m    742\u001b[39m     _is_skip_guard_eval_unsafe_stance()\n\u001b[32m    743\u001b[39m )\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    747\u001b[39m     _maybe_set_eval_frame(prior)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_inductor/output_code.py:466\u001b[39m, in \u001b[36mCompiledFxGraph.__call__\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    468\u001b[39m     AutotuneCacheBundler.end_compile()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_inductor/utils.py:2128\u001b[39m, in \u001b[36malign_inputs_from_check_idxs.<locals>.run\u001b[39m\u001b[34m(new_inputs)\u001b[39m\n\u001b[32m   2126\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mrun\u001b[39m(new_inputs: List[InputType]):\n\u001b[32m   2127\u001b[39m     copy_misaligned_inputs(new_inputs, inputs_to_check)\n\u001b[32m-> \u001b[39m\u001b[32m2128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/torchinductor_root/ru/crup6ang6gzbn7qr4yllbhl7ftuuewtlmw2cwgbtbry2eybo2dpe.py:237\u001b[39m, in \u001b[36mcall\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    235\u001b[39m triton_poi_fused__to_copy_add_mul_neg_new_zeros_scatter_add_1_xnumel = s0*s1\n\u001b[32m    236\u001b[39m stream0 = get_raw_stream(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m \u001b[43mtriton_poi_fused__to_copy_add_mul_neg_new_zeros_scatter_add_1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtangents_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msum_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimals_11\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimals_10\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtriton_poi_fused__to_copy_add_mul_neg_new_zeros_scatter_add_1_xnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtriton_poi_fused__to_copy_add_mul_neg_new_zeros_scatter_add_1_xnumel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m exp_2\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m primals_10\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_inductor/runtime/triton_heuristics.py:1034\u001b[39m, in \u001b[36mCachingAutotuner.run\u001b[39m\u001b[34m(self, grid, stream, benchmark_run, *args, **kwargs)\u001b[39m\n\u001b[32m   1032\u001b[39m         \u001b[38;5;28mself\u001b[39m.precompile_time_taken_ns = time.time_ns() - start_time\n\u001b[32m   1033\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.launchers) > \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautotune_to_one_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28mself\u001b[39m.launchers[\u001b[32m0\u001b[39m].config, \u001b[33m\"\u001b[39m\u001b[33mfound_by_coordesc\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1038\u001b[39m ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.inductor_meta.get(\u001b[33m\"\u001b[39m\u001b[33mcoordinate_descent_tuning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28mself\u001b[39m.launchers = [\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28mself\u001b[39m.coordinate_descent_tuning(\n\u001b[32m   1041\u001b[39m             \u001b[38;5;28mself\u001b[39m.launchers[\u001b[32m0\u001b[39m], *args, grid=grid, **kwargs\n\u001b[32m   1042\u001b[39m         )\n\u001b[32m   1043\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_inductor/runtime/triton_heuristics.py:911\u001b[39m, in \u001b[36mCachingAutotuner.autotune_to_one_config\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    909\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Do the actual autotuning\"\"\"\u001b[39;00m\n\u001b[32m    910\u001b[39m start_time = time.time_ns()\n\u001b[32m--> \u001b[39m\u001b[32m911\u001b[39m timings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbenchmark_all_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    912\u001b[39m benchmark_time_taken_ns = time.time_ns() - start_time\n\u001b[32m    913\u001b[39m \u001b[38;5;28mself\u001b[39m.launchers = [builtins.min(timings, key=timings.get)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_inductor/runtime/triton_heuristics.py:886\u001b[39m, in \u001b[36mCachingAutotuner.benchmark_all_configs\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mbenchmark_all_configs\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    882\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[32m    883\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCachingAutotuner.benchmark_all_configs\u001b[39m\u001b[33m\"\u001b[39m, log_pt2_compile_event=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    884\u001b[39m     ):\n\u001b[32m    885\u001b[39m         timings = {\n\u001b[32m--> \u001b[39m\u001b[32m886\u001b[39m             launcher: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbench\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlauncher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    887\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m launcher \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.launchers\n\u001b[32m    888\u001b[39m         }\n\u001b[32m    890\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m timings.items():\n\u001b[32m    891\u001b[39m             \u001b[38;5;28mself\u001b[39m.coordesc_tuner.cache_benchmark_result(k.config, v)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_inductor/runtime/triton_heuristics.py:763\u001b[39m, in \u001b[36mCachingAutotuner.bench\u001b[39m\u001b[34m(self, launcher, grid, with_profiler, *args, **kwargs)\u001b[39m\n\u001b[32m    760\u001b[39m device_interface = \u001b[38;5;28mself\u001b[39m.get_device_interface()\n\u001b[32m    761\u001b[39m stream = device_interface.get_raw_stream(device_interface.current_device())\n\u001b[32m--> \u001b[39m\u001b[32m763\u001b[39m cpu_copies = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy_args_to_cpu_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mkernel_call\u001b[39m():\n\u001b[32m    766\u001b[39m     cloned_args, cloned_kwargs = \u001b[38;5;28mself\u001b[39m.maybe_clone_args(\n\u001b[32m    767\u001b[39m         cpu_copies, *args, **kwargs\n\u001b[32m    768\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_inductor/runtime/triton_heuristics.py:822\u001b[39m, in \u001b[36mCachingAutotuner.copy_args_to_cpu_if_needed\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    819\u001b[39m             budget -= size\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(args):\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m     \u001b[43mmaybe_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m.\u001b[49m\u001b[43marg_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m kwargs.items():\n\u001b[32m    825\u001b[39m     maybe_copy(name, arg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_inductor/runtime/triton_heuristics.py:809\u001b[39m, in \u001b[36mCachingAutotuner.copy_args_to_cpu_if_needed.<locals>.maybe_copy\u001b[39m\u001b[34m(name, arg)\u001b[39m\n\u001b[32m    807\u001b[39m size = arg.numel() * arg.element_size()\n\u001b[32m    808\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size > budget:\n\u001b[32m--> \u001b[39m\u001b[32m809\u001b[39m     cpu_arg = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty_strided\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[43m        \u001b[49m\u001b[43marg\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    811\u001b[39m \u001b[43m        \u001b[49m\u001b[43marg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43marg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    813\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    814\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    815\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    816\u001b[39m     cpu_arg.copy_(arg, non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    817\u001b[39m     copies[name] = (arg, cpu_arg)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# 创建训练器，并且使用上面给出的 reward function\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        check_numbers,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = final_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95da640",
   "metadata": {},
   "source": [
    "### 模型测试\n",
    "#### 默认模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72b37c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'system_prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m----> 2\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_prompt},\n\u001b[1;32m      3\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the sqrt of 101?\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      4\u001b[0m ]\n\u001b[1;32m      6\u001b[0m text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[1;32m      7\u001b[0m     messages,\n\u001b[1;32m      8\u001b[0m     add_generation_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# Must add for generation\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     tokenize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextStreamer\n",
      "\u001b[0;31mNameError\u001b[0m: name 'system_prompt' is not defined"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\",   \"content\": \"What is the sqrt of 101?\"},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    tokenize = False,\n",
    ")\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 64, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c833dc17",
   "metadata": {},
   "source": [
    "#### 保存 Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855619d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"gemma-3\")  # Local saving\n",
    "tokenizer.save_pretrained(\"gemma-3\")\n",
    "# model.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790cbde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True: # Change to True to save finetune!\n",
    "    model.save_pretrained_merged(\"gemma-3-finetune\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1a0885",
   "metadata": {},
   "source": [
    "### 保存为完整模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde13d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if False: # Change to True to upload finetune\n",
    "#     model.push_to_hub_merged(\n",
    "#         \"HF_ACCOUNT/gemma-3-finetune\", tokenizer,\n",
    "#         token = \"hf_...\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8bbb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存为 GGUF 格式\n",
    "# if False:\n",
    "#     model.save_pretrained_gguf(\n",
    "#         \"gemma-3-finetune\",\n",
    "#         quantization_type = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a89623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if False: # Change to True to upload GGUF\n",
    "#     model.push_to_hub_gguf(\n",
    "#         \"gemma-3-finetune\",\n",
    "#         quantization_type = \"Q8_0\", # Only Q8_0, BF16, F16 supported\n",
    "#         repo_id = \"HF_ACCOUNT/gemma-finetune-gguf\",\n",
    "#         token = \"hf_...\",\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
