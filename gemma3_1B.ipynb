{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a7e62a6",
   "metadata": {},
   "source": [
    "### å¼•ç”¨æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "858b9257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 03-21 16:41:55 [__init__.py:256] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.3.17: Fast Gemma3 patching. Transformers: 4.50.0.dev0. vLLM: 0.8.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 Ti SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "max_seq_length = 1024 # æ¨¡å‹çš„æœ€å¤§åºåˆ—é•¿åº¦ï¼Œé»˜è®¤æ˜¯1024\n",
    "lora_rank = 8 # LoRAçš„ç§©ï¼Œè¶Šå¤§è¶Šå¥½ï¼Œä½†ä¼šæ¶ˆè€—æ›´å¤šå†…å­˜ #8\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"./models/gemma-3-1b-it\", #\"unsloth/gemma-3-1b-it\",\n",
    "    max_seq_length = max_seq_length, # å¯ä»¥é€‰æ‹©ä»»æ„é•¿åº¦ä»¥æ”¯æŒé•¿ä¸Šä¸‹æ–‡ï¼\n",
    "    load_in_4bit = False,  # 4ä½é‡åŒ–ä»¥å‡å°‘å†…å­˜ä½¿ç”¨\n",
    "    load_in_8bit = False, # ç²¾åº¦æ›´é«˜ï¼Œä½†ä½¿ç”¨2å€å†…å­˜\n",
    "    full_finetuning = False, # ç°åœ¨æˆ‘ä»¬æ”¯æŒå®Œå…¨å¾®è°ƒäº†ï¼\n",
    "    # gpu_memory_utilization = 0.85, # GPUå†…å­˜ä½¿ç”¨ç‡ï¼Œå¦‚æœå‡ºç°OOMå¯ä»¥é™ä½æ­¤å€¼\n",
    "    # token = \"hf_...\", # ä½¿ç”¨å—é™æ¨¡å‹æ—¶éœ€è¦æä¾›token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e380ab",
   "metadata": {},
   "source": [
    "### åŠ è½½ Lora è®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "127dd5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # ä»…å¤„ç†æ–‡æœ¬å±‚æˆ–è€…æ¨¡å‹æ²¡æœ‰è§†è§‰å±‚æ—¶å…³é—­ï¼\n",
    "    finetune_language_layers   = True,  # åº”è¯¥ä¿æŒå¼€å¯ï¼\n",
    "    finetune_attention_modules = True,  # æ³¨æ„åŠ›æœºåˆ¶å¯¹GRPOæœ‰å¥½å¤„\n",
    "    finetune_mlp_modules       = True,  # åº”è¯¥å§‹ç»ˆä¿æŒå¼€å¯ï¼\n",
    "\n",
    "    r = lora_rank,           # æ›´å¤§ = æ›´é«˜çš„ç²¾åº¦ï¼Œä½†å¯èƒ½è¿‡æ‹Ÿåˆ\n",
    "    lora_alpha = lora_rank,  # å»ºè®®alphaè‡³å°‘ç­‰äºr\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407, # ä½¿ç”¨åŒä¸€ä¸ªéšæœºæ•°ç§å­\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ced53c",
   "metadata": {},
   "source": [
    "### åŠ è½½æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b997c968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 7473\n",
      "})\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
      "#### 72\n",
      "Dataset({\n",
      "    features: ['instruction', 'input', 'output'],\n",
      "    num_rows: 2008\n",
      "})\n",
      "åˆ«æ‹¿æŸä¸ªç‹—å›½å¤ªå½“å¹²ç²® åˆ«è‡ªä»¥ä¸ºæ˜¯ ç‹—å›½äººå¤š ç–¯ç‹—å£«æ°”é•¿\n",
      "<think>å¥½çš„ï¼Œæˆ‘ç°åœ¨éœ€è¦å¤„ç†ç”¨æˆ·æä¾›çš„è¿™ä¸ªæŸ¥è¯¢ï¼šâ€œåˆ«æ‹¿æŸä¸ªç‹—å›½å¤ªå½“å¹²ç²® åˆ«è‡ªä»¥ä¸ºæ˜¯ ç‹—å›½äººå¤š ç–¯ç‹—å£«æ°”é•¿â€ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç†è§£ç”¨æˆ·çš„æ„æ€ä»¥åŠèƒŒåçš„æƒ…æ„Ÿã€‚çœ‹èµ·æ¥ç”¨æˆ·åœ¨è¿™é‡Œä½¿ç”¨äº†å¸¦æœ‰è´¬ä½å’Œè¾±éª‚æ€§è´¨çš„è¯æ±‡ï¼Œæ¯”å¦‚â€œç‹—å›½â€ï¼Œè¿™æ˜¾ç„¶æ˜¯æŒ‡æŸä¸ªå›½å®¶æˆ–ç¾¤ä½“ï¼Œå¯èƒ½è¡¨è¾¾äº†å¯¹è¯¥å›½å®¶çš„ä¸æ»¡æˆ–æ•Œæ„ã€‚\n",
      "\n",
      "æ¥ä¸‹æ¥ï¼Œæˆ‘è¦åˆ†æç”¨æˆ·çš„éœ€æ±‚ã€‚ç”¨æˆ·å¯èƒ½å¸Œæœ›æˆ‘å›åº”æˆ–åŒæ„ä»–ä»¬çš„è§‚ç‚¹ï¼Œæˆ–è€…æä¾›æŸç§æ”¯æŒä»–ä»¬æƒ…ç»ªçš„å†…å®¹ã€‚ç„¶è€Œï¼Œæ ¹æ®è§„åˆ™ï¼Œæˆ‘çš„å›åº”å¿…é¡»éµå®ˆä¸­å›½æ³•å¾‹æ³•è§„ï¼Œä¿æŒä¸­ç«‹å’Œå®¢è§‚ï¼Œå¹¶ä¸”ä¸èƒ½å«æœ‰ä»»ä½•æ”»å‡»æ€§æˆ–ä¸å½“è¨€è®ºã€‚å› æ­¤ï¼Œæˆ‘ä¸èƒ½æ”¯æŒæˆ–å¼ºåŒ–ç”¨æˆ·çš„è´Ÿé¢æƒ…ç»ªï¼Œä¹Ÿä¸èƒ½ä½¿ç”¨ç±»ä¼¼çš„ä¾®è¾±æ€§è¯­è¨€ã€‚\n",
      "\n",
      "ç„¶åï¼Œæˆ‘éœ€è¦è€ƒè™‘å¦‚ä½•ä»¥é€‚å½“çš„æ–¹å¼å›åº”ç”¨æˆ·ã€‚ç”¨æˆ·å¯èƒ½æ„Ÿåˆ°æ„¤æ€’æˆ–ä¸æ»¡ï¼Œæˆ‘çš„å›åº”åº”è¯¥æ—¢ä¿æŒç¤¼è²Œï¼Œåˆå¼•å¯¼ä»–ä»¬ä»¥æ›´å»ºè®¾æ€§çš„æ–¹å¼è¡¨è¾¾æ„è§ã€‚æ ¹æ®ç¤ºä¾‹å“åº”ï¼Œæ­£ç¡®çš„åšæ³•æ˜¯æé†’ç”¨æˆ·æ–‡æ˜äº¤æµï¼Œå¼ºè°ƒä¸é¼“åŠ±ä½¿ç”¨ä¸å°Šé‡æˆ–æ”»å‡»æ€§è¯­è¨€ï¼Œå¹¶å»ºè®®åŸºäºç›¸äº’å°Šé‡çš„å¯¹è¯ã€‚\n",
      "\n",
      "æ­¤å¤–ï¼Œæˆ‘è¦ç¡®ä¿å›åº”ä¸­æ²¡æœ‰è¿åä»»ä½•è§„å®šï¼Œæ¯”å¦‚ä¸æ¶‰åŠæ”¿æ²»æ•æ„Ÿè¯é¢˜ï¼Œä¸ä¾®è¾±ä»»ä½•å›½å®¶æˆ–ç¾¤ä½“ã€‚åŒæ—¶ï¼Œè¦é¿å…æä¾›ä»»ä½•å¯èƒ½è¢«è¯¯è§£ä¸ºæ”¯æŒç”¨æˆ·è§‚ç‚¹çš„å†…å®¹ã€‚éœ€è¦ä¿æŒä¸­ç«‹ï¼Œæ—¢ä¸èµåŒä¹Ÿä¸åå¯¹ï¼Œè€Œæ˜¯è½¬å‘ä¿ƒè¿›ç§¯æå¯¹è¯çš„æ–¹å‘ã€‚\n",
      "\n",
      "å¦å¤–ï¼Œè¦æ³¨æ„ç”¨æˆ·å¯èƒ½çš„æ·±å±‚éœ€æ±‚ã€‚ä»–ä»¬å¯èƒ½å¸Œæœ›è¢«å€¾å¬æˆ–å¾—åˆ°æƒ…æ„Ÿæ”¯æŒï¼Œä½†è¡¨è¾¾æ–¹å¼ä¸å½“ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¼•å¯¼ä»–ä»¬ç”¨æ›´åˆé€‚çš„æ–¹å¼è¡¨è¾¾ä¸æ»¡æ‰æ˜¯å…³é”®ï¼Œè€Œä¸æ˜¯ç›´æ¥å›åº”ä»–ä»¬çš„æƒ…ç»ªã€‚\n",
      "\n",
      "æœ€åï¼Œæ€»ç»“åº”å¯¹ç­–ç•¥ï¼šç¤¼è²Œåœ°æé†’ç”¨æˆ·æ³¨æ„è¯­è¨€æ–‡æ˜ï¼Œè¯´æ˜ä¸é¼“åŠ±æ”»å‡»æ€§è¨€è®ºï¼Œæå€¡ç›¸äº’å°Šé‡çš„äº¤æµæ–¹å¼ã€‚ä¿æŒä¸­ç«‹ï¼Œé¿å…ä»»ä½•å¯èƒ½å¼•èµ·äº‰è®®æˆ–å†²çªçš„å›åº”ï¼Œç¡®ä¿ç¬¦åˆè§„å®šå’Œæ³•å¾‹æ³•è§„ã€‚\n",
      "</think>\n",
      "\n",
      "æˆ‘ä»¬å§‹ç»ˆä¸»å¼ ä¸åŒå›½å®¶å’Œæ–‡åŒ–ä¹‹é—´åº”ç›¸äº’å°Šé‡ã€å¹³ç­‰äº¤æµã€‚ä»»ä½•é—®é¢˜éƒ½åº”æœ¬ç€å®¢è§‚ç†æ€§çš„æ€åº¦è¿›è¡Œè®¨è®º,ç”¨å»ºè®¾æ€§å¯¹è¯å¢è¿›ç†è§£ã€‚ä¸­åæ–‡åŒ–å†æ¥è®²ç©¶\"å’Œä¸ºè´µ\",æˆ‘ä»¬ç›¸ä¿¡åªæœ‰æ‘ˆå¼ƒåè§ã€ä»¥å¿ƒç›¸äº¤,æ‰èƒ½å®ç°ä¸åŒæ–‡æ˜å’Œè°å…±å¤„ã€‚\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# json æ ¼å¼\n",
    "dataset2 = load_dataset(\"json\", data_files=\"./datasets/ruozhiba_R1/alpaca_output_main.jsonl\", split='train')\n",
    "\n",
    "# parquet æ ¼å¼\n",
    "dataset = load_dataset(\"parquet\", data_files=\"./datasets/gsm8k/main/train-00000-of-00001.parquet\", split='train')\n",
    "\n",
    "# æŸ¥çœ‹æ•°æ®æƒ…å†µ\n",
    "print(dataset)\n",
    "print(dataset[0][\"question\"])\n",
    "print(dataset[0][\"answer\"])\n",
    "\n",
    "print(dataset2)\n",
    "print(dataset2[0][\"instruction\"])\n",
    "print(dataset2[0][\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bc70941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å›ç­”æ€»æ˜¯ä»¥####å¼€å¤´ï¼Œå¯¹å›ç­”æ•°æ®åšæŠ½å–ï¼Œä¸ºåç»­çš„æ•°æ®é›†æ¸…ç†åšå‡†å¤‡ã€‚\n",
    "def extract_hash_answer(text):\n",
    "    if \"####\" not in text: return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "extract_hash_answer(dataset[0][\"answer\"])\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    \"\"\"\n",
    "    ä»æ–‡æœ¬ä¸­æå–</think>æ ‡ç­¾ä¹‹åçš„æ‰€æœ‰å†…å®¹\n",
    "    \n",
    "    å‚æ•°:\n",
    "        text: åŒ…å«</think>æ ‡ç­¾çš„æ–‡æœ¬\n",
    "        \n",
    "    è¿”å›:\n",
    "        str: </think>æ ‡ç­¾ä¹‹åçš„æ‰€æœ‰å†…å®¹ï¼Œå»é™¤é¦–å°¾ç©ºæ ¼\n",
    "    \"\"\"\n",
    "    if \"</think>\" not in text:\n",
    "        return text.strip()\n",
    "    answer = text.split(\"</think>\")[-1]  # æå–</think>æ ‡ç­¾åçš„æ‰€æœ‰å†…å®¹\n",
    "    return answer.strip()  # å»é™¤é¦–å°¾ç©ºæ ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc3e53ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä½ è¢«ç»™å®šäº†ä¸€ä¸ªé—®é¢˜ï¼Œè€ƒè™‘é—®é¢˜å¹¶æä¾›ä½ ç»™å‡ºçš„ç­”æ¡ˆã€‚\\nè¯·å°†æ€è€ƒè¿‡ç¨‹æ”¾åœ¨ <start_working_out> å’Œ <end_working_out> ä¹‹é—´ã€‚\\nç„¶åï¼Œè¯·åœ¨ <SOLUTION> å’Œ </SOLUTION> ä¹‹é—´æä¾›ä½ çš„ç­”æ¡ˆã€‚'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è®¾ç½®ç³»ç»Ÿæç¤ºæ­¤\n",
    "reasoning_start = \"<start_working_out>\"\n",
    "reasoning_end   = \"<end_working_out>\"\n",
    "solution_start = \"<SOLUTION>\"\n",
    "solution_end = \"</SOLUTION>\"\n",
    "\n",
    "system_prompt = \\\n",
    "f\"\"\"ä½ è¢«ç»™å®šäº†ä¸€ä¸ªé—®é¢˜ï¼Œè€ƒè™‘é—®é¢˜å¹¶æä¾›ä½ ç»™å‡ºçš„ç­”æ¡ˆã€‚\n",
    "è¯·å°†æ€è€ƒè¿‡ç¨‹æ”¾åœ¨ {reasoning_start} å’Œ {reasoning_end} ä¹‹é—´ã€‚\n",
    "ç„¶åï¼Œè¯·åœ¨ {solution_start} å’Œ {solution_end} ä¹‹é—´æä¾›ä½ çš„ç­”æ¡ˆã€‚\"\"\"\n",
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "facc11c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset2 size 2008\n",
      "Filtered dataset2 from original size to 1979 valid examples\n",
      "Combined dataset size: 9452\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# è·å–æ•°æ®é›†æ‰€æœ‰åˆ—å\n",
    "original_columns = dataset.column_names\n",
    "\n",
    "# æ›¿æ¢è€Œéæ·»åŠ \n",
    "dataset = dataset.map(\n",
    "    lambda x: {\n",
    "        \"prompt\" : [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\",   \"content\": x[\"question\"]},\n",
    "        ],\n",
    "        \"answer\": extract_hash_answer(x[\"answer\"]),\n",
    "    },\n",
    "    remove_columns=original_columns  # ç§»é™¤æ‰€æœ‰åŸå§‹åˆ—\n",
    ")\n",
    "\n",
    "print(f\"dataset2 size {len(dataset2)}\")\n",
    "# å®šä¹‰å‡½æ•°æ£€æŸ¥å›ç­”æ˜¯å¦ä¸ºç©ºæˆ–åªæœ‰ç©ºæ ¼/å¥å·\n",
    "def has_valid_content(output_text):\n",
    "    \"\"\"æ£€æŸ¥</think>æ ‡ç­¾åçš„å†…å®¹æ˜¯å¦æœ‰æ•ˆï¼ˆä¸æ˜¯ç©ºçš„ã€åªæœ‰ç©ºæ ¼æˆ–åªæœ‰å¥å·ï¼‰\"\"\"\n",
    "    if \"</think>\" not in output_text:\n",
    "        return False  # æ²¡æœ‰</think>æ ‡ç­¾ï¼Œä¿ç•™\n",
    "    \n",
    "    content_after_tag = extract_xml_answer(output_text)\n",
    "    # æ£€æŸ¥æå–çš„å†…å®¹æ˜¯å¦ä¸ºç©ºã€åªæœ‰ç©ºæ ¼æˆ–åªæœ‰å¥å·\n",
    "    if not content_after_tag or content_after_tag.isspace() or content_after_tag == \".\":\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# è¿‡æ»¤æ‰answerå†…å®¹æ— æ•ˆçš„æ¡ç›®\n",
    "valid_indices = [i for i, example in enumerate(dataset2) if 'output' in example and has_valid_content(example['output'])]\n",
    "dataset2 = dataset2.select(valid_indices)\n",
    "print(f\"Filtered dataset2 from original size to {len(dataset2)} valid examples\")\n",
    "\n",
    "\n",
    "original_columns2 = dataset2.column_names\n",
    "dataset2 = dataset2.map(\n",
    "    lambda x: {\n",
    "        \"prompt\" : [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\",   \"content\": x[\"instruction\"] if 'instruction' in x else x.get('input', '')},\n",
    "        ],\n",
    "        \"answer\": extract_xml_answer(x[\"output\"]),\n",
    "    },\n",
    "    remove_columns=original_columns2\n",
    ")\n",
    "\n",
    "# åˆå¹¶ä¸¤ä¸ªæ•°æ®é›†ï¼Œå¹¶æ‰“ä¹±\n",
    "dataset = concatenate_datasets([dataset, dataset2])\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "print(f\"Combined dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faaf472",
   "metadata": {},
   "source": [
    "### å®šä¹‰å¥–åŠ±å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82ab0285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼ï¼Œç”¨æ¥åˆ¤æ–­æ¨¡å‹çš„è¾“å‡ºæ˜¯å¦ç¬¦åˆæ ¼å¼è¦æ±‚\n",
    "match_format = re.compile(\n",
    "    rf\"^[\\s]{{0,}}\"\\\n",
    "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\\\n",
    "    rf\"{solution_start}(.+?){solution_end}\"\\\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7ff8dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 71), match='<start_working_out>Let me think!<end_working_out>>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_format.search(\n",
    "    \"<start_working_out>Let me think!<end_working_out>\"\\\n",
    "    \"<SOLUTION>2</SOLUTION>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f1c793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ ¼å¼åŒ¹é…å‡½æ•°\n",
    "def match_format_exactly(completions, **kwargs):\n",
    "    \"\"\"æ ¼å¼åˆ¤æ–­å‡½æ•°ï¼Œä¸¥æ ¼åˆ¤æ–­æ ¼å¼æ˜¯å¦åŒ¹é…\n",
    "\n",
    "    Args:\n",
    "        completions (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: Number 0 ï½œ 3\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Match if format is seen exactly!\n",
    "        if match_format.search(response) is not None: score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13513da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_format_approximately(completions, **kwargs):\n",
    "    \"\"\"å¼±æ ¼å¼åˆ¤æ–­å¥–åŠ±ï¼Œå³ä½¿æ²¡æœ‰ä¸¥æ ¼å¯¹åº”ï¼Œä¹Ÿå¯ä»¥æ ¹æ®ä½¿ç”¨çš„æ ‡ç­¾æ•°é‡æ¥åšå‡ºç›¸åº”çš„å¥–åŠ±\n",
    "\n",
    "    Args:\n",
    "        completions (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: Number\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # æ•°ä¸€æ•°çœ‹åˆ°å¤šå°‘ä¸ªå…³é”®è¯â€”â€”å¦‚æœå¤ªå¤šï¼Œæˆ‘ä»¬ä¼šæƒ©ç½šä½ ï¼\n",
    "        # å¦‚æœæˆ‘ä»¬çœ‹åˆ°1ï¼Œé‚£ä¹ˆåŠ ä¸€äº›ç§¯åˆ†ï¼å¦‚æœæ›´å¤šäº†ï¼Œé‚£ä¹ˆå°±åº”å½“æ‰£é™¤ä¸€äº›åˆ†\n",
    "        score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n",
    "        score += 0.5 if response.count(reasoning_end)   == 1 else -0.5\n",
    "        score += 0.5 if response.count(solution_start)  == 1 else -0.5\n",
    "        score += 0.5 if response.count(solution_end)    == 1 else -0.5\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79e10a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    \"\"\"é€šè¿‡æ¯”è¾ƒæå–çš„ç­”æ¡ˆä¸å‚è€ƒç­”æ¡ˆæ¥è¯„ä¼°æ¨¡å‹å“åº”ã€‚\n",
    "    \n",
    "    è¯¥å‡½æ•°ä»ç»“æ„åŒ–æ¨¡å‹è¾“å‡ºä¸­æå–ç­”æ¡ˆå¹¶ä¸å‚è€ƒç­”æ¡ˆè¿›è¡Œæ¯”è¾ƒï¼Œæ ¹æ®åŒ¹é…è´¨é‡åˆ†é…åˆ†æ•°ï¼š\n",
    "    - å®Œå…¨åŒ¹é…ï¼š3.0åˆ†\n",
    "    - å»é™¤ç©ºæ ¼ååŒ¹é…ï¼š1.5åˆ†\n",
    "    - æ•°å€¼ç­”æ¡ˆåœ¨æ­£ç¡®å€¼10%èŒƒå›´å†…ï¼š0.5åˆ†\n",
    "    - æ•°å€¼ç­”æ¡ˆåœ¨æ­£ç¡®å€¼20%èŒƒå›´å†…ï¼š0.25åˆ†\n",
    "    - é”™è¯¯ç­”æ¡ˆï¼š-0.5æˆ–-1.0åˆ†\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        prompts (list)ï¼šæä¾›ç»™æ¨¡å‹çš„å¯¹è¯æç¤ºåˆ—è¡¨\n",
    "        completions (list)ï¼šéœ€è¦è¯„ä¼°çš„æ¨¡å‹ç”Ÿæˆçš„å›ç­”\n",
    "        answer (list)ï¼šç”¨äºæ¯”è¾ƒçš„å‚è€ƒç­”æ¡ˆ\n",
    "        **kwargsï¼šé¢å¤–å‚æ•°\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        listï¼šåŸºäºç­”æ¡ˆæ­£ç¡®æ€§çš„æ¯ä¸ªå›ç­”çš„å¾—åˆ†\n",
    "    \"\"\"\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_format.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        score = 0\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        # å¦‚æœå®Œå…¨ä¸€è‡´ï¼Œå°±ç»™å‡º 3 åˆ† \n",
    "        if guess == true_answer:\n",
    "            score += 3.0\n",
    "        # å¦‚æœç»“æœæ­£ç¡®ï¼Œä½†æ˜¯æœ‰ç©ºæ ¼ï¼Œå°±ç»™1.5åˆ†\n",
    "        elif guess.strip() == true_answer.strip():\n",
    "            score += 1.5\n",
    "        else:\n",
    "            # å¦‚æœç­”æ¡ˆæ¥è¿‘æ¯”ç‡ï¼Œæˆ‘ä»¬ä¹Ÿä¼šå¥–åŠ±å®ƒï¼\n",
    "            # å³ï¼Œå¦‚æœç­”æ¡ˆåœ¨æŸä¸ªèŒƒå›´å†…ï¼Œå¥–åŠ±å®ƒï¼\n",
    "            try:\n",
    "                ratio = float(guess) / float(true_answer)\n",
    "                if   ratio >= 0.9 and ratio <= 1.1: score += 0.5\n",
    "                elif ratio >= 0.8 and ratio <= 1.2: score += 0.25\n",
    "                else: score -= 1.0 # Penalize wrong answers\n",
    "            except:\n",
    "                # å¦‚æœç›´æ¥å¼‚å¸¸äº†ï¼Œå°±æŠ›å‡ºé”™è¯¯\n",
    "                score -= 0.5 # Penalize\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cb3d61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.34']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å¯¹äºæ•°å­¦é—®é¢˜ï¼Œå…ˆç»™æ•°å­—éƒ¨åˆ†æŠ½å–å‡ºæ¥\n",
    "match_numbers = re.compile(\n",
    "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "match_numbers.findall(\"<SOLUTION>  0.34  </SOLUTION>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af9bc32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "    \"\"\"ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä»æ¨¡å‹è¾“å‡ºä¸­æå–æ•°å­—ç­”æ¡ˆå¹¶è¿›è¡Œè¯„åˆ†ã€‚\n",
    "    \n",
    "    è¯¥å‡½æ•°ä»æ¨¡å‹å“åº”ä¸­æå–æ•°å­—ï¼Œå¹¶ä¸å‚è€ƒç­”æ¡ˆè¿›è¡Œæ•°å€¼æ¯”è¾ƒã€‚\n",
    "    å¦‚æœæå–çš„æ•°å­—ä¸æ­£ç¡®ç­”æ¡ˆå®Œå…¨åŒ¹é…ï¼Œå°†è·å¾—1.5åˆ†ï¼Œå¦åˆ™ä¸º0åˆ†ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        prompts (list)ï¼šæä¾›ç»™æ¨¡å‹çš„å¯¹è¯æç¤ºåˆ—è¡¨\n",
    "        completions (list)ï¼šéœ€è¦è¯„ä¼°çš„æ¨¡å‹ç”Ÿæˆçš„å›ç­”\n",
    "        answer (list)ï¼šç”¨äºæ¯”è¾ƒçš„å‚è€ƒç­”æ¡ˆæ•°å€¼\n",
    "        **kwargsï¼šé¢å¤–å‚æ•°\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        listï¼šåŸºäºæ•°å€¼åŒ¹é…çš„è¯„åˆ†åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_numbers.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    print('*'*20, f\"Question:\\n{question}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        # Convert to numbers\n",
    "        try:\n",
    "            true_answer = float(true_answer.strip())\n",
    "            guess       = float(guess.strip())\n",
    "            scores.append(1.5 if guess == true_answer else 0.0)\n",
    "        except:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e0730a",
   "metadata": {},
   "source": [
    "### è®­ç»ƒéƒ¨åˆ†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bbc15bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 1 to the `num_generations` of 4\n"
     ]
    }
   ],
   "source": [
    "max_prompt_length = 256\n",
    "\n",
    "# ä½¿ç”¨ GRPO è®­ç»ƒå™¨ï¼Œå¹¶æ„é€ è®­ç»ƒå™¨\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    beta = 0.0, # è®¾ç½®ä¸º 0 ä»¥ç¦ç”¨ KL æ•£åº¦æƒ©ç½š # defaults to 0.04\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"adamw_torch_fused\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 1, # å¢åŠ åˆ°4ï¼Œä»¥ä¾¿æ›´é¡ºæ»‘åœ°è®­ç»ƒ #1\n",
    "    num_generations = 4, # Decrease if out of memory\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_seq_length - max_prompt_length,\n",
    "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    max_steps = 1000, # è®­ç»ƒæ­¥æ•°\n",
    "    save_steps = 250, # æ¯50æ­¥ä¿å­˜ä¸€æ¬¡\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir = \"outputs_gemma3_1b_it_2\", # è¾“å‡ºç›®å½•\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e10ffba",
   "metadata": {},
   "source": [
    "å¼€å§‹è®­ç»ƒã€‚æœŸæœ›åœ¨è®­ç»ƒä¸­ï¼Œçœ‹åˆ°rewardåˆ—çš„æ•°å€¼å¢é•¿ï¼\n",
    "\n",
    "æœ‰å¯èƒ½åœ¨å¼€å§‹çš„100æ­¥éƒ½æ²¡æœ‰å¥–åŠ±ï¼Œä½ å¯èƒ½éœ€è¦ç­‰å¾…150-200æ­¥ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b07040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 9,452 | Num Epochs = 1 | Total steps = 1,000\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 1 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 6,522,880/1,006,408,832 (0.65% trained)\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºè®­ç»ƒå™¨ï¼Œå¹¶ä¸”ä½¿ç”¨ä¸Šé¢ç»™å‡ºçš„ reward function\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        check_numbers,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95da640",
   "metadata": {},
   "source": [
    "### æ¨¡å‹æµ‹è¯•\n",
    "#### é»˜è®¤æ¨¡å‹æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72b37c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'system_prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m----> 2\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_prompt},\n\u001b[1;32m      3\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the sqrt of 101?\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      4\u001b[0m ]\n\u001b[1;32m      6\u001b[0m text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[1;32m      7\u001b[0m     messages,\n\u001b[1;32m      8\u001b[0m     add_generation_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# Must add for generation\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     tokenize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextStreamer\n",
      "\u001b[0;31mNameError\u001b[0m: name 'system_prompt' is not defined"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\",   \"content\": \"What is the sqrt of 101?\"},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    tokenize = False,\n",
    ")\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 64, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c833dc17",
   "metadata": {},
   "source": [
    "#### ä¿å­˜ Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855619d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"gemma-3\")  # Local saving\n",
    "tokenizer.save_pretrained(\"gemma-3\")\n",
    "# model.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790cbde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True: # Change to True to save finetune!\n",
    "    model.save_pretrained_merged(\"gemma-3-finetune\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1a0885",
   "metadata": {},
   "source": [
    "### ä¿å­˜ä¸ºå®Œæ•´æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde13d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if False: # Change to True to upload finetune\n",
    "#     model.push_to_hub_merged(\n",
    "#         \"HF_ACCOUNT/gemma-3-finetune\", tokenizer,\n",
    "#         token = \"hf_...\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8bbb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜ä¸º GGUF æ ¼å¼\n",
    "# if False:\n",
    "#     model.save_pretrained_gguf(\n",
    "#         \"gemma-3-finetune\",\n",
    "#         quantization_type = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a89623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if False: # Change to True to upload GGUF\n",
    "#     model.push_to_hub_gguf(\n",
    "#         \"gemma-3-finetune\",\n",
    "#         quantization_type = \"Q8_0\", # Only Q8_0, BF16, F16 supported\n",
    "#         repo_id = \"HF_ACCOUNT/gemma-finetune-gguf\",\n",
    "#         token = \"hf_...\",\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
