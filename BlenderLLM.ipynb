{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a7e62a6",
   "metadata": {},
   "source": [
    "### 加载\n",
    "#### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b9257",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "from client import ClientSender\n",
    "import uuid\n",
    "import zmq\n",
    "import msgpack\n",
    "\n",
    "# 实例化网络传输对象\n",
    "client = ClientSender(server_address=\"10.30.244.17\", port=5555)\n",
    "\n",
    "max_seq_length = 2048 # 模型的最大序列长度，默认是1024\n",
    "lora_rank = 16 # LoRA的秩，越大越好，但会消耗更多内存 #8\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"./models/gemma-3-12b-it\", #\"unsloth/gemma-3-1b-it\",\n",
    "    max_seq_length = max_seq_length, # 可以选择任意长度以支持长上下文！\n",
    "    load_in_4bit = True,  # 4位量化以减少内存使用\n",
    "    load_in_8bit = False, # 精度更高，但使用2倍内存\n",
    "    full_finetuning = False, # 完全微调\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e380ab",
   "metadata": {},
   "source": [
    "#### 加载 Lora 设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127dd5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.language_model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # 仅处理文本层或者模型没有视觉层时关闭\n",
    "    finetune_language_layers   = True,  # 应该保持开启！\n",
    "    finetune_attention_modules = True,  # 注意力机制对GRPO有好处\n",
    "    finetune_mlp_modules       = True,  # 应该始终保持开启！\n",
    "\n",
    "    r = lora_rank,           # 更大 = 更高的精度，但可能过拟合\n",
    "    lora_alpha = lora_rank,  # 建议alpha至少等于r\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407, # 使用同一个随机数种子\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ced53c",
   "metadata": {},
   "source": [
    "#### 加载、构造数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d619bc97",
   "metadata": {},
   "source": [
    "##### 构造系统提示词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e53ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你是一个 Blender 的材质生成器，你将会考虑问题并提供材质对应的 python 代码，该代码应该可以且仅在 Blender 中创建对应材质，你生成出的python代码应当就是最终结果，用户可以直接使用，不需要用户更改，你也不会使用任何外部文件。\\n请将思考过程放在 <think> 和 </think> 之间。\\n然后，请在 <code> 和 </code> 之间提供你的答案。'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置系统提示此\n",
    "reasoning_start = \"<think>\"\n",
    "reasoning_end   = \"</think>\"\n",
    "solution_start = \"<code>\"\n",
    "solution_end   = \"</code>\"\n",
    "\n",
    "system_prompt = \\\n",
    "f\"\"\"你是一个 Blender 的材质生成器，你将会考虑问题并提供材质对应的 python 代码，该代码应该可以且仅在 Blender 中创建对应材质，你生成出的python代码应当就是最终结果，用户可以直接使用，不需要用户更改，你也不会使用任何外部文件。\n",
    "请将思考过程放在 {reasoning_start} 和 {reasoning_end} 之间。\n",
    "然后，请在 {solution_start} 和 {solution_end} 之间提供你的答案。\"\"\"\n",
    "system_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e63be34",
   "metadata": {},
   "source": [
    "##### 构造数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f67b2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "dataset = []\n",
    "\n",
    "# 设置三个不同的难度等级\n",
    "level1 = [\n",
    "    \"红色的材质\", \"蓝色的材质\", \"蓝色到黄的渐变材质\", \"绿色的材质\", \"紫色的材质\", \"金色的材质\", \n",
    "    \"银色的材质\", \"透明的材质\", \"棕色的木质材质\", \"白色的材质\", \"黑色的材质\", \"灰色的材质\",\n",
    "    \"橙色的材质\", \"粉红色的材质\", \"黄色的材质\", \"深红色的材质\",     \"红色的材质\", \"蓝色的材质\", \"蓝色到黄色的渐变材质\", \"绿色到黄色的渐变材质\", \"紫色到粉色的渐变材质\",\n",
    "    \"金色到银色的渐变材质\", \"绿色到白色的渐变材质\", \"橙色到红色的渐变材质\", \"红色到黑色的渐变材质\", \"青色的材质\", \n",
    "    \"深蓝色的材质\", \"墨绿色的材质\", \"淡蓝色的材质\", \"米色的材质\", \"炭灰色的材质\",\n",
    "    \"浅黄色的材质\", \"彩虹色的材质\", \"霓虹色的材质\", \"紫黑色的材质\", \"金属色的材质\",\n",
    "    \"皮革材质\", \"深棕色的材质\", \"大理石材质\", \"沙土材质\", \"亮光材质\", \n",
    "    \"亮金色的材质\", \"暗棕色的材质\", \"液体材质\", \"冷白色的材质\", \"塑料材质\",\n",
    "    \"深蓝色的材质\", \"热气流材质\", \"透明的材质\", \"冰块材质\", \"深蓝到浅蓝的渐变材质\", \"蓝色到紫色的渐变材质\", \"粉色到紫色的渐变材质\", \"红色到绿色的渐变材质\",\n",
    "    \"黄色到绿色的渐变材质\", \"深紫到浅紫的渐变材质\", \"紫色到蓝色的渐变材质\", \"蓝色到白色的渐变材质\", \n",
    "    \"青色到白色的渐变材质\", \"橙色到黄色的渐变材质\", \"黑色到灰色的渐变材质\"\n",
    "]\n",
    "level2 = [\n",
    "    \"红色的金属材质：这种材质表面光滑，金属光泽非常突出，适合表现科技感强的物体，给人一种坚硬而现代的感觉。\",\n",
    "    \"蓝色的金属材质：它呈现出深邃的蓝色，金属反射效果明显，适合用在精密机械或未来感十足的设计中。\",\n",
    "    \"红色的木头材质：这种木材的表面有明显的纹理，颜色鲜艳且富有自然感，适合制作温暖、自然的环境。\",\n",
    "    \"深紫色的钢铁材质：这种材质混合了紫色和灰色，具有较强的视觉冲击感，适合用于表现坚固而神秘的物体。\",\n",
    "    \"绿松石色的塑料材质：该材质呈现出绿色的光泽感，给人一种清新自然的感觉，适合用于现代简约风格。\",\n",
    "    \"金色的光滑材质：它的表面非常光滑，反射效果强，通常用于奢华或高贵的设计风格中。\",\n",
    "    \"银白色的钛金属材质：该材质具有冷光感和金属质感，质地非常坚硬且耐用，适合用于科技产品或高级饰品。\",\n",
    "    \"白色的瓷砖材质：这种材质表面平整且具有光泽，适合用于现代简约或清新风格的空间装饰。\",\n",
    "    \"金属质感的深灰色材质：这种材质表面有着明显的金属光泽，深灰色调给人稳重、高科技的感觉。\",\n",
    "    \"绿色的玻璃材质：这种玻璃表面呈现出绿色的透光效果，具有良好的透明度，适合用于装饰或外墙材质。\",\n",
    "    \"橙色的陶瓷质感材质：材质表面有细腻的纹理和温暖的橙色调，适合用在舒适、温馨的环境中。\",\n",
    "    \"深蓝色的铝合金材质：它表面平滑且坚固，深蓝色使其更具科技感，适合应用于高端电子产品。\",\n",
    "    \"深红色的硬木材质：表面有着清晰的木纹，深红色使得材质更显高贵和传统，适用于古典风格的设计。\",\n",
    "    \"高亮黑色的炭材质：表面呈现出黑色的光泽，给人一种现代、简约的感觉，适合极简风格的设计。\",\n",
    "    \"浅黄色的金属氧化物材质：这款材质呈现浅黄色，表面有一定的金属质感，适合用于现代艺术品或装饰。\",\n",
    "    \"紫色的有机玻璃材质：它呈现出紫色的透明效果，给人一种现代、梦幻的感觉，适合用于创意设计中。\",\n",
    "    \"铁灰色的铝合金材质：表面坚硬且具金属光泽，深沉的铁灰色给人稳重、工业感强的视觉效果。\",\n",
    "    \"红色的亮面木质材料：红色木质材料的光泽感非常明显，色调明亮且温暖，适合用于家具和室内装饰。\",\n",
    "    \"铜色的老化金属材质：这种材质的表面有着岁月的痕迹，带有金属的光泽和老化效果，适合复古风格设计。\",\n",
    "    \"钢铁质感的蓝色材质：这种材质坚硬且具有冷金属光泽，深蓝色增加了它的冷峻感，适用于工业设计或机械构件。\"\n",
    "]\n",
    "\n",
    "level3 = [\n",
    "    \"红色的拉丝金属材质：这种材质具有拉丝效果的金属表面，表面上有一层细微的划痕，折射出红色光泽。拉丝金属具有非常独特的视觉效果，常用于高端产品的外壳设计。它不仅具备金属质感的坚固，还能通过表面纹理传递出一种精细且独特的工业风格。适合用在电子设备、汽车设计或任何需要展现现代感和精致感的物品上。\",\n",
    "    \"椅子上的纹路：这款材质的纹理呈现出精致的花纹，可能是圆形、对称或者不规则的图案，表面看起来既舒适又具有视觉冲击力。椅子上的纹理材质常常是通过精细的织物或者皮革等材质呈现，给使用者带来视觉上的享受与舒适的触感。设计师往往会利用这些纹路来增强座椅的美感与舒适度，特别适用于豪华沙发和高端办公椅。\",\n",
    "    \"科幻战舰的纹路：这种材质是为了打造科幻风格的战舰外壳效果而设计，表面有类似航空材料的纹理和结构。光滑的表面上布满了复杂的机械纹路和细致的金属拼接效果，通常搭配冷光的材质和强烈的金属质感。适用于未来科技、科幻电影或高端模型的设计，给人一种高科技、坚固、且充满未来感的视觉体验。\",\n",
    "    \"深蓝色的抛光金属材质：此材质采用深蓝色金属合金，其表面经过精细的抛光处理，展现出光滑如镜的效果。光泽度和反射效果非常强，给人一种现代和高端的感觉。深蓝色调使得材质更加具有神秘感，适合用于航空航天、精密机械或任何需要强烈视觉冲击的高科技设计。\",\n",
    "    \"水面反射的紫色金属材质：这种材质表面呈现出紫色和蓝色的渐变效果，仿佛水面反射的光辉。金属质感结合了水面般的流动感，光线在表面折射，产生多层次的视觉效果。适合用于未来科技、科幻影视道具，或者高端艺术作品中的元素，能够带给人一种极为独特的感觉。\",\n",
    "    \"白色皮革的纹理：这款皮革材质表面呈现出精细的纹路，质感柔软且富有弹性，白色使得它显得清新且高贵。常用于时尚品牌的服装、鞋包设计或者豪华家具中。它不仅具备皮革的耐用性，还通过纹理的变化提升了整体的美感与触感，成为时尚和品味的象征。\",\n",
    "    \"蓝色光泽的铁板材质：铁板表面采用了特殊的蓝色光泽涂层，形成了一种极具现代感和科技感的视觉效果。表面虽然有金属的坚硬感，但由于光泽的反射，表现出了独特的动感。适合用于电子产品、建筑装饰或者其他需要坚固且具有现代感的产品设计。\",\n",
    "    \"铝合金表面的细小纹理：这种材质的表面细腻且均匀，呈现出细小的铝合金纹理，整体上既坚固又具有非常精致的工艺感。适合用于精密机械、航空器材以及现代建筑的设计。其表面纹理不仅提高了抗磨损的能力，还能使得整体设计显得更具现代科技感。\",\n",
    "    \"深棕色的木纹纹理：这种木材表面呈现出深棕色的纹理，纹路自然且清晰，带有浓厚的自然气息。常用于家具、地板或室内装饰中，能够为空间带来温暖且舒适的氛围。木纹质感的变化使得每一件产品都有独特的个性，适合用于传统或现代混合风格的室内设计。\",\n",
    "    \"经典的金属拉丝效果：这款金属材质的表面具有经典的拉丝效果，通过金属表面细致的刷纹处理，创造出独特的视觉效果。拉丝金属不仅能够反射光线，还赋予表面一种精致且简约的外观，适合用于现代家居、电子产品以及高端装饰物品中。\"\n",
    "]\n",
    "\n",
    "# 合并材质请求\n",
    "tasks = level1 + level2 + level3\n",
    "\n",
    "# 任务前缀列表\n",
    "user_start = [\"做这个材质:\", \"帮我生成这个：\", \"这个问题需要你的帮助, 帮我生成\", \"我希望你能帮助我生成\", \"请帮我做一个材质\", \"希望你帮我生成\", \"请你为我做一个\", \"你能做一下这个吗?\", \"\", \"请完成以下任务:\", \"帮我搞一个材质:\", \"能否帮我生成\", \"请协助我生成一个\"]\n",
    "\n",
    "# 构造数据集\n",
    "for task in tasks:\n",
    "    user_prompts = user_start + user_start\n",
    "    random.shuffle(user_prompts)\n",
    "    for user in user_prompts:\n",
    "        dataset.append({\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user + task},\n",
    "            ],\n",
    "            \"taskid\": uuid.uuid4().hex,\n",
    "            \"goal\": task,\n",
    "        })\n",
    "    \n",
    "# 输出最终数据集\n",
    "final_dataset = Dataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faaf472",
   "metadata": {},
   "source": [
    "### 定义奖励函数\n",
    "#### 定义标准格式形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ab0285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 42), match='<think>Let me think!</think><code>2</code>'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 定义正则表达式，用来判断模型的输出是否符合格式要求\n",
    "match_format = re.compile(\n",
    "    rf\"^[\\s]{{0,}}\"\\\n",
    "    rf\"<think>.+?</think>.*?\"\\\n",
    "    rf\"<code>(.+?)</code>\"\\\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "match_format.search(\n",
    "    \"<think>Let me think!</think>\"\\\n",
    "    \"<code>2</code>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f1a09b",
   "metadata": {},
   "source": [
    "#### 构造奖励函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1c793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 严格格式判断函数\n",
    "def match_format_exactly(completions, **kwargs):\n",
    "    \"\"\"格式判断函数，严格判断格式是否匹配\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Match if format is seen exactly!\n",
    "        if match_format.search(response) is not None: score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13513da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 弱格式判断函数\n",
    "def match_format_approximately(prompts, completions, **kwargs):\n",
    "    \"\"\"弱格式判断奖励，即使没有严格对应，也可以根据使用的标签数量来做出相应的奖励\n",
    "    \"\"\"\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "        \n",
    "    print('*'*20, f\"Question:\\n{question}\", f\"\\nResponse:\\n{responses[0]}\")\n",
    "    \n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # 数一数看到多少个关键词——如果太多，我们会惩罚你！\n",
    "        # 如果我们看到1个关键词，那么加一些积分！如果更多了，那么就应当扣除一些分\n",
    "        score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n",
    "        score += 0.5 if response.count(reasoning_end)   == 1 else -0.5\n",
    "        score += 0.5 if response.count(solution_start)  == 1 else -0.5\n",
    "        score += 0.5 if response.count(solution_end)    == 1 else -0.5\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618e3980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取代码\n",
    "def code_extractor(code):\n",
    "    match = re.search(r'<code>(.*?)</code>', code, flags=re.DOTALL)\n",
    "    if not match:\n",
    "        return \"\"\n",
    "    content = match.group(1).strip()\n",
    "    if not content:\n",
    "        return \"\"\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cdb864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成的图像和描述的相似度\n",
    "def accuracy_reward(goal, taskid, completions, prompts, **kwargs):\n",
    "    \"\"\"计算生成的图像和描述的相似度\n",
    "    \"\"\"\n",
    "    \n",
    "    WEIGHT = 2 # 用来在归一化之后加权\n",
    "    scores = []\n",
    "    names = []\n",
    "    \n",
    "    # 构造传输对象\n",
    "    materials = {\n",
    "        \"head\": {\n",
    "            \"input\": goal[0],\n",
    "            \"taskid\": taskid[0],\n",
    "            \"request\": []\n",
    "        },\n",
    "        \"outputs\": []\n",
    "    }\n",
    "    \n",
    "    # 填充材质代码\n",
    "    for completion in completions:\n",
    "        response = completion[0][\"content\"]\n",
    "        code = code_extractor(response)\n",
    "        print(\"AR_CODE________________\")\n",
    "        print(type(code))\n",
    "        print(code)\n",
    "        \n",
    "        name = f\"M{len(materials['outputs'])+1}\"\n",
    "        names.append(name)\n",
    "        \n",
    "        materials[\"outputs\"].append({\n",
    "            \"name\": name,\n",
    "            \"code\": code\n",
    "        })\n",
    "    \n",
    "    print(\"AR_MT________________\")\n",
    "    print(type(materials))\n",
    "    print(materials)\n",
    "\n",
    "    # 添加分数\n",
    "    c =  client.send_materials(materials)\n",
    "    print(\"AR_P________________\")\n",
    "    print(type(c))\n",
    "    print(c)\n",
    "    results = c.get(\"accuracy_rank\", {})\n",
    "\n",
    "    \n",
    "    for name in names:\n",
    "        score = int(results.get(name, 0))\n",
    "        scores.append(score)\n",
    "    \n",
    "    # 归一化并加权\n",
    "    min_s, max_s = min(scores), max(scores)\n",
    "    if max_s > min_s:\n",
    "        scores[:] = [(s - min_s) / (max_s - min_s) for s in scores]\n",
    "        # 对分数加权\n",
    "        scores = [s * WEIGHT for s in scores]\n",
    "    else:\n",
    "        scores = [0,0,0,0]\n",
    "    \n",
    "    # 返回分数\n",
    "    print(\"accuracy_reward\" + str(results))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8124a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图像是否有意义\n",
    "def meaning_reward(goal, taskid, completions, **kwargs):\n",
    "    \"\"\"计算生成的图像是否有意义\n",
    "    \"\"\"\n",
    "    \n",
    "    WEIGHT = 1 # 用来在归一化之后加权\n",
    "    scores = []\n",
    "    names = []\n",
    "    \n",
    "    # 构造传输对象\n",
    "    materials = {\n",
    "        \"head\": {\n",
    "            \"input\": goal[0],\n",
    "            \"taskid\": taskid[0],\n",
    "            \"request\": []\n",
    "        },\n",
    "        \"outputs\": []\n",
    "    }\n",
    "    \n",
    "    # 填充材质代码\n",
    "    for completion in completions:\n",
    "        response = completion[0][\"content\"]\n",
    "        code = code_extractor(response)\n",
    "        print(\"MR_CODE________________\")\n",
    "        print(type(code))\n",
    "        print(code)\n",
    "        \n",
    "        name = f\"M{len(materials['outputs'])+1}\"\n",
    "        names.append(name)\n",
    "        \n",
    "        materials[\"outputs\"].append({\n",
    "            \"name\": name,\n",
    "            \"code\": code\n",
    "        })\n",
    "\n",
    "    print(\"MR_MT________________\")\n",
    "    print(type(materials))\n",
    "    print(materials)\n",
    "    \n",
    "    # 添加分数\n",
    "    c =  client.send_materials(materials)\n",
    "    print(\"MR_P________________\")\n",
    "    print(type(c))\n",
    "    print(c)\n",
    "    results = c.get(\"meaning_rank\", {})\n",
    "    \n",
    "    for name in names:\n",
    "        score = int(results.get(name, 0))\n",
    "        scores.append(score)\n",
    "    \n",
    "    # 归一化并加权\n",
    "    min_s, max_s = min(scores), max(scores)\n",
    "    if max_s > min_s:\n",
    "        scores[:] = [(s - min_s) / (max_s - min_s) for s in scores]\n",
    "        scores[:] = [s * WEIGHT for s in scores]\n",
    "    else:\n",
    "        scores = [0,0,0,0]\n",
    "    \n",
    "    # 返回分数\n",
    "    print(\"meaning_reward\" + str(results))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc0d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代码是否报错\n",
    "def error_check(goal, taskid, completions, **kwargs):\n",
    "    \"\"\"检查生成的代码是否报错\n",
    "    \"\"\"\n",
    "    \n",
    "    WEIGHT = 2\n",
    "    scores = []\n",
    "    names = []\n",
    "    \n",
    "    # 构造传输对象\n",
    "    materials = {\n",
    "        \"head\": {\n",
    "            \"input\": goal[0],\n",
    "            \"taskid\": taskid[0],\n",
    "            \"request\": []\n",
    "        },\n",
    "        \"outputs\": []\n",
    "    }\n",
    "    \n",
    "    # 填充材质代码\n",
    "    for completion in completions:\n",
    "        response = completion[0][\"content\"]\n",
    "        code = code_extractor(response)\n",
    "        print(\"EC_CODE________________\")\n",
    "        print(type(code))\n",
    "        print(code)\n",
    "        \n",
    "        name = f\"M{len(materials['outputs'])+1}\"\n",
    "        names.append(name)\n",
    "        \n",
    "        materials[\"outputs\"].append({\n",
    "            \"name\": name,\n",
    "            \"code\": code\n",
    "        })\n",
    "\n",
    "    print(\"EC_MT________________\")\n",
    "    print(type(materials))\n",
    "    print(materials)\n",
    "    \n",
    "    # 添加分数\n",
    "    c =  client.send_materials(materials)\n",
    "    print(\"EC_P________________\")\n",
    "    print(type(c))\n",
    "    print(c)\n",
    "    results = c.get(\"status\", {})\n",
    "    # results = client.send_materials(materials).get(\"status\", {})\n",
    "    \n",
    "    for name in names:\n",
    "        score = results.get(name, False)\n",
    "        scores.append(WEIGHT if score else 0)\n",
    "    \n",
    "    # 检查是否存在梯度，如果没有梯度了就放弃\n",
    "    min_s, max_s = min(scores), max(scores)\n",
    "    if not max_s > min_s:\n",
    "        scores = [0,0,0,0]\n",
    "    \n",
    "    # 返回分数\n",
    "    print(\"error_check\" + str(results))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e0730a",
   "metadata": {},
   "source": [
    "### 训练部分\n",
    "#### 训练配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbc15bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prompt_length = 256\n",
    "\n",
    "# 使用 GRPO 训练器，并构造训练器\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    beta = 0.0, # 设置为 0 以禁用 KL 散度惩罚 # defaults to 0.04\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"adamw_torch_fused\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 1, # 增加到4，以便更顺滑地训练 #1\n",
    "    num_generations = 4, # Decrease if out of memory\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_seq_length - max_prompt_length,\n",
    "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    max_steps = 1200, # 训练步数\n",
    "    save_steps = 200, # 每200步保存一次\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir = \"outputs_ArtI_12b_bf4_qlora\", # 输出目录\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e10ffba",
   "metadata": {},
   "source": [
    "#### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b07040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2,366 | Num Epochs = 1 | Total steps = 1,200\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 1 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 65,470,464/12,000,000,000 (0.55% trained)\n",
      "`generation_config` default values have been modified to match model-specific defaults: {'top_k': 64, 'top_p': 0.95, 'bos_token_id': 2, 'eos_token_id': [1, 106]}. If this is not desired, please set these values explicitly.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Question:\n",
      "希望你帮我生成透明的材质 \n",
      "Response:\n",
      "<think>\n",
      "The task is to create a transparent (glass-like) material using Python within Blender. The core process is to modify the material's transmission and roughness values. Transmission should be set to 1.000000000, and roughness set to a low value.\n",
      "</think>\n",
      "\n",
      "<code>\n",
      "import bpy\n",
      "\n",
      "# Create a new material\n",
      "mat = bpy.data.materials.new(name=\"Transparent\")\n",
      "\n",
      "# Set shading to eevee if available\n",
      "if 'EEPROD' in bpy.context:\n",
      "    bpy.context.scene.shader_nodes.eevee\n",
      "\n",
      "# Set transmission to 1\n",
      "mat.transmission = 1.000000000\n",
      "\n",
      "# Set roughness to a low value\n",
      "mat.roughness = 0.0\n",
      "return\n",
      "</code>\n",
      "AR_ODE________________\n",
      "<class 'str'>\n",
      "import bpy\n",
      "\n",
      "# Create a new material\n",
      "mat = bpy.data.materials.new(name=\"Transparent\")\n",
      "\n",
      "# Set shading to eevee if available\n",
      "if 'EEPROD' in bpy.context:\n",
      "    bpy.context.scene.shader_nodes.eevee\n",
      "\n",
      "# Set transmission to 1\n",
      "mat.transmission = 1.000000000\n",
      "\n",
      "# Set roughness to a low value\n",
      "mat.roughness = 0.0\n",
      "return\n",
      "AR_ODE________________\n",
      "<class 'str'>\n",
      "import bpy\n",
      "\n",
      "mat = bpy.data.materials.new(name=\"Transparent\")\n",
      "mat.use_nodes = True\n",
      "nodes = mat.node_tree.nodes\n",
      "\n",
      "bsdf_node = nodes[\"Principled BSDF\"]\n",
      "transparency_value = 1.000\n",
      "bsdf_node.node_tree.nodes[\"Principled BSDF\"].transparency = transparency_value\n",
      "\n",
      "if 'Material Output' in nodes:\n",
      "    pass\n",
      "else:\n",
      "    mat.node_tree.nodes.new(type='Material Output', name='Material Output')\n",
      "    mat.node_tree.links.new(nodes[\"Principled BSDF\"].outputs[0], nodes[\"Material Output\"].inputs[0])\n",
      "\n",
      "bpy.data.materials.active = mat\n",
      "# Example: Assigning the material to a plane\n",
      "\n",
      "plane = bpy.data.objects[\"Plane\"]\n",
      "if plane is not None:\n",
      "    plane.data.material_slots[0].material = mat\n",
      "else:\n",
      "    print(\"Plane object not found, cannot assign the material\")\n",
      "AR_ODE________________\n",
      "<class 'str'>\n",
      "import math\n",
      "\n",
      "material = math.new_material()\n",
      "material.transmission = 1.0\n",
      "material.alpha = 1.0\n",
      "material.blend_mode = 'transmission'\n",
      "material.transmission_roughness = 0.0\n",
      "\n",
      "return material\n",
      "AR_ODE________________\n",
      "<class 'str'>\n",
      "import bpy\n",
      "\n",
      "material = bpy.data.materials.new(name=\"TransparentMaterial\")\n",
      "material.use_nodes = True\n",
      "\n",
      "nodes = material.node_tree.nodes\n",
      "\n",
      "principled_bsdf = nodes[\"Principled BSDF\"]\n",
      "principledged_bsdf.index = 0\n",
      "\n",
      "nodes[\"Principled BSDF\"].location = (0,0)\n",
      "\n",
      "nodes[\"Principled BSDF\"].translucency = 0\n",
      "nodes[\"Principled BSDF\"].alpha = 1\n",
      "nodes[\"Principled BSDF\"].blend_method = 'VALUE'\n",
      "\n",
      "bpy.data.materials.active = material\n",
      "\n",
      "if len(bpy.data.materials) > 0:\n",
      "    bpy.data.materials[0] = material\n",
      "AR_MT________________\n",
      "<class 'dict'>\n",
      "{'head': {'input': '透明的材质', 'taskid': '0b1e52e5d3a84958b8a4ef5dc5f66a96', 'request': []}, 'outputs': [{'name': 'M1', 'code': 'import bpy\\n\\n# Create a new material\\nmat = bpy.data.materials.new(name=\"Transparent\")\\n\\n# Set shading to eevee if available\\nif \\'EEPROD\\' in bpy.context:\\n    bpy.context.scene.shader_nodes.eevee\\n\\n# Set transmission to 1\\nmat.transmission = 1.000000000\\n\\n# Set roughness to a low value\\nmat.roughness = 0.0\\nreturn'}, {'name': 'M2', 'code': 'import bpy\\n\\nmat = bpy.data.materials.new(name=\"Transparent\")\\nmat.use_nodes = True\\nnodes = mat.node_tree.nodes\\n\\nbsdf_node = nodes[\"Principled BSDF\"]\\ntransparency_value = 1.000\\nbsdf_node.node_tree.nodes[\"Principled BSDF\"].transparency = transparency_value\\n\\nif \\'Material Output\\' in nodes:\\n    pass\\nelse:\\n    mat.node_tree.nodes.new(type=\\'Material Output\\', name=\\'Material Output\\')\\n    mat.node_tree.links.new(nodes[\"Principled BSDF\"].outputs[0], nodes[\"Material Output\"].inputs[0])\\n\\nbpy.data.materials.active = mat\\n# Example: Assigning the material to a plane\\n\\nplane = bpy.data.objects[\"Plane\"]\\nif plane is not None:\\n    plane.data.material_slots[0].material = mat\\nelse:\\n    print(\"Plane object not found, cannot assign the material\")'}, {'name': 'M3', 'code': \"import math\\n\\nmaterial = math.new_material()\\nmaterial.transmission = 1.0\\nmaterial.alpha = 1.0\\nmaterial.blend_mode = 'transmission'\\nmaterial.transmission_roughness = 0.0\\n\\nreturn material\"}, {'name': 'M4', 'code': 'import bpy\\n\\nmaterial = bpy.data.materials.new(name=\"TransparentMaterial\")\\nmaterial.use_nodes = True\\n\\nnodes = material.node_tree.nodes\\n\\nprincipled_bsdf = nodes[\"Principled BSDF\"]\\nprincipledged_bsdf.index = 0\\n\\nnodes[\"Principled BSDF\"].location = (0,0)\\n\\nnodes[\"Principled BSDF\"].translucency = 0\\nnodes[\"Principled BSDF\"].alpha = 1\\nnodes[\"Principled BSDF\"].blend_method = \\'VALUE\\'\\n\\nbpy.data.materials.active = material\\n\\nif len(bpy.data.materials) > 0:\\n    bpy.data.materials[0] = material'}]}\n",
      "尝试连接服务器... (1/5)\n",
      "连接到服务器 10.30.244.17:5555...\n",
      "已连接到服务器 10.30.244.17:5555\n",
      "发送材质组数据... (1/5)\n",
      "正在发送材质组 (4个材质) 到 10.30.244.17:5555\n",
      "等待服务器响应...\n",
      "AR_P________________\n",
      "<class 'dict'>\n",
      "{'accuracy_rank': {'M1': 1, 'M2': 2, 'M3': 3, 'M4': 4}, 'meaning_rank': {'M1': 1, 'M2': 2, 'M3': 3, 'M4': 4}, 'status': {'M1': False, 'M2': False, 'M3': False, 'M4': False}, 'error_msg': {'M1': \"执行材质代码时出错: 'return' outside function (<string>, line 15)\", 'M2': \"执行材质代码时出错: 'ShaderNodeBsdfPrincipled' object has no attribute 'node_tree'\", 'M3': \"执行材质代码时出错: 'return' outside function (<string>, line 9)\", 'M4': \"执行材质代码时出错: name 'principledged_bsdf' is not defined\"}, 'id': {'M1': None, 'M2': None, 'M3': None, 'M4': None}, 'name': {'M1': 'M1', 'M2': 'M2', 'M3': 'M3', 'M4': 'M4'}}\n",
      "accuracy_reward{'M1': 1, 'M2': 2, 'M3': 3, 'M4': 4}\n",
      "发送材质组数据... (1/5)\n",
      "正在发送材质组 (4个材质) 到 10.30.244.17:5555\n",
      "等待服务器响应...\n",
      "meaning_reward{'M1': 1, 'M2': 2, 'M3': 3, 'M4': 4}\n",
      "发送材质组数据... (1/5)\n",
      "正在发送材质组 (4个材质) 到 10.30.244.17:5555\n",
      "等待服务器响应...\n",
      "error_check{'M1': False, 'M2': False, 'M3': False, 'M4': False}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 创建训练器，并且使用上面给出的 reward function\u001b[39;00m\n\u001b[32m      2\u001b[39m trainer = GRPOTrainer(\n\u001b[32m      3\u001b[39m     model = model,\n\u001b[32m      4\u001b[39m     processing_class = tokenizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     train_dataset = final_dataset,\n\u001b[32m     20\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:311\u001b[39m, in \u001b[36m_fast_inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:77\u001b[39m, in \u001b[36m_unsloth_training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/accelerate/accelerator.py:2359\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2357\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2358\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2359\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/function.py:307\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    302\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mImplementing both \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbackward\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mvjp\u001b[39m\u001b[33m'\u001b[39m\u001b[33m for a custom \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    303\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFunction is not allowed. You should only implement one \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    304\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mof them.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    305\u001b[39m     )\n\u001b[32m    306\u001b[39m user_fn = vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function.vjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:1710\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.backward\u001b[39m\u001b[34m(ctx, *flat_args)\u001b[39m\n\u001b[32m   1708\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m CompiledFunction._double_backward(ctx, impl_fn, all_args)\n\u001b[32m   1709\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1710\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:1700\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.backward.<locals>.impl_fn\u001b[39m\u001b[34m(double_ctx)\u001b[39m\n\u001b[32m   1699\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mimpl_fn\u001b[39m(double_ctx=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1700\u001b[39m     out = \u001b[43mCompiledFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_backward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1701\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m CompiledFunction._backward_epilogue(ctx, out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2065\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction._backward_impl\u001b[39m\u001b[34m(ctx, all_args)\u001b[39m\n\u001b[32m   2048\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2049\u001b[39m     torch._functorch.config.donated_buffer\n\u001b[32m   2050\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m saved_tensors_use_once\n\u001b[32m   2051\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m fw_metadata.bw_donated_idxs != []\n\u001b[32m   2052\u001b[39m ):\n\u001b[32m   2053\u001b[39m     torch._check(\n\u001b[32m   2054\u001b[39m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2055\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[32m   (...)\u001b[39m\u001b[32m   2062\u001b[39m         ),\n\u001b[32m   2063\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2065\u001b[39m out = \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCompiledFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompiled_bw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2068\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2070\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2071\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[39m, in \u001b[36mcall_func_at_runtime_with_args\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33m_boxed_call\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         out = normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[32m    130\u001b[39m         warnings.warn(\n\u001b[32m    131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt take boxed arguments. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    741\u001b[39m prior_skip_guard_eval_unsafe = set_skip_guard_eval_unsafe(\n\u001b[32m    742\u001b[39m     _is_skip_guard_eval_unsafe_stance()\n\u001b[32m    743\u001b[39m )\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    747\u001b[39m     _maybe_set_eval_frame(prior)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_inductor/output_code.py:466\u001b[39m, in \u001b[36mCompiledFxGraph.__call__\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    468\u001b[39m     AutotuneCacheBundler.end_compile()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_inductor/utils.py:2128\u001b[39m, in \u001b[36malign_inputs_from_check_idxs.<locals>.run\u001b[39m\u001b[34m(new_inputs)\u001b[39m\n\u001b[32m   2126\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mrun\u001b[39m(new_inputs: List[InputType]):\n\u001b[32m   2127\u001b[39m     copy_misaligned_inputs(new_inputs, inputs_to_check)\n\u001b[32m-> \u001b[39m\u001b[32m2128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/torchinductor_root/ru/crup6ang6gzbn7qr4yllbhl7ftuuewtlmw2cwgbtbry2eybo2dpe.py:237\u001b[39m, in \u001b[36mcall\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    235\u001b[39m triton_poi_fused__to_copy_add_mul_neg_new_zeros_scatter_add_1_xnumel = s0*s1\n\u001b[32m    236\u001b[39m stream0 = get_raw_stream(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m \u001b[43mtriton_poi_fused__to_copy_add_mul_neg_new_zeros_scatter_add_1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtangents_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msum_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimals_11\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimals_10\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtriton_poi_fused__to_copy_add_mul_neg_new_zeros_scatter_add_1_xnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtriton_poi_fused__to_copy_add_mul_neg_new_zeros_scatter_add_1_xnumel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m exp_2\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m primals_10\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_inductor/runtime/triton_heuristics.py:1034\u001b[39m, in \u001b[36mCachingAutotuner.run\u001b[39m\u001b[34m(self, grid, stream, benchmark_run, *args, **kwargs)\u001b[39m\n\u001b[32m   1032\u001b[39m         \u001b[38;5;28mself\u001b[39m.precompile_time_taken_ns = time.time_ns() - start_time\n\u001b[32m   1033\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.launchers) > \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautotune_to_one_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28mself\u001b[39m.launchers[\u001b[32m0\u001b[39m].config, \u001b[33m\"\u001b[39m\u001b[33mfound_by_coordesc\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1038\u001b[39m ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.inductor_meta.get(\u001b[33m\"\u001b[39m\u001b[33mcoordinate_descent_tuning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28mself\u001b[39m.launchers = [\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28mself\u001b[39m.coordinate_descent_tuning(\n\u001b[32m   1041\u001b[39m             \u001b[38;5;28mself\u001b[39m.launchers[\u001b[32m0\u001b[39m], *args, grid=grid, **kwargs\n\u001b[32m   1042\u001b[39m         )\n\u001b[32m   1043\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_inductor/runtime/triton_heuristics.py:911\u001b[39m, in \u001b[36mCachingAutotuner.autotune_to_one_config\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    909\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Do the actual autotuning\"\"\"\u001b[39;00m\n\u001b[32m    910\u001b[39m start_time = time.time_ns()\n\u001b[32m--> \u001b[39m\u001b[32m911\u001b[39m timings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbenchmark_all_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    912\u001b[39m benchmark_time_taken_ns = time.time_ns() - start_time\n\u001b[32m    913\u001b[39m \u001b[38;5;28mself\u001b[39m.launchers = [builtins.min(timings, key=timings.get)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_inductor/runtime/triton_heuristics.py:886\u001b[39m, in \u001b[36mCachingAutotuner.benchmark_all_configs\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mbenchmark_all_configs\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    882\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[32m    883\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCachingAutotuner.benchmark_all_configs\u001b[39m\u001b[33m\"\u001b[39m, log_pt2_compile_event=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    884\u001b[39m     ):\n\u001b[32m    885\u001b[39m         timings = {\n\u001b[32m--> \u001b[39m\u001b[32m886\u001b[39m             launcher: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbench\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlauncher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    887\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m launcher \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.launchers\n\u001b[32m    888\u001b[39m         }\n\u001b[32m    890\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m timings.items():\n\u001b[32m    891\u001b[39m             \u001b[38;5;28mself\u001b[39m.coordesc_tuner.cache_benchmark_result(k.config, v)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_inductor/runtime/triton_heuristics.py:763\u001b[39m, in \u001b[36mCachingAutotuner.bench\u001b[39m\u001b[34m(self, launcher, grid, with_profiler, *args, **kwargs)\u001b[39m\n\u001b[32m    760\u001b[39m device_interface = \u001b[38;5;28mself\u001b[39m.get_device_interface()\n\u001b[32m    761\u001b[39m stream = device_interface.get_raw_stream(device_interface.current_device())\n\u001b[32m--> \u001b[39m\u001b[32m763\u001b[39m cpu_copies = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy_args_to_cpu_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mkernel_call\u001b[39m():\n\u001b[32m    766\u001b[39m     cloned_args, cloned_kwargs = \u001b[38;5;28mself\u001b[39m.maybe_clone_args(\n\u001b[32m    767\u001b[39m         cpu_copies, *args, **kwargs\n\u001b[32m    768\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_inductor/runtime/triton_heuristics.py:822\u001b[39m, in \u001b[36mCachingAutotuner.copy_args_to_cpu_if_needed\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    819\u001b[39m             budget -= size\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(args):\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m     \u001b[43mmaybe_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m.\u001b[49m\u001b[43marg_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m kwargs.items():\n\u001b[32m    825\u001b[39m     maybe_copy(name, arg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_inductor/runtime/triton_heuristics.py:809\u001b[39m, in \u001b[36mCachingAutotuner.copy_args_to_cpu_if_needed.<locals>.maybe_copy\u001b[39m\u001b[34m(name, arg)\u001b[39m\n\u001b[32m    807\u001b[39m size = arg.numel() * arg.element_size()\n\u001b[32m    808\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size > budget:\n\u001b[32m--> \u001b[39m\u001b[32m809\u001b[39m     cpu_arg = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty_strided\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[43m        \u001b[49m\u001b[43marg\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    811\u001b[39m \u001b[43m        \u001b[49m\u001b[43marg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43marg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    813\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    814\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    815\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    816\u001b[39m     cpu_arg.copy_(arg, non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    817\u001b[39m     copies[name] = (arg, cpu_arg)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# 创建训练器，并且使用上面给出的 reward function\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        # 格式奖励\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        \n",
    "        # 图像意义奖励\n",
    "        accuracy_reward,\n",
    "        meaning_reward,\n",
    "        \n",
    "        # 运行错误检查\n",
    "        error_check,\n",
    "        \n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = final_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bbbf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "没有数据可以绘图!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "\n",
    "# 设置Seaborn样式以获得更好看的图表\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "def extract_rewards_from_log(log_path):\n",
    "    \"\"\"从训练日志文件中提取奖励数据\n",
    "    \n",
    "    参数:\n",
    "        log_path: 日志文件路径\n",
    "        \n",
    "    返回:\n",
    "        包含步骤和对应奖励的pandas DataFrame\n",
    "    \"\"\"\n",
    "    # 存储数据的字典\n",
    "    data = defaultdict(list)\n",
    "    step_pattern = re.compile(r'Step\\s+(\\d+)')\n",
    "    reward_pattern = re.compile(r'Reward_(\\d+):\\s+([-\\d.]+)')\n",
    "    mean_reward_pattern = re.compile(r'Mean Reward:\\s+([-\\d.]+)')\n",
    "    \n",
    "    if not os.path.exists(log_path):\n",
    "        print(f\"日志文件 {log_path} 不存在!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    with open(log_path, 'r') as f:\n",
    "        for line in f:\n",
    "            # 提取步骤\n",
    "            step_match = step_pattern.search(line)\n",
    "            if step_match:\n",
    "                current_step = int(step_match.group(1))\n",
    "                data['step'].append(current_step)\n",
    "                \n",
    "                # 提取各个奖励函数的值\n",
    "                rewards = reward_pattern.findall(line)\n",
    "                for idx, value in rewards:\n",
    "                    data[f'reward_{idx}'].append(float(value))\n",
    "                \n",
    "                # 提取平均奖励\n",
    "                mean_match = mean_reward_pattern.search(line)\n",
    "                if mean_match:\n",
    "                    data['mean_reward'].append(float(mean_match.group(1)))\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def extract_rewards_from_trainer(trainer):\n",
    "    \"\"\"从trainer对象中直接提取奖励数据\n",
    "    \n",
    "    参数:\n",
    "        trainer: GRPOTrainer对象\n",
    "        \n",
    "    返回:\n",
    "        包含步骤和对应奖励的pandas DataFrame\n",
    "    \"\"\"\n",
    "    if hasattr(trainer, 'state') and hasattr(trainer.state, 'log_history'):\n",
    "        data = defaultdict(list)\n",
    "        for entry in trainer.state.log_history:\n",
    "            if 'step' in entry:\n",
    "                data['step'].append(entry['step'])\n",
    "                \n",
    "                # 提取各个奖励\n",
    "                for key, value in entry.items():\n",
    "                    if key.startswith('reward_'):\n",
    "                        data[key].append(value)\n",
    "                \n",
    "                # 提取平均奖励\n",
    "                if 'mean_reward' in entry:\n",
    "                    data['mean_reward'].append(entry['mean_reward'])\n",
    "                \n",
    "        return pd.DataFrame(data)\n",
    "    else:\n",
    "        print(\"训练器没有日志历史或者结构不符合预期!\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def plot_rewards(data, title=\"GRPO训练奖励曲线\", save_path=None, moving_avg_window=5):\n",
    "    \"\"\"绘制奖励折线图\n",
    "    \n",
    "    参数:\n",
    "        data: 包含奖励数据的DataFrame\n",
    "        title: 图表标题\n",
    "        save_path: 保存图表的路径，如果为None则显示图表\n",
    "        moving_avg_window: 移动平均窗口大小\n",
    "    \"\"\"\n",
    "    if data.empty:\n",
    "        print(\"没有数据可以绘图!\")\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # 定义一组专业的颜色\n",
    "    colors = sns.color_palette('viridis', n_colors=len(data.columns)-1)\n",
    "    \n",
    "    # 绘制每个奖励函数的曲线\n",
    "    for i, col in enumerate([col for col in data.columns if col != 'step']):\n",
    "        # 原始数据点（透明度降低）\n",
    "        ax.plot(data['step'], data[col], alpha=0.3, color=colors[i], label=f\"{col} (raw)\")\n",
    "        \n",
    "        # 添加移动平均线\n",
    "        if len(data) >= moving_avg_window:\n",
    "            moving_avg = data[col].rolling(window=moving_avg_window).mean()\n",
    "            ax.plot(data['step'], moving_avg, linewidth=2, color=colors[i], label=f\"{col} ({moving_avg_window}-point avg)\")\n",
    "    \n",
    "    # 添加标题和标签\n",
    "    ax.set_title(title, fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Training Steps', fontsize=14)\n",
    "    ax.set_ylabel('Reward', fontsize=14)\n",
    "    \n",
    "    # 添加网格线和图例\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax.legend(loc='best', fontsize=12)\n",
    "    \n",
    "    # 添加统计信息\n",
    "    if 'mean_reward' in data.columns:\n",
    "        final_mean = data['mean_reward'].iloc[-1]\n",
    "        max_mean = data['mean_reward'].max()\n",
    "        min_mean = data['mean_reward'].min()\n",
    "        stats_text = f\"Final mean reward: {final_mean:.4f}\\nMax mean reward: {max_mean:.4f}\\nMin mean reward: {min_mean:.4f}\"\n",
    "        plt.figtext(0.02, 0.02, stats_text, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 保存或显示图表\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"图表已保存到 {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "# 示例用法\n",
    "def visualize_rewards(trainer=None, log_file=None, output_path=None):\n",
    "    \"\"\"可视化训练奖励\n",
    "    \n",
    "    参数:\n",
    "        trainer: GRPOTrainer对象，如果提供则直接从训练器中提取数据\n",
    "        log_file: 日志文件路径，如果trainer不可用则从日志文件中提取数据\n",
    "        output_path: 图表保存路径，默认为当前目录下的'reward_plot.png'\n",
    "    \"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = 'reward_plot.png'\n",
    "    \n",
    "    if trainer is not None:\n",
    "        data = extract_rewards_from_trainer(trainer)\n",
    "    elif log_file is not None:\n",
    "        data = extract_rewards_from_log(log_file)\n",
    "    else:\n",
    "        print(\"请提供trainer对象或日志文件路径!\")\n",
    "        return\n",
    "    \n",
    "    plot_rewards(data, save_path=output_path)\n",
    "    \n",
    "    # 输出一些统计信息\n",
    "    if not data.empty and 'mean_reward' in data.columns:\n",
    "        print(\"\\n--- 奖励统计信息 ---\")\n",
    "        print(f\"最终平均奖励: {data['mean_reward'].iloc[-1]:.4f}\")\n",
    "        print(f\"最大平均奖励: {data['mean_reward'].max():.4f}\")\n",
    "        print(f\"最小平均奖励: {data['mean_reward'].min():.4f}\")\n",
    "        \n",
    "        # 计算奖励增长率\n",
    "        if len(data) > 1:\n",
    "            first_reward = data['mean_reward'].iloc[0]\n",
    "            last_reward = data['mean_reward'].iloc[-1]\n",
    "            growth = ((last_reward - first_reward) / abs(first_reward)) * 100 if first_reward != 0 else float('inf')\n",
    "            print(f\"奖励增长率: {growth:.2f}%\")\n",
    "\n",
    "# 用法示例\n",
    "# 1. 使用训练器对象\n",
    "# visualize_rewards(trainer=trainer)\n",
    "\n",
    "# 2. 或者使用日志文件\n",
    "# visualize_rewards(log_file=\"./outputs_gemma-3_grpo_lora/opt_gemm3_2.log\")\n",
    "\n",
    "# 从训练后直接可视化\n",
    "# 在训练后调用以下代码即可直接可视化\n",
    "visualize_rewards(trainer=trainer, output_path=\"reward_trends.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95da640",
   "metadata": {},
   "source": [
    "### 模型测试\n",
    "#### 默认模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72b37c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import bpy\n",
      "\n",
      "def create_gradient_material(material_name=\"GradientMaterial\"):\n",
      "    \"\"\"\n",
      "    Creates a grey to yellow gradient material in Blender using GLSL shader.\n",
      "\n",
      "    Args:\n",
      "        material_name (str): The name of the material to create.\n",
      "    \"\"\"\n",
      "\n",
      "    # Check if material already exists\n",
      "    if material_name in bpy.data.materials:\n",
      "        return\n",
      "\n",
      "    # Create a new material\n",
      "    mat = bpy.data.materials.new(name=material_name)\n",
      "    mat.use_nodes = True\n",
      "    nodes = mat.node_tree.nodes\n",
      "\n",
      "    # Clear default nodes\n",
      "    for node in nodes:\n",
      "        nodes.remove(node)\n",
      "\n",
      "    # Create Principled BSDF node\n",
      "    principled_bsdf = nodes.new(type='ShaderNodeBsdfPrincipled')\n",
      "    principled_bsdf.location = (200, 0)\n",
      "    principled_bsdf.inputs['Base Color'].default_value = (0.0, 0.0, 0.0, 1.0) # Initial black color\n",
      "\n",
      "    # Create ShaderNodeGLSL\n",
      "    glsl_node = nodes.new(type='ShaderNodeGLSL')\n",
      "    glsl_node.location = (-200, 0)\n",
      "    glsl_node.inputs['GLSL Code'].default_value = \"\"\"\n",
      "    #version 330 core\n",
      "    uniform float time;\n",
      "    \n",
      "    vec3 color = vec3(0.5); // Start with grey\n",
      "\n",
      "    float gradient_position = fract(time * 0.2);  // Adjust speed as needed\n",
      "    \n",
      "    // Blend from grey to yellow\n",
      "    color += "
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    # {\"role\": \"system\", \"content\": \"你是一个 GLSL Shader 生成器，你生成出来的应当就是最终结果，可以直接使用，你也不会使用任何外部文件，纯粹程序化生成\"},\n",
    "    # {\"role\": \"system\", \"content\": \"你是一个 blender 节点解释器，会和我解释 blender 节点是干什么用的\"},\n",
    "    {\"role\": \"system\", \"content\": \"你是一个 Blender 的材质生成器，会直接生成材质对应的 Python 代码，该代码应该可以且仅在 Blender 中创建对应材质，你生成出来的应当就是最终结果，用户可以直接使用，不需要用户更改，你也不会使用任何外部文件\"},\n",
    "    {\"role\": \"user\",   \"content\": \"给我生成一个的灰色渐变到黄色的材质\"},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    tokenize = False,\n",
    ")\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 1024*2, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a49ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载原始模型（不包含微调）\n",
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "# 定义相同的参数\n",
    "max_seq_length = 1024\n",
    "\n",
    "# 重新加载原始模型（不应用LoRA权重）\n",
    "original_model, original_tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"./models/gemma-3-1b-it\",  # 使用原始模型路径\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = False,\n",
    "    load_in_8bit = False,\n",
    ")\n",
    "\n",
    "# 测试问题\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},  # 使用之前定义的系统提示词\n",
    "    {\"role\": \"user\", \"content\": \"What is the sqrt of 101?\"},  # 使用同样的测试问题以便比较\n",
    "]\n",
    "\n",
    "# 准备输入\n",
    "test_text = original_tokenizer.apply_chat_template(\n",
    "    test_messages,\n",
    "    add_generation_prompt = True,\n",
    "    tokenize = False,\n",
    ")\n",
    "\n",
    "# 使用TextStreamer直接查看输出\n",
    "from transformers import TextStreamer\n",
    "print(\"\\n原始模型输出：\")\n",
    "_ = original_model.generate(\n",
    "    **original_tokenizer(test_text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 1024,\n",
    "    temperature = 0.8,  # 使用与微调模型相同的温度\n",
    "    top_p = 0.95,\n",
    "    top_k = 64,\n",
    "    streamer = TextStreamer(original_tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7426953b",
   "metadata": {},
   "source": [
    "#### finetuning 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfea2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存 Lora\n",
    "model.save_lora(\"grpo_saved_lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c833dc17",
   "metadata": {},
   "source": [
    "#### 保存 Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855619d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gemma-3/tokenizer_config.json',\n",
       " 'gemma-3/special_tokens_map.json',\n",
       " 'gemma-3/tokenizer.model',\n",
       " 'gemma-3/added_tokens.json',\n",
       " 'gemma-3/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"gemma-3\")  # Local saving\n",
    "tokenizer.save_pretrained(\"gemma-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "790cbde8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'startswith'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:125\u001b[0m, in \u001b[0;36mHfFileSystem._repo_and_revision_exist\u001b[0;34m(self, repo_type, repo_id, revision)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api\u001b[38;5;241m.\u001b[39mrepo_info(\n\u001b[1;32m    126\u001b[0m         repo_id, revision\u001b[38;5;241m=\u001b[39mrevision, repo_type\u001b[38;5;241m=\u001b[39mrepo_type, timeout\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mHF_HUB_ETAG_TIMEOUT\n\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (RepositoryNotFoundError, HFValidationError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     validate_repo_id(arg_value)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:160\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './models'.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/unsloth_zoo/saving_utils.py:545\u001b[0m, in \u001b[0;36mmerge_and_overwrite_lora\u001b[0;34m(get_model_name, model, tokenizer, save_directory, push_to_hub, private, token, output_dtype, low_disk_space_usage, use_temp_file, cleanup_temp_file)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 545\u001b[0m     file_list \u001b[38;5;241m=\u001b[39m HfFileSystem(token \u001b[38;5;241m=\u001b[39m token)\u001b[38;5;241m.\u001b[39mls(model_name, detail \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:368\u001b[0m, in \u001b[0;36mHfFileSystem.ls\u001b[0;34m(self, path, detail, refresh, revision, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03mList the contents of a directory.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    dictionaries (if detail=True).\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m resolved_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_path(path, revision\u001b[38;5;241m=\u001b[39mrevision)\n\u001b[1;32m    369\u001b[0m path \u001b[38;5;241m=\u001b[39m resolved_path\u001b[38;5;241m.\u001b[39munresolve()\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:216\u001b[0m, in \u001b[0;36mHfFileSystem.resolve_path\u001b[0;34m(self, path, revision)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m repo_and_revision_exist:\n\u001b[0;32m--> 216\u001b[0m         _raise_file_not_found(path, err)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:1136\u001b[0m, in \u001b[0;36m_raise_file_not_found\u001b[0;34m(path, err)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (invalid repository id)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: ./models/gemma-3-1b-it (invalid repository id)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m: \u001b[38;5;66;03m# Change to True to save finetune!\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m     model\u001b[38;5;241m.\u001b[39msave_pretrained_merged(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma-3-finetune\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/unsloth/save.py:2357\u001b[0m, in \u001b[0;36munsloth_generic_save_pretrained_merged\u001b[0;34m(self, save_directory, tokenizer, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   2355\u001b[0m arguments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   2356\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m arguments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m-> 2357\u001b[0m unsloth_generic_save(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39marguments)\n\u001b[1;32m   2358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m   2359\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/unsloth/save.py:2304\u001b[0m, in \u001b[0;36munsloth_generic_save\u001b[0;34m(model, tokenizer, save_directory, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, use_temp_dir, commit_message, private, create_pr, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   2274\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39minference_mode\n\u001b[1;32m   2275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munsloth_generic_save\u001b[39m(\n\u001b[1;32m   2276\u001b[0m     model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2301\u001b[0m     maximum_memory_usage : \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m   2302\u001b[0m ):\n\u001b[1;32m   2303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m push_to_hub: token \u001b[38;5;241m=\u001b[39m get_token()\n\u001b[0;32m-> 2304\u001b[0m     merge_and_overwrite_lora(\n\u001b[1;32m   2305\u001b[0m         get_model_name,\n\u001b[1;32m   2306\u001b[0m         model                \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m   2307\u001b[0m         tokenizer            \u001b[38;5;241m=\u001b[39m tokenizer,\n\u001b[1;32m   2308\u001b[0m         save_directory       \u001b[38;5;241m=\u001b[39m save_directory,\n\u001b[1;32m   2309\u001b[0m         push_to_hub          \u001b[38;5;241m=\u001b[39m push_to_hub,\n\u001b[1;32m   2310\u001b[0m         private              \u001b[38;5;241m=\u001b[39m private,\n\u001b[1;32m   2311\u001b[0m         token                \u001b[38;5;241m=\u001b[39m token,\n\u001b[1;32m   2312\u001b[0m         output_dtype         \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2313\u001b[0m         low_disk_space_usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2314\u001b[0m         use_temp_file        \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   2315\u001b[0m     )\n\u001b[1;32m   2316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/unsloth_zoo/saving_utils.py:549\u001b[0m, in \u001b[0;36mmerge_and_overwrite_lora\u001b[0;34m(get_model_name, model, tokenizer, save_directory, push_to_hub, private, token, output_dtype, low_disk_space_usage, use_temp_file, cleanup_temp_file)\u001b[0m\n\u001b[1;32m    547\u001b[0m     original_model_id \u001b[38;5;241m=\u001b[39m get_original_model_id(model_name)\n\u001b[1;32m    548\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m original_model_id\n\u001b[0;32m--> 549\u001b[0m     file_list \u001b[38;5;241m=\u001b[39m HfFileSystem(token \u001b[38;5;241m=\u001b[39m token)\u001b[38;5;241m.\u001b[39mls(model_name, detail \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    551\u001b[0m safetensors_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    552\u001b[0m max_size_in_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:368\u001b[0m, in \u001b[0;36mHfFileSystem.ls\u001b[0;34m(self, path, detail, refresh, revision, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mls\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m, detail: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, refresh: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, revision: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    341\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Union[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]]:\n\u001b[1;32m    342\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    List the contents of a directory.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m        dictionaries (if detail=True).\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     resolved_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_path(path, revision\u001b[38;5;241m=\u001b[39mrevision)\n\u001b[1;32m    369\u001b[0m     path \u001b[38;5;241m=\u001b[39m resolved_path\u001b[38;5;241m.\u001b[39munresolve()\n\u001b[1;32m    370\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpand_info\u001b[39m\u001b[38;5;124m\"\u001b[39m: detail, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:172\u001b[0m, in \u001b[0;36mHfFileSystem.resolve_path\u001b[0;34m(self, path, revision)\u001b[0m\n\u001b[1;32m    169\u001b[0m         revision \u001b[38;5;241m=\u001b[39m revision_in_path\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m revision\n\u001b[0;32m--> 172\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strip_protocol(path)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# can't list repositories at root\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccess to repositories lists is not implemented.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/fsspec/spec.py:198\u001b[0m, in \u001b[0;36mAbstractFileSystem._strip_protocol\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    196\u001b[0m protos \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mprotocol,) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mprotocol, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mprotocol\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m protocol \u001b[38;5;129;01min\u001b[39;00m protos:\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mstartswith(protocol \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m://\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    199\u001b[0m         path \u001b[38;5;241m=\u001b[39m path[\u001b[38;5;28mlen\u001b[39m(protocol) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m :]\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mstartswith(protocol \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m::\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'startswith'"
     ]
    }
   ],
   "source": [
    "if True: # Change to True to save finetune!\n",
    "    model.save_pretrained_merged(\"gemma-3-finetune\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1a0885",
   "metadata": {},
   "source": [
    "### 保存为完整模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27072ac2",
   "metadata": {},
   "source": [
    "##### 保存为 bf16 格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f4381df",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'startswith'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:125\u001b[0m, in \u001b[0;36mHfFileSystem._repo_and_revision_exist\u001b[0;34m(self, repo_type, repo_id, revision)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api\u001b[38;5;241m.\u001b[39mrepo_info(\n\u001b[1;32m    126\u001b[0m         repo_id, revision\u001b[38;5;241m=\u001b[39mrevision, repo_type\u001b[38;5;241m=\u001b[39mrepo_type, timeout\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mHF_HUB_ETAG_TIMEOUT\n\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (RepositoryNotFoundError, HFValidationError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     validate_repo_id(arg_value)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:160\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './models'.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/unsloth_zoo/saving_utils.py:545\u001b[0m, in \u001b[0;36mmerge_and_overwrite_lora\u001b[0;34m(get_model_name, model, tokenizer, save_directory, push_to_hub, private, token, output_dtype, low_disk_space_usage, use_temp_file, cleanup_temp_file)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 545\u001b[0m     file_list \u001b[38;5;241m=\u001b[39m HfFileSystem(token \u001b[38;5;241m=\u001b[39m token)\u001b[38;5;241m.\u001b[39mls(model_name, detail \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:368\u001b[0m, in \u001b[0;36mHfFileSystem.ls\u001b[0;34m(self, path, detail, refresh, revision, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03mList the contents of a directory.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    dictionaries (if detail=True).\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m resolved_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_path(path, revision\u001b[38;5;241m=\u001b[39mrevision)\n\u001b[1;32m    369\u001b[0m path \u001b[38;5;241m=\u001b[39m resolved_path\u001b[38;5;241m.\u001b[39munresolve()\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:216\u001b[0m, in \u001b[0;36mHfFileSystem.resolve_path\u001b[0;34m(self, path, revision)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m repo_and_revision_exist:\n\u001b[0;32m--> 216\u001b[0m         _raise_file_not_found(path, err)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:1136\u001b[0m, in \u001b[0;36m_raise_file_not_found\u001b[0;34m(path, err)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (invalid repository id)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: ./models/gemma-3-1b-it (invalid repository id)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Merge to 16bit\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m: model\u001b[38;5;241m.\u001b[39msave_pretrained_merged(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer, save_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerged_16bit\u001b[39m\u001b[38;5;124m\"\u001b[39m,)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m: model\u001b[38;5;241m.\u001b[39mpush_to_hub_merged(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf/model\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer, save_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerged_16bit\u001b[39m\u001b[38;5;124m\"\u001b[39m, token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Merge to 4bit\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/unsloth/save.py:2357\u001b[0m, in \u001b[0;36munsloth_generic_save_pretrained_merged\u001b[0;34m(self, save_directory, tokenizer, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   2355\u001b[0m arguments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   2356\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m arguments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m-> 2357\u001b[0m unsloth_generic_save(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39marguments)\n\u001b[1;32m   2358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m   2359\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/unsloth/save.py:2304\u001b[0m, in \u001b[0;36munsloth_generic_save\u001b[0;34m(model, tokenizer, save_directory, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, use_temp_dir, commit_message, private, create_pr, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   2274\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39minference_mode\n\u001b[1;32m   2275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munsloth_generic_save\u001b[39m(\n\u001b[1;32m   2276\u001b[0m     model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2301\u001b[0m     maximum_memory_usage : \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m   2302\u001b[0m ):\n\u001b[1;32m   2303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m push_to_hub: token \u001b[38;5;241m=\u001b[39m get_token()\n\u001b[0;32m-> 2304\u001b[0m     merge_and_overwrite_lora(\n\u001b[1;32m   2305\u001b[0m         get_model_name,\n\u001b[1;32m   2306\u001b[0m         model                \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m   2307\u001b[0m         tokenizer            \u001b[38;5;241m=\u001b[39m tokenizer,\n\u001b[1;32m   2308\u001b[0m         save_directory       \u001b[38;5;241m=\u001b[39m save_directory,\n\u001b[1;32m   2309\u001b[0m         push_to_hub          \u001b[38;5;241m=\u001b[39m push_to_hub,\n\u001b[1;32m   2310\u001b[0m         private              \u001b[38;5;241m=\u001b[39m private,\n\u001b[1;32m   2311\u001b[0m         token                \u001b[38;5;241m=\u001b[39m token,\n\u001b[1;32m   2312\u001b[0m         output_dtype         \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2313\u001b[0m         low_disk_space_usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2314\u001b[0m         use_temp_file        \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   2315\u001b[0m     )\n\u001b[1;32m   2316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/unsloth_zoo/saving_utils.py:549\u001b[0m, in \u001b[0;36mmerge_and_overwrite_lora\u001b[0;34m(get_model_name, model, tokenizer, save_directory, push_to_hub, private, token, output_dtype, low_disk_space_usage, use_temp_file, cleanup_temp_file)\u001b[0m\n\u001b[1;32m    547\u001b[0m     original_model_id \u001b[38;5;241m=\u001b[39m get_original_model_id(model_name)\n\u001b[1;32m    548\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m original_model_id\n\u001b[0;32m--> 549\u001b[0m     file_list \u001b[38;5;241m=\u001b[39m HfFileSystem(token \u001b[38;5;241m=\u001b[39m token)\u001b[38;5;241m.\u001b[39mls(model_name, detail \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    551\u001b[0m safetensors_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    552\u001b[0m max_size_in_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:368\u001b[0m, in \u001b[0;36mHfFileSystem.ls\u001b[0;34m(self, path, detail, refresh, revision, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mls\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m, detail: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, refresh: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, revision: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    341\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Union[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]]:\n\u001b[1;32m    342\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    List the contents of a directory.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m        dictionaries (if detail=True).\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     resolved_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_path(path, revision\u001b[38;5;241m=\u001b[39mrevision)\n\u001b[1;32m    369\u001b[0m     path \u001b[38;5;241m=\u001b[39m resolved_path\u001b[38;5;241m.\u001b[39munresolve()\n\u001b[1;32m    370\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpand_info\u001b[39m\u001b[38;5;124m\"\u001b[39m: detail, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:172\u001b[0m, in \u001b[0;36mHfFileSystem.resolve_path\u001b[0;34m(self, path, revision)\u001b[0m\n\u001b[1;32m    169\u001b[0m         revision \u001b[38;5;241m=\u001b[39m revision_in_path\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m revision\n\u001b[0;32m--> 172\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strip_protocol(path)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# can't list repositories at root\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccess to repositories lists is not implemented.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/fsspec/spec.py:198\u001b[0m, in \u001b[0;36mAbstractFileSystem._strip_protocol\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    196\u001b[0m protos \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mprotocol,) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mprotocol, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mprotocol\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m protocol \u001b[38;5;129;01min\u001b[39;00m protos:\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mstartswith(protocol \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m://\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    199\u001b[0m         path \u001b[38;5;241m=\u001b[39m path[\u001b[38;5;28mlen\u001b[39m(protocol) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m :]\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mstartswith(protocol \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m::\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'startswith'"
     ]
    }
   ],
   "source": [
    "# Merge to 16bit\n",
    "if True: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if True: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde13d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # Change to True to upload finetune\n",
    "    model.push_to_hub_merged(\n",
    "        \"HF_ACCOUNT/gemma-3-finetune\", tokenizer,\n",
    "        token = \"hf_...\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8bbb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存为 GGUF 格式\n",
    "# if False:\n",
    "#     model.save_pretrained_gguf(\n",
    "#         \"gemma-3-finetune\",\n",
    "#         quantization_type = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a89623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if False: # Change to True to upload GGUF\n",
    "#     model.push_to_hub_gguf(\n",
    "#         \"gemma-3-finetune\",\n",
    "#         quantization_type = \"Q8_0\", # Only Q8_0, BF16, F16 supported\n",
    "#         repo_id = \"HF_ACCOUNT/gemma-finetune-gguf\",\n",
    "#         token = \"hf_...\",\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
