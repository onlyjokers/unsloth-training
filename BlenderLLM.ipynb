{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a7e62a6",
   "metadata": {},
   "source": [
    "### åŠ è½½\n",
    "#### åŠ è½½æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b9257",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-20 20:05:30 [__init__.py:256] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.3.19: Fast Gemma3 patching. Transformers: 4.50.0.dev0. vLLM: 0.8.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 Ti SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92ae5a5deb247a18b8c16244929f067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "from client import ClientSender\n",
    "import uuid\n",
    "import zmq\n",
    "import msgpack\n",
    "\n",
    "# å®ä¾‹åŒ–ç½‘ç»œä¼ è¾“å¯¹è±¡\n",
    "client = ClientSender(server_address=\"\", port=5555)\n",
    "\n",
    "max_seq_length = 2048 # æ¨¡å‹çš„æœ€å¤§åºåˆ—é•¿åº¦ï¼Œé»˜è®¤æ˜¯1024\n",
    "lora_rank = 8 # LoRAçš„ç§©ï¼Œè¶Šå¤§è¶Šå¥½ï¼Œä½†ä¼šæ¶ˆè€—æ›´å¤šå†…å­˜ #8\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"./models/gemma-3-4b-it\", #\"unsloth/gemma-3-1b-it\",\n",
    "    max_seq_length = max_seq_length, # å¯ä»¥é€‰æ‹©ä»»æ„é•¿åº¦ä»¥æ”¯æŒé•¿ä¸Šä¸‹æ–‡ï¼\n",
    "    load_in_4bit = True,  # 4ä½é‡åŒ–ä»¥å‡å°‘å†…å­˜ä½¿ç”¨\n",
    "    load_in_8bit = False, # ç²¾åº¦æ›´é«˜ï¼Œä½†ä½¿ç”¨2å€å†…å­˜\n",
    "    full_finetuning = False, # å®Œå…¨å¾®è°ƒ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e380ab",
   "metadata": {},
   "source": [
    "#### åŠ è½½ Lora è®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "127dd5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.language_model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # ä»…å¤„ç†æ–‡æœ¬å±‚æˆ–è€…æ¨¡å‹æ²¡æœ‰è§†è§‰å±‚æ—¶å…³é—­\n",
    "    finetune_language_layers   = True,  # åº”è¯¥ä¿æŒå¼€å¯ï¼\n",
    "    finetune_attention_modules = True,  # æ³¨æ„åŠ›æœºåˆ¶å¯¹GRPOæœ‰å¥½å¤„\n",
    "    finetune_mlp_modules       = True,  # åº”è¯¥å§‹ç»ˆä¿æŒå¼€å¯ï¼\n",
    "\n",
    "    r = lora_rank,           # æ›´å¤§ = æ›´é«˜çš„ç²¾åº¦ï¼Œä½†å¯èƒ½è¿‡æ‹Ÿåˆ\n",
    "    lora_alpha = lora_rank,  # å»ºè®®alphaè‡³å°‘ç­‰äºr\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407, # ä½¿ç”¨åŒä¸€ä¸ªéšæœºæ•°ç§å­\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ced53c",
   "metadata": {},
   "source": [
    "#### åŠ è½½ã€æ„é€ æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d619bc97",
   "metadata": {},
   "source": [
    "##### æ„é€ ç³»ç»Ÿæç¤ºè¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc3e53ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä½ æ˜¯ä¸€ä¸ª Blender çš„æè´¨ç”Ÿæˆå™¨ï¼Œä½ å°†ä¼šè€ƒè™‘é—®é¢˜å¹¶æä¾›æè´¨å¯¹åº”çš„ python ä»£ç ï¼Œè¯¥ä»£ç åº”è¯¥å¯ä»¥ä¸”ä»…åœ¨ Blender ä¸­åˆ›å»ºå¯¹åº”æè´¨ï¼Œä½ ç”Ÿæˆå‡ºçš„pythonä»£ç åº”å½“å°±æ˜¯æœ€ç»ˆç»“æœï¼Œç”¨æˆ·å¯ä»¥ç›´æ¥ä½¿ç”¨ï¼Œä¸éœ€è¦ç”¨æˆ·æ›´æ”¹ï¼Œä½ ä¹Ÿä¸ä¼šä½¿ç”¨ä»»ä½•å¤–éƒ¨æ–‡ä»¶ã€‚\\nè¯·å°†æ€è€ƒè¿‡ç¨‹æ”¾åœ¨ <think> å’Œ </think> ä¹‹é—´ã€‚\\nç„¶åï¼Œè¯·åœ¨ <code> å’Œ </code> ä¹‹é—´æä¾›ä½ çš„ç­”æ¡ˆã€‚'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è®¾ç½®ç³»ç»Ÿæç¤ºæ­¤\n",
    "reasoning_start = \"<think>\"\n",
    "reasoning_end   = \"</think>\"\n",
    "solution_start = \"<code>\"\n",
    "solution_end   = \"</code>\"\n",
    "\n",
    "system_prompt = \\\n",
    "f\"\"\"ä½ æ˜¯ä¸€ä¸ª Blender çš„æè´¨ç”Ÿæˆå™¨ï¼Œä½ å°†ä¼šè€ƒè™‘é—®é¢˜å¹¶æä¾›æè´¨å¯¹åº”çš„ python ä»£ç ï¼Œè¯¥ä»£ç åº”è¯¥å¯ä»¥ä¸”ä»…åœ¨ Blender ä¸­åˆ›å»ºå¯¹åº”æè´¨ï¼Œä½ ç”Ÿæˆå‡ºçš„pythonä»£ç åº”å½“å°±æ˜¯æœ€ç»ˆç»“æœï¼Œç”¨æˆ·å¯ä»¥ç›´æ¥ä½¿ç”¨ï¼Œä¸éœ€è¦ç”¨æˆ·æ›´æ”¹ï¼Œä½ ä¹Ÿä¸ä¼šä½¿ç”¨ä»»ä½•å¤–éƒ¨æ–‡ä»¶ã€‚\n",
    "è¯·å°†æ€è€ƒè¿‡ç¨‹æ”¾åœ¨ {reasoning_start} å’Œ {reasoning_end} ä¹‹é—´ã€‚\n",
    "ç„¶åï¼Œè¯·åœ¨ {solution_start} å’Œ {solution_end} ä¹‹é—´æä¾›ä½ çš„ç­”æ¡ˆã€‚\"\"\"\n",
    "system_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e63be34",
   "metadata": {},
   "source": [
    "##### æ„é€ æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f67b2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "dataset = []\n",
    "\n",
    "# è®¾ç½®ä¸‰ä¸ªä¸åŒçš„éš¾åº¦ç­‰çº§\n",
    "level1 = [\n",
    "    \"çº¢è‰²çš„æè´¨\", \"è“è‰²çš„æè´¨\", \"è“è‰²åˆ°é»„çš„æ¸å˜æè´¨\", \"ç»¿è‰²çš„æè´¨\", \"ç´«è‰²çš„æè´¨\", \"é‡‘è‰²çš„æè´¨\", \n",
    "    \"é“¶è‰²çš„æè´¨\", \"é€æ˜çš„æè´¨\", \"æ£•è‰²çš„æœ¨è´¨æè´¨\", \"ç™½è‰²çš„æè´¨\", \"é»‘è‰²çš„æè´¨\", \"ç°è‰²çš„æè´¨\",\n",
    "    \"æ©™è‰²çš„æè´¨\", \"ç²‰çº¢è‰²çš„æè´¨\", \"é»„è‰²çš„æè´¨\", \"æ·±çº¢è‰²çš„æè´¨\",     \"çº¢è‰²çš„æè´¨\", \"è“è‰²çš„æè´¨\", \"è“è‰²åˆ°é»„è‰²çš„æ¸å˜æè´¨\", \"ç»¿è‰²åˆ°é»„è‰²çš„æ¸å˜æè´¨\", \"ç´«è‰²åˆ°ç²‰è‰²çš„æ¸å˜æè´¨\",\n",
    "    \"é‡‘è‰²åˆ°é“¶è‰²çš„æ¸å˜æè´¨\", \"ç»¿è‰²åˆ°ç™½è‰²çš„æ¸å˜æè´¨\", \"æ©™è‰²åˆ°çº¢è‰²çš„æ¸å˜æè´¨\", \"çº¢è‰²åˆ°é»‘è‰²çš„æ¸å˜æè´¨\", \"é’è‰²çš„æè´¨\", \n",
    "    \"æ·±è“è‰²çš„æè´¨\", \"å¢¨ç»¿è‰²çš„æè´¨\", \"æ·¡è“è‰²çš„æè´¨\", \"ç±³è‰²çš„æè´¨\", \"ç‚­ç°è‰²çš„æè´¨\",\n",
    "    \"æµ…é»„è‰²çš„æè´¨\", \"å½©è™¹è‰²çš„æè´¨\", \"éœ“è™¹è‰²çš„æè´¨\", \"ç´«é»‘è‰²çš„æè´¨\", \"é‡‘å±è‰²çš„æè´¨\",\n",
    "    \"çš®é©æè´¨\", \"æ·±æ£•è‰²çš„æè´¨\", \"å¤§ç†çŸ³æè´¨\", \"æ²™åœŸæè´¨\", \"äº®å…‰æè´¨\", \n",
    "    \"äº®é‡‘è‰²çš„æè´¨\", \"æš—æ£•è‰²çš„æè´¨\", \"æ¶²ä½“æè´¨\", \"å†·ç™½è‰²çš„æè´¨\", \"å¡‘æ–™æè´¨\",\n",
    "    \"æ·±è“è‰²çš„æè´¨\", \"çƒ­æ°”æµæè´¨\", \"é€æ˜çš„æè´¨\", \"å†°å—æè´¨\", \"æ·±è“åˆ°æµ…è“çš„æ¸å˜æè´¨\", \"è“è‰²åˆ°ç´«è‰²çš„æ¸å˜æè´¨\", \"ç²‰è‰²åˆ°ç´«è‰²çš„æ¸å˜æè´¨\", \"çº¢è‰²åˆ°ç»¿è‰²çš„æ¸å˜æè´¨\",\n",
    "    \"é»„è‰²åˆ°ç»¿è‰²çš„æ¸å˜æè´¨\", \"æ·±ç´«åˆ°æµ…ç´«çš„æ¸å˜æè´¨\", \"ç´«è‰²åˆ°è“è‰²çš„æ¸å˜æè´¨\", \"è“è‰²åˆ°ç™½è‰²çš„æ¸å˜æè´¨\", \n",
    "    \"é’è‰²åˆ°ç™½è‰²çš„æ¸å˜æè´¨\", \"æ©™è‰²åˆ°é»„è‰²çš„æ¸å˜æè´¨\", \"é»‘è‰²åˆ°ç°è‰²çš„æ¸å˜æè´¨\"\n",
    "]\n",
    "level2 = [\n",
    "    \"çº¢è‰²çš„é‡‘å±æè´¨ï¼šè¿™ç§æè´¨è¡¨é¢å…‰æ»‘ï¼Œé‡‘å±å…‰æ³½éå¸¸çªå‡ºï¼Œé€‚åˆè¡¨ç°ç§‘æŠ€æ„Ÿå¼ºçš„ç‰©ä½“ï¼Œç»™äººä¸€ç§åšç¡¬è€Œç°ä»£çš„æ„Ÿè§‰ã€‚\",\n",
    "    \"è“è‰²çš„é‡‘å±æè´¨ï¼šå®ƒå‘ˆç°å‡ºæ·±é‚ƒçš„è“è‰²ï¼Œé‡‘å±åå°„æ•ˆæœæ˜æ˜¾ï¼Œé€‚åˆç”¨åœ¨ç²¾å¯†æœºæ¢°æˆ–æœªæ¥æ„Ÿåè¶³çš„è®¾è®¡ä¸­ã€‚\",\n",
    "    \"çº¢è‰²çš„æœ¨å¤´æè´¨ï¼šè¿™ç§æœ¨æçš„è¡¨é¢æœ‰æ˜æ˜¾çš„çº¹ç†ï¼Œé¢œè‰²é²œè‰³ä¸”å¯Œæœ‰è‡ªç„¶æ„Ÿï¼Œé€‚åˆåˆ¶ä½œæ¸©æš–ã€è‡ªç„¶çš„ç¯å¢ƒã€‚\",\n",
    "    \"æ·±ç´«è‰²çš„é’¢é“æè´¨ï¼šè¿™ç§æè´¨æ··åˆäº†ç´«è‰²å’Œç°è‰²ï¼Œå…·æœ‰è¾ƒå¼ºçš„è§†è§‰å†²å‡»æ„Ÿï¼Œé€‚åˆç”¨äºè¡¨ç°åšå›ºè€Œç¥ç§˜çš„ç‰©ä½“ã€‚\",\n",
    "    \"ç»¿æ¾çŸ³è‰²çš„å¡‘æ–™æè´¨ï¼šè¯¥æè´¨å‘ˆç°å‡ºç»¿è‰²çš„å…‰æ³½æ„Ÿï¼Œç»™äººä¸€ç§æ¸…æ–°è‡ªç„¶çš„æ„Ÿè§‰ï¼Œé€‚åˆç”¨äºç°ä»£ç®€çº¦é£æ ¼ã€‚\",\n",
    "    \"é‡‘è‰²çš„å…‰æ»‘æè´¨ï¼šå®ƒçš„è¡¨é¢éå¸¸å…‰æ»‘ï¼Œåå°„æ•ˆæœå¼ºï¼Œé€šå¸¸ç”¨äºå¥¢åæˆ–é«˜è´µçš„è®¾è®¡é£æ ¼ä¸­ã€‚\",\n",
    "    \"é“¶ç™½è‰²çš„é’›é‡‘å±æè´¨ï¼šè¯¥æè´¨å…·æœ‰å†·å…‰æ„Ÿå’Œé‡‘å±è´¨æ„Ÿï¼Œè´¨åœ°éå¸¸åšç¡¬ä¸”è€ç”¨ï¼Œé€‚åˆç”¨äºç§‘æŠ€äº§å“æˆ–é«˜çº§é¥°å“ã€‚\",\n",
    "    \"ç™½è‰²çš„ç“·ç –æè´¨ï¼šè¿™ç§æè´¨è¡¨é¢å¹³æ•´ä¸”å…·æœ‰å…‰æ³½ï¼Œé€‚åˆç”¨äºç°ä»£ç®€çº¦æˆ–æ¸…æ–°é£æ ¼çš„ç©ºé—´è£…é¥°ã€‚\",\n",
    "    \"é‡‘å±è´¨æ„Ÿçš„æ·±ç°è‰²æè´¨ï¼šè¿™ç§æè´¨è¡¨é¢æœ‰ç€æ˜æ˜¾çš„é‡‘å±å…‰æ³½ï¼Œæ·±ç°è‰²è°ƒç»™äººç¨³é‡ã€é«˜ç§‘æŠ€çš„æ„Ÿè§‰ã€‚\",\n",
    "    \"ç»¿è‰²çš„ç»ç’ƒæè´¨ï¼šè¿™ç§ç»ç’ƒè¡¨é¢å‘ˆç°å‡ºç»¿è‰²çš„é€å…‰æ•ˆæœï¼Œå…·æœ‰è‰¯å¥½çš„é€æ˜åº¦ï¼Œé€‚åˆç”¨äºè£…é¥°æˆ–å¤–å¢™æè´¨ã€‚\",\n",
    "    \"æ©™è‰²çš„é™¶ç“·è´¨æ„Ÿæè´¨ï¼šæè´¨è¡¨é¢æœ‰ç»†è…»çš„çº¹ç†å’Œæ¸©æš–çš„æ©™è‰²è°ƒï¼Œé€‚åˆç”¨åœ¨èˆ’é€‚ã€æ¸©é¦¨çš„ç¯å¢ƒä¸­ã€‚\",\n",
    "    \"æ·±è“è‰²çš„é“åˆé‡‘æè´¨ï¼šå®ƒè¡¨é¢å¹³æ»‘ä¸”åšå›ºï¼Œæ·±è“è‰²ä½¿å…¶æ›´å…·ç§‘æŠ€æ„Ÿï¼Œé€‚åˆåº”ç”¨äºé«˜ç«¯ç”µå­äº§å“ã€‚\",\n",
    "    \"æ·±çº¢è‰²çš„ç¡¬æœ¨æè´¨ï¼šè¡¨é¢æœ‰ç€æ¸…æ™°çš„æœ¨çº¹ï¼Œæ·±çº¢è‰²ä½¿å¾—æè´¨æ›´æ˜¾é«˜è´µå’Œä¼ ç»Ÿï¼Œé€‚ç”¨äºå¤å…¸é£æ ¼çš„è®¾è®¡ã€‚\",\n",
    "    \"é«˜äº®é»‘è‰²çš„ç‚­æè´¨ï¼šè¡¨é¢å‘ˆç°å‡ºé»‘è‰²çš„å…‰æ³½ï¼Œç»™äººä¸€ç§ç°ä»£ã€ç®€çº¦çš„æ„Ÿè§‰ï¼Œé€‚åˆæç®€é£æ ¼çš„è®¾è®¡ã€‚\",\n",
    "    \"æµ…é»„è‰²çš„é‡‘å±æ°§åŒ–ç‰©æè´¨ï¼šè¿™æ¬¾æè´¨å‘ˆç°æµ…é»„è‰²ï¼Œè¡¨é¢æœ‰ä¸€å®šçš„é‡‘å±è´¨æ„Ÿï¼Œé€‚åˆç”¨äºç°ä»£è‰ºæœ¯å“æˆ–è£…é¥°ã€‚\",\n",
    "    \"ç´«è‰²çš„æœ‰æœºç»ç’ƒæè´¨ï¼šå®ƒå‘ˆç°å‡ºç´«è‰²çš„é€æ˜æ•ˆæœï¼Œç»™äººä¸€ç§ç°ä»£ã€æ¢¦å¹»çš„æ„Ÿè§‰ï¼Œé€‚åˆç”¨äºåˆ›æ„è®¾è®¡ä¸­ã€‚\",\n",
    "    \"é“ç°è‰²çš„é“åˆé‡‘æè´¨ï¼šè¡¨é¢åšç¡¬ä¸”å…·é‡‘å±å…‰æ³½ï¼Œæ·±æ²‰çš„é“ç°è‰²ç»™äººç¨³é‡ã€å·¥ä¸šæ„Ÿå¼ºçš„è§†è§‰æ•ˆæœã€‚\",\n",
    "    \"çº¢è‰²çš„äº®é¢æœ¨è´¨ææ–™ï¼šçº¢è‰²æœ¨è´¨ææ–™çš„å…‰æ³½æ„Ÿéå¸¸æ˜æ˜¾ï¼Œè‰²è°ƒæ˜äº®ä¸”æ¸©æš–ï¼Œé€‚åˆç”¨äºå®¶å…·å’Œå®¤å†…è£…é¥°ã€‚\",\n",
    "    \"é“œè‰²çš„è€åŒ–é‡‘å±æè´¨ï¼šè¿™ç§æè´¨çš„è¡¨é¢æœ‰ç€å²æœˆçš„ç—•è¿¹ï¼Œå¸¦æœ‰é‡‘å±çš„å…‰æ³½å’Œè€åŒ–æ•ˆæœï¼Œé€‚åˆå¤å¤é£æ ¼è®¾è®¡ã€‚\",\n",
    "    \"é’¢é“è´¨æ„Ÿçš„è“è‰²æè´¨ï¼šè¿™ç§æè´¨åšç¡¬ä¸”å…·æœ‰å†·é‡‘å±å…‰æ³½ï¼Œæ·±è“è‰²å¢åŠ äº†å®ƒçš„å†·å³»æ„Ÿï¼Œé€‚ç”¨äºå·¥ä¸šè®¾è®¡æˆ–æœºæ¢°æ„ä»¶ã€‚\"\n",
    "]\n",
    "\n",
    "level3 = [\n",
    "    \"çº¢è‰²çš„æ‹‰ä¸é‡‘å±æè´¨ï¼šè¿™ç§æè´¨å…·æœ‰æ‹‰ä¸æ•ˆæœçš„é‡‘å±è¡¨é¢ï¼Œè¡¨é¢ä¸Šæœ‰ä¸€å±‚ç»†å¾®çš„åˆ’ç—•ï¼ŒæŠ˜å°„å‡ºçº¢è‰²å…‰æ³½ã€‚æ‹‰ä¸é‡‘å±å…·æœ‰éå¸¸ç‹¬ç‰¹çš„è§†è§‰æ•ˆæœï¼Œå¸¸ç”¨äºé«˜ç«¯äº§å“çš„å¤–å£³è®¾è®¡ã€‚å®ƒä¸ä»…å…·å¤‡é‡‘å±è´¨æ„Ÿçš„åšå›ºï¼Œè¿˜èƒ½é€šè¿‡è¡¨é¢çº¹ç†ä¼ é€’å‡ºä¸€ç§ç²¾ç»†ä¸”ç‹¬ç‰¹çš„å·¥ä¸šé£æ ¼ã€‚é€‚åˆç”¨åœ¨ç”µå­è®¾å¤‡ã€æ±½è½¦è®¾è®¡æˆ–ä»»ä½•éœ€è¦å±•ç°ç°ä»£æ„Ÿå’Œç²¾è‡´æ„Ÿçš„ç‰©å“ä¸Šã€‚\",\n",
    "    \"æ¤…å­ä¸Šçš„çº¹è·¯ï¼šè¿™æ¬¾æè´¨çš„çº¹ç†å‘ˆç°å‡ºç²¾è‡´çš„èŠ±çº¹ï¼Œå¯èƒ½æ˜¯åœ†å½¢ã€å¯¹ç§°æˆ–è€…ä¸è§„åˆ™çš„å›¾æ¡ˆï¼Œè¡¨é¢çœ‹èµ·æ¥æ—¢èˆ’é€‚åˆå…·æœ‰è§†è§‰å†²å‡»åŠ›ã€‚æ¤…å­ä¸Šçš„çº¹ç†æè´¨å¸¸å¸¸æ˜¯é€šè¿‡ç²¾ç»†çš„ç»‡ç‰©æˆ–è€…çš®é©ç­‰æè´¨å‘ˆç°ï¼Œç»™ä½¿ç”¨è€…å¸¦æ¥è§†è§‰ä¸Šçš„äº«å—ä¸èˆ’é€‚çš„è§¦æ„Ÿã€‚è®¾è®¡å¸ˆå¾€å¾€ä¼šåˆ©ç”¨è¿™äº›çº¹è·¯æ¥å¢å¼ºåº§æ¤…çš„ç¾æ„Ÿä¸èˆ’é€‚åº¦ï¼Œç‰¹åˆ«é€‚ç”¨äºè±ªåæ²™å‘å’Œé«˜ç«¯åŠå…¬æ¤…ã€‚\",\n",
    "    \"ç§‘å¹»æˆ˜èˆ°çš„çº¹è·¯ï¼šè¿™ç§æè´¨æ˜¯ä¸ºäº†æ‰“é€ ç§‘å¹»é£æ ¼çš„æˆ˜èˆ°å¤–å£³æ•ˆæœè€Œè®¾è®¡ï¼Œè¡¨é¢æœ‰ç±»ä¼¼èˆªç©ºææ–™çš„çº¹ç†å’Œç»“æ„ã€‚å…‰æ»‘çš„è¡¨é¢ä¸Šå¸ƒæ»¡äº†å¤æ‚çš„æœºæ¢°çº¹è·¯å’Œç»†è‡´çš„é‡‘å±æ‹¼æ¥æ•ˆæœï¼Œé€šå¸¸æ­é…å†·å…‰çš„æè´¨å’Œå¼ºçƒˆçš„é‡‘å±è´¨æ„Ÿã€‚é€‚ç”¨äºæœªæ¥ç§‘æŠ€ã€ç§‘å¹»ç”µå½±æˆ–é«˜ç«¯æ¨¡å‹çš„è®¾è®¡ï¼Œç»™äººä¸€ç§é«˜ç§‘æŠ€ã€åšå›ºã€ä¸”å……æ»¡æœªæ¥æ„Ÿçš„è§†è§‰ä½“éªŒã€‚\",\n",
    "    \"æ·±è“è‰²çš„æŠ›å…‰é‡‘å±æè´¨ï¼šæ­¤æè´¨é‡‡ç”¨æ·±è“è‰²é‡‘å±åˆé‡‘ï¼Œå…¶è¡¨é¢ç»è¿‡ç²¾ç»†çš„æŠ›å…‰å¤„ç†ï¼Œå±•ç°å‡ºå…‰æ»‘å¦‚é•œçš„æ•ˆæœã€‚å…‰æ³½åº¦å’Œåå°„æ•ˆæœéå¸¸å¼ºï¼Œç»™äººä¸€ç§ç°ä»£å’Œé«˜ç«¯çš„æ„Ÿè§‰ã€‚æ·±è“è‰²è°ƒä½¿å¾—æè´¨æ›´åŠ å…·æœ‰ç¥ç§˜æ„Ÿï¼Œé€‚åˆç”¨äºèˆªç©ºèˆªå¤©ã€ç²¾å¯†æœºæ¢°æˆ–ä»»ä½•éœ€è¦å¼ºçƒˆè§†è§‰å†²å‡»çš„é«˜ç§‘æŠ€è®¾è®¡ã€‚\",\n",
    "    \"æ°´é¢åå°„çš„ç´«è‰²é‡‘å±æè´¨ï¼šè¿™ç§æè´¨è¡¨é¢å‘ˆç°å‡ºç´«è‰²å’Œè“è‰²çš„æ¸å˜æ•ˆæœï¼Œä»¿ä½›æ°´é¢åå°„çš„å…‰è¾‰ã€‚é‡‘å±è´¨æ„Ÿç»“åˆäº†æ°´é¢èˆ¬çš„æµåŠ¨æ„Ÿï¼Œå…‰çº¿åœ¨è¡¨é¢æŠ˜å°„ï¼Œäº§ç”Ÿå¤šå±‚æ¬¡çš„è§†è§‰æ•ˆæœã€‚é€‚åˆç”¨äºæœªæ¥ç§‘æŠ€ã€ç§‘å¹»å½±è§†é“å…·ï¼Œæˆ–è€…é«˜ç«¯è‰ºæœ¯ä½œå“ä¸­çš„å…ƒç´ ï¼Œèƒ½å¤Ÿå¸¦ç»™äººä¸€ç§æä¸ºç‹¬ç‰¹çš„æ„Ÿè§‰ã€‚\",\n",
    "    \"ç™½è‰²çš®é©çš„çº¹ç†ï¼šè¿™æ¬¾çš®é©æè´¨è¡¨é¢å‘ˆç°å‡ºç²¾ç»†çš„çº¹è·¯ï¼Œè´¨æ„ŸæŸ”è½¯ä¸”å¯Œæœ‰å¼¹æ€§ï¼Œç™½è‰²ä½¿å¾—å®ƒæ˜¾å¾—æ¸…æ–°ä¸”é«˜è´µã€‚å¸¸ç”¨äºæ—¶å°šå“ç‰Œçš„æœè£…ã€é‹åŒ…è®¾è®¡æˆ–è€…è±ªåå®¶å…·ä¸­ã€‚å®ƒä¸ä»…å…·å¤‡çš®é©çš„è€ç”¨æ€§ï¼Œè¿˜é€šè¿‡çº¹ç†çš„å˜åŒ–æå‡äº†æ•´ä½“çš„ç¾æ„Ÿä¸è§¦æ„Ÿï¼Œæˆä¸ºæ—¶å°šå’Œå“å‘³çš„è±¡å¾ã€‚\",\n",
    "    \"è“è‰²å…‰æ³½çš„é“æ¿æè´¨ï¼šé“æ¿è¡¨é¢é‡‡ç”¨äº†ç‰¹æ®Šçš„è“è‰²å…‰æ³½æ¶‚å±‚ï¼Œå½¢æˆäº†ä¸€ç§æå…·ç°ä»£æ„Ÿå’Œç§‘æŠ€æ„Ÿçš„è§†è§‰æ•ˆæœã€‚è¡¨é¢è™½ç„¶æœ‰é‡‘å±çš„åšç¡¬æ„Ÿï¼Œä½†ç”±äºå…‰æ³½çš„åå°„ï¼Œè¡¨ç°å‡ºäº†ç‹¬ç‰¹çš„åŠ¨æ„Ÿã€‚é€‚åˆç”¨äºç”µå­äº§å“ã€å»ºç­‘è£…é¥°æˆ–è€…å…¶ä»–éœ€è¦åšå›ºä¸”å…·æœ‰ç°ä»£æ„Ÿçš„äº§å“è®¾è®¡ã€‚\",\n",
    "    \"é“åˆé‡‘è¡¨é¢çš„ç»†å°çº¹ç†ï¼šè¿™ç§æè´¨çš„è¡¨é¢ç»†è…»ä¸”å‡åŒ€ï¼Œå‘ˆç°å‡ºç»†å°çš„é“åˆé‡‘çº¹ç†ï¼Œæ•´ä½“ä¸Šæ—¢åšå›ºåˆå…·æœ‰éå¸¸ç²¾è‡´çš„å·¥è‰ºæ„Ÿã€‚é€‚åˆç”¨äºç²¾å¯†æœºæ¢°ã€èˆªç©ºå™¨æä»¥åŠç°ä»£å»ºç­‘çš„è®¾è®¡ã€‚å…¶è¡¨é¢çº¹ç†ä¸ä»…æé«˜äº†æŠ—ç£¨æŸçš„èƒ½åŠ›ï¼Œè¿˜èƒ½ä½¿å¾—æ•´ä½“è®¾è®¡æ˜¾å¾—æ›´å…·ç°ä»£ç§‘æŠ€æ„Ÿã€‚\",\n",
    "    \"æ·±æ£•è‰²çš„æœ¨çº¹çº¹ç†ï¼šè¿™ç§æœ¨æè¡¨é¢å‘ˆç°å‡ºæ·±æ£•è‰²çš„çº¹ç†ï¼Œçº¹è·¯è‡ªç„¶ä¸”æ¸…æ™°ï¼Œå¸¦æœ‰æµ“åšçš„è‡ªç„¶æ°”æ¯ã€‚å¸¸ç”¨äºå®¶å…·ã€åœ°æ¿æˆ–å®¤å†…è£…é¥°ä¸­ï¼Œèƒ½å¤Ÿä¸ºç©ºé—´å¸¦æ¥æ¸©æš–ä¸”èˆ’é€‚çš„æ°›å›´ã€‚æœ¨çº¹è´¨æ„Ÿçš„å˜åŒ–ä½¿å¾—æ¯ä¸€ä»¶äº§å“éƒ½æœ‰ç‹¬ç‰¹çš„ä¸ªæ€§ï¼Œé€‚åˆç”¨äºä¼ ç»Ÿæˆ–ç°ä»£æ··åˆé£æ ¼çš„å®¤å†…è®¾è®¡ã€‚\",\n",
    "    \"ç»å…¸çš„é‡‘å±æ‹‰ä¸æ•ˆæœï¼šè¿™æ¬¾é‡‘å±æè´¨çš„è¡¨é¢å…·æœ‰ç»å…¸çš„æ‹‰ä¸æ•ˆæœï¼Œé€šè¿‡é‡‘å±è¡¨é¢ç»†è‡´çš„åˆ·çº¹å¤„ç†ï¼Œåˆ›é€ å‡ºç‹¬ç‰¹çš„è§†è§‰æ•ˆæœã€‚æ‹‰ä¸é‡‘å±ä¸ä»…èƒ½å¤Ÿåå°„å…‰çº¿ï¼Œè¿˜èµ‹äºˆè¡¨é¢ä¸€ç§ç²¾è‡´ä¸”ç®€çº¦çš„å¤–è§‚ï¼Œé€‚åˆç”¨äºç°ä»£å®¶å±…ã€ç”µå­äº§å“ä»¥åŠé«˜ç«¯è£…é¥°ç‰©å“ä¸­ã€‚\"\n",
    "]\n",
    "\n",
    "# åˆå¹¶æè´¨è¯·æ±‚\n",
    "tasks = level1 + level2 + level3\n",
    "\n",
    "# ä»»åŠ¡å‰ç¼€åˆ—è¡¨\n",
    "user_start = [\"åšè¿™ä¸ªæè´¨:\", \"å¸®æˆ‘ç”Ÿæˆè¿™ä¸ªï¼š\", \"è¿™ä¸ªé—®é¢˜éœ€è¦ä½ çš„å¸®åŠ©, å¸®æˆ‘ç”Ÿæˆ\", \"æˆ‘å¸Œæœ›ä½ èƒ½å¸®åŠ©æˆ‘ç”Ÿæˆ\", \"è¯·å¸®æˆ‘åšä¸€ä¸ªæè´¨\", \"å¸Œæœ›ä½ å¸®æˆ‘ç”Ÿæˆ\", \"è¯·ä½ ä¸ºæˆ‘åšä¸€ä¸ª\", \"ä½ èƒ½åšä¸€ä¸‹è¿™ä¸ªå—?\", \"\", \"è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡:\", \"å¸®æˆ‘æä¸€ä¸ªæè´¨:\", \"èƒ½å¦å¸®æˆ‘ç”Ÿæˆ\", \"è¯·ååŠ©æˆ‘ç”Ÿæˆä¸€ä¸ª\"]\n",
    "\n",
    "# æ„é€ æ•°æ®é›†\n",
    "for task in tasks:\n",
    "    user_prompts = user_start + user_start\n",
    "    random.shuffle(user_prompts)\n",
    "    for user in user_prompts:\n",
    "        dataset.append({\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user + task},\n",
    "            ],\n",
    "            \"taskid\": uuid.uuid4().hex,\n",
    "            \"goal\": task,\n",
    "        })\n",
    "    \n",
    "# è¾“å‡ºæœ€ç»ˆæ•°æ®é›†\n",
    "final_dataset = Dataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faaf472",
   "metadata": {},
   "source": [
    "### å®šä¹‰å¥–åŠ±å‡½æ•°\n",
    "#### å®šä¹‰æ ‡å‡†æ ¼å¼å½¢å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82ab0285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 42), match='<think>Let me think!</think><code>2</code>'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼ï¼Œç”¨æ¥åˆ¤æ–­æ¨¡å‹çš„è¾“å‡ºæ˜¯å¦ç¬¦åˆæ ¼å¼è¦æ±‚\n",
    "match_format = re.compile(\n",
    "    rf\"^[\\s]{{0,}}\"\\\n",
    "    rf\"<think>.+?</think>.*?\"\\\n",
    "    rf\"<code>(.+?)</code>\"\\\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "match_format.search(\n",
    "    \"<think>Let me think!</think>\"\\\n",
    "    \"<code>2</code>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f1a09b",
   "metadata": {},
   "source": [
    "#### æ„é€ å¥–åŠ±å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f1c793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸¥æ ¼æ ¼å¼åˆ¤æ–­å‡½æ•°\n",
    "def match_format_exactly(completions, **kwargs):\n",
    "    \"\"\"æ ¼å¼åˆ¤æ–­å‡½æ•°ï¼Œä¸¥æ ¼åˆ¤æ–­æ ¼å¼æ˜¯å¦åŒ¹é…\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Match if format is seen exactly!\n",
    "        if match_format.search(response) is not None: score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13513da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¼±æ ¼å¼åˆ¤æ–­å‡½æ•°\n",
    "def match_format_approximately(prompts, completions, **kwargs):\n",
    "    \"\"\"å¼±æ ¼å¼åˆ¤æ–­å¥–åŠ±ï¼Œå³ä½¿æ²¡æœ‰ä¸¥æ ¼å¯¹åº”ï¼Œä¹Ÿå¯ä»¥æ ¹æ®ä½¿ç”¨çš„æ ‡ç­¾æ•°é‡æ¥åšå‡ºç›¸åº”çš„å¥–åŠ±\n",
    "    \"\"\"\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "        \n",
    "    print('*'*20, f\"Question:\\n{question}\", f\"\\nResponse:\\n{responses[0]}\")\n",
    "    \n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # æ•°ä¸€æ•°çœ‹åˆ°å¤šå°‘ä¸ªå…³é”®è¯â€”â€”å¦‚æœå¤ªå¤šï¼Œæˆ‘ä»¬ä¼šæƒ©ç½šä½ ï¼\n",
    "        # å¦‚æœæˆ‘ä»¬çœ‹åˆ°1ä¸ªå…³é”®è¯ï¼Œé‚£ä¹ˆåŠ ä¸€äº›ç§¯åˆ†ï¼å¦‚æœæ›´å¤šäº†ï¼Œé‚£ä¹ˆå°±åº”å½“æ‰£é™¤ä¸€äº›åˆ†\n",
    "        score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n",
    "        score += 0.5 if response.count(reasoning_end)   == 1 else -0.5\n",
    "        score += 0.5 if response.count(solution_start)  == 1 else -0.5\n",
    "        score += 0.5 if response.count(solution_end)    == 1 else -0.5\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "618e3980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è·å–ä»£ç \n",
    "def code_extractor(code):\n",
    "    match = re.search(r'<code>(.*?)</code>', code, flags=re.DOTALL)\n",
    "    if not match:\n",
    "        return \"\"\n",
    "    content = match.group(1).strip()\n",
    "    if not content:\n",
    "        return \"\"\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33cdb864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆçš„å›¾åƒå’Œæè¿°çš„ç›¸ä¼¼åº¦\n",
    "def accuracy_reward(goal, taskid, completions, prompts, **kwargs):\n",
    "    \"\"\"è®¡ç®—ç”Ÿæˆçš„å›¾åƒå’Œæè¿°çš„ç›¸ä¼¼åº¦\n",
    "    \"\"\"\n",
    "    \n",
    "    WEIGHT = 2 # ç”¨æ¥åœ¨å½’ä¸€åŒ–ä¹‹ååŠ æƒ\n",
    "    scores = []\n",
    "    names = []\n",
    "    \n",
    "    # æ„é€ ä¼ è¾“å¯¹è±¡\n",
    "    materials = {\n",
    "        \"head\": {\n",
    "            \"input\": goal[0],\n",
    "            \"taskid\": taskid[0],\n",
    "            \"request\": []\n",
    "        },\n",
    "        \"outputs\": []\n",
    "    }\n",
    "    \n",
    "    # å¡«å……æè´¨ä»£ç \n",
    "    for completion in completions:\n",
    "        response = completion[0][\"content\"]\n",
    "        code = code_extractor(response)\n",
    "        print(\"AR_CODE________________\")\n",
    "        print(type(code))\n",
    "        print(code)\n",
    "        \n",
    "        name = f\"M{len(materials['outputs'])+1}\"\n",
    "        names.append(name)\n",
    "        \n",
    "        materials[\"outputs\"].append({\n",
    "            \"name\": name,\n",
    "            \"code\": code\n",
    "        })\n",
    "    \n",
    "    print(\"AR_MT________________\")\n",
    "    print(type(materials))\n",
    "    print(materials)\n",
    "\n",
    "    # æ·»åŠ åˆ†æ•°\n",
    "    c =  client.send_materials(materials)\n",
    "    print(\"AR_P________________\")\n",
    "    print(type(c))\n",
    "    print(c)\n",
    "    results = c.get(\"accuracy_rank\", {})\n",
    "\n",
    "    \n",
    "    for name in names:\n",
    "        score = int(results.get(name, 0))\n",
    "        scores.append(score)\n",
    "    \n",
    "    # å½’ä¸€åŒ–å¹¶åŠ æƒ\n",
    "    min_s, max_s = min(scores), max(scores)\n",
    "    if max_s > min_s:\n",
    "        scores[:] = [(s - min_s) / (max_s - min_s) for s in scores]\n",
    "        # å¯¹åˆ†æ•°åŠ æƒ\n",
    "        scores = [s * WEIGHT for s in scores]\n",
    "    else:\n",
    "        scores = [0,0,0,0]\n",
    "    \n",
    "    # è¿”å›åˆ†æ•°\n",
    "    print(\"accuracy_reward\" + str(results))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8124a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å›¾åƒæ˜¯å¦æœ‰æ„ä¹‰\n",
    "def meaning_reward(goal, taskid, completions, **kwargs):\n",
    "    \"\"\"è®¡ç®—ç”Ÿæˆçš„å›¾åƒæ˜¯å¦æœ‰æ„ä¹‰\n",
    "    \"\"\"\n",
    "    \n",
    "    WEIGHT = 1 # ç”¨æ¥åœ¨å½’ä¸€åŒ–ä¹‹ååŠ æƒ\n",
    "    scores = []\n",
    "    names = []\n",
    "    \n",
    "    # æ„é€ ä¼ è¾“å¯¹è±¡\n",
    "    materials = {\n",
    "        \"head\": {\n",
    "            \"input\": goal[0],\n",
    "            \"taskid\": taskid[0],\n",
    "            \"request\": []\n",
    "        },\n",
    "        \"outputs\": []\n",
    "    }\n",
    "    \n",
    "    # å¡«å……æè´¨ä»£ç \n",
    "    for completion in completions:\n",
    "        response = completion[0][\"content\"]\n",
    "        code = code_extractor(response)\n",
    "        print(\"MR_CODE________________\")\n",
    "        print(type(code))\n",
    "        print(code)\n",
    "        \n",
    "        name = f\"M{len(materials['outputs'])+1}\"\n",
    "        names.append(name)\n",
    "        \n",
    "        materials[\"outputs\"].append({\n",
    "            \"name\": name,\n",
    "            \"code\": code\n",
    "        })\n",
    "\n",
    "    print(\"MR_MT________________\")\n",
    "    print(type(materials))\n",
    "    print(materials)\n",
    "    \n",
    "    # æ·»åŠ åˆ†æ•°\n",
    "    c =  client.send_materials(materials)\n",
    "    print(\"MR_P________________\")\n",
    "    print(type(c))\n",
    "    print(c)\n",
    "    results = c.get(\"meaning_rank\", {})\n",
    "    \n",
    "    for name in names:\n",
    "        score = int(results.get(name, 0))\n",
    "        scores.append(score)\n",
    "    \n",
    "    # å½’ä¸€åŒ–å¹¶åŠ æƒ\n",
    "    min_s, max_s = min(scores), max(scores)\n",
    "    if max_s > min_s:\n",
    "        scores[:] = [(s - min_s) / (max_s - min_s) for s in scores]\n",
    "        scores[:] = [s * WEIGHT for s in scores]\n",
    "    else:\n",
    "        scores = [0,0,0,0]\n",
    "    \n",
    "    # è¿”å›åˆ†æ•°\n",
    "    print(\"meaning_reward\" + str(results))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bcc0d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»£ç æ˜¯å¦æŠ¥é”™\n",
    "def error_check(goal, taskid, completions, **kwargs):\n",
    "    \"\"\"æ£€æŸ¥ç”Ÿæˆçš„ä»£ç æ˜¯å¦æŠ¥é”™\n",
    "    \"\"\"\n",
    "    \n",
    "    WEIGHT = 2\n",
    "    scores = []\n",
    "    names = []\n",
    "    \n",
    "    # æ„é€ ä¼ è¾“å¯¹è±¡\n",
    "    materials = {\n",
    "        \"head\": {\n",
    "            \"input\": goal[0],\n",
    "            \"taskid\": taskid[0],\n",
    "            \"request\": []\n",
    "        },\n",
    "        \"outputs\": []\n",
    "    }\n",
    "    \n",
    "    # å¡«å……æè´¨ä»£ç \n",
    "    for completion in completions:\n",
    "        response = completion[0][\"content\"]\n",
    "        code = code_extractor(response)\n",
    "        print(\"EC_CODE________________\")\n",
    "        print(type(code))\n",
    "        print(code)\n",
    "        \n",
    "        name = f\"M{len(materials['outputs'])+1}\"\n",
    "        names.append(name)\n",
    "        \n",
    "        materials[\"outputs\"].append({\n",
    "            \"name\": name,\n",
    "            \"code\": code\n",
    "        })\n",
    "\n",
    "    print(\"EC_MT________________\")\n",
    "    print(type(materials))\n",
    "    print(materials)\n",
    "    \n",
    "    # æ·»åŠ åˆ†æ•°\n",
    "    c =  client.send_materials(materials)\n",
    "    print(\"EC_P________________\")\n",
    "    print(type(c))\n",
    "    print(c)\n",
    "    results = c.get(\"status\", {})\n",
    "    # results = client.send_materials(materials).get(\"status\", {})\n",
    "    \n",
    "    for name in names:\n",
    "        score = results.get(name, False)\n",
    "        scores.append(WEIGHT if score else 0)\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ¢¯åº¦ï¼Œå¦‚æœæ²¡æœ‰æ¢¯åº¦äº†å°±æ”¾å¼ƒ\n",
    "    min_s, max_s = min(scores), max(scores)\n",
    "    if not max_s > min_s:\n",
    "        scores = [0,0,0,0]\n",
    "    \n",
    "    # è¿”å›åˆ†æ•°\n",
    "    print(\"error_check\" + str(results))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e0730a",
   "metadata": {},
   "source": [
    "### è®­ç»ƒéƒ¨åˆ†\n",
    "#### è®­ç»ƒé…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bbc15bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prompt_length = 256\n",
    "\n",
    "# ä½¿ç”¨ GRPO è®­ç»ƒå™¨ï¼Œå¹¶æ„é€ è®­ç»ƒå™¨\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    beta = 0.0, # è®¾ç½®ä¸º 0 ä»¥ç¦ç”¨ KL æ•£åº¦æƒ©ç½š # defaults to 0.04\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"adamw_torch_fused\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 1, # å¢åŠ åˆ°4ï¼Œä»¥ä¾¿æ›´é¡ºæ»‘åœ°è®­ç»ƒ #1\n",
    "    num_generations = 4, # Decrease if out of memory\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_seq_length - max_prompt_length,\n",
    "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    max_steps = 500, # è®­ç»ƒæ­¥æ•°\n",
    "    save_steps = 200, # æ¯200æ­¥ä¿å­˜ä¸€æ¬¡\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir = \"outputs_ArtI_12b_bf4_qlora\", # è¾“å‡ºç›®å½•\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e10ffba",
   "metadata": {},
   "source": [
    "#### å¼€å§‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46b07040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2,366 | Num Epochs = 1 | Total steps = 500\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 1 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 29,802,496/4,000,000,000 (0.75% trained)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Question:\n",
      "å¸Œæœ›ä½ å¸®æˆ‘ç”Ÿæˆé€æ˜çš„æè´¨ \n",
      "Response:\n",
      "<code></code><code></code><code></code><code></code><code></code><code></code><code></code><code></code>\n",
      "\n",
      "<think>\n",
      "Okay, the user wants a transparent material. This usually means weâ€™ll want to decrease the roughness value and potentially use a value like 1 to represent full transparency. We want to avoid external files. We should also be aware that a truly transparent material in Blender needs to be carefully set up â€“ primarily through usingler and adjustinger. We need to generate python code that can be directly used to create a transparent material that can be directly used. We will focus on a value of 1 for complete transparency and will decrease roughness for a more visually transparent material.\n",
      "</think>\n",
      "\n",
      "<code></code><code></code><code></code><code></code><code></code><code></code><code></code><code></code><code></code>\n",
      "\n",
      "<answer>\n",
      "<code></code><code></code><code></code><code></code><code></code><code></code><code></code><code></code>\n",
      "\n",
      "<python>\n",
      "<code></code><code></code><code></code><code></code><code></code><code></code><code></code><code></code>\n",
      "\n",
      "<explanation>\n",
      "This isochionerpython <explanation> isochioner <explanation> isochioner<explanation>. <explanation> isochioner <explanation>. <explanation> isochioner <explanation>. <explanation> isochioner<explanation>. <explanation>\n",
      "</explanation>\n",
      "<code></code><code></code><code></code><code></code><code></code><code></code><code></code><code></code>\n",
      "\n",
      "</python>\n",
      "\n",
      "</answer>\n",
      "\n",
      "AR_CODE________________\n",
      "<class 'str'>\n",
      "\n",
      "AR_CODE________________\n",
      "<class 'str'>\n",
      "\n",
      "AR_CODE________________\n",
      "<class 'str'>\n",
      "\n",
      "AR_CODE________________\n",
      "<class 'str'>\n",
      "\n",
      "AR_MT________________\n",
      "<class 'dict'>\n",
      "{'head': {'input': 'é€æ˜çš„æè´¨', 'taskid': '471f2abdf9d74e79a484fdd8d33b7d8c', 'request': []}, 'outputs': [{'name': 'M1', 'code': ''}, {'name': 'M2', 'code': ''}, {'name': 'M3', 'code': ''}, {'name': 'M4', 'code': ''}]}\n",
      "å°è¯•è¿æ¥æœåŠ¡å™¨... (1/5)\n",
      "è¿æ¥åˆ°æœåŠ¡å™¨ 10.30.244.17:5555...\n",
      "å·²è¿æ¥åˆ°æœåŠ¡å™¨ 10.30.244.17:5555\n",
      "å‘é€æè´¨ç»„æ•°æ®... (1/5)\n",
      "æ­£åœ¨å‘é€æè´¨ç»„ (4ä¸ªæè´¨) åˆ° 10.30.244.17:5555\n",
      "ç­‰å¾…æœåŠ¡å™¨å“åº”...\n",
      "AR_P________________\n",
      "<class 'dict'>\n",
      "{'accuracy_rank': {'M1': 1, 'M2': 2, 'M3': 3, 'M4': 4}, 'meaning_rank': {'M1': 1, 'M2': 2, 'M3': 3, 'M4': 4}, 'status': {'M1': False, 'M2': False, 'M3': False, 'M4': False}, 'error_msg': {'M1': 'æ²¡æœ‰æä¾›æè´¨ä»£ç ', 'M2': 'æ²¡æœ‰æä¾›æè´¨ä»£ç ', 'M3': 'æ²¡æœ‰æä¾›æè´¨ä»£ç ', 'M4': 'æ²¡æœ‰æä¾›æè´¨ä»£ç '}, 'id': {'M1': None, 'M2': None, 'M3': None, 'M4': None}, 'name': {'M1': 'M1', 'M2': 'M2', 'M3': 'M3', 'M4': 'M4'}}\n",
      "accuracy_reward{'M1': 1, 'M2': 2, 'M3': 3, 'M4': 4}\n",
      "MR_CODE________________\n",
      "<class 'str'>\n",
      "\n",
      "MR_CODE________________\n",
      "<class 'str'>\n",
      "\n",
      "MR_CODE________________\n",
      "<class 'str'>\n",
      "\n",
      "MR_CODE________________\n",
      "<class 'str'>\n",
      "\n",
      "MR_MT________________\n",
      "<class 'dict'>\n",
      "{'head': {'input': 'é€æ˜çš„æè´¨', 'taskid': '471f2abdf9d74e79a484fdd8d33b7d8c', 'request': []}, 'outputs': [{'name': 'M1', 'code': ''}, {'name': 'M2', 'code': ''}, {'name': 'M3', 'code': ''}, {'name': 'M4', 'code': ''}]}\n",
      "å‘é€æè´¨ç»„æ•°æ®... (1/5)\n",
      "æ­£åœ¨å‘é€æè´¨ç»„ (4ä¸ªæè´¨) åˆ° 10.30.244.17:5555\n",
      "ç­‰å¾…æœåŠ¡å™¨å“åº”...\n",
      "MR_P________________\n",
      "<class 'dict'>\n",
      "{'accuracy_rank': {'M1': 1, 'M2': 2, 'M3': 3, 'M4': 4}, 'meaning_rank': {'M1': 1, 'M2': 2, 'M3': 3, 'M4': 4}, 'status': {'M1': False, 'M2': False, 'M3': False, 'M4': False}, 'error_msg': {'M1': 'æ²¡æœ‰æä¾›æè´¨ä»£ç ', 'M2': 'æ²¡æœ‰æä¾›æè´¨ä»£ç ', 'M3': 'æ²¡æœ‰æä¾›æè´¨ä»£ç ', 'M4': 'æ²¡æœ‰æä¾›æè´¨ä»£ç '}, 'id': {'M1': None, 'M2': None, 'M3': None, 'M4': None}, 'name': {'M1': 'M1', 'M2': 'M2', 'M3': 'M3', 'M4': 'M4'}}\n",
      "meaning_reward{'M1': 1, 'M2': 2, 'M3': 3, 'M4': 4}\n",
      "EC_CODE________________\n",
      "<class 'str'>\n",
      "\n",
      "EC_CODE________________\n",
      "<class 'str'>\n",
      "\n",
      "EC_CODE________________\n",
      "<class 'str'>\n",
      "\n",
      "EC_CODE________________\n",
      "<class 'str'>\n",
      "\n",
      "EC_MT________________\n",
      "<class 'dict'>\n",
      "{'head': {'input': 'é€æ˜çš„æè´¨', 'taskid': '471f2abdf9d74e79a484fdd8d33b7d8c', 'request': []}, 'outputs': [{'name': 'M1', 'code': ''}, {'name': 'M2', 'code': ''}, {'name': 'M3', 'code': ''}, {'name': 'M4', 'code': ''}]}\n",
      "å‘é€æè´¨ç»„æ•°æ®... (1/5)\n",
      "æ­£åœ¨å‘é€æè´¨ç»„ (4ä¸ªæè´¨) åˆ° 10.30.244.17:5555\n",
      "ç­‰å¾…æœåŠ¡å™¨å“åº”...\n",
      "EC_P________________\n",
      "<class 'dict'>\n",
      "{'accuracy_rank': {'M1': 1, 'M2': 2, 'M3': 3, 'M4': 4}, 'meaning_rank': {'M1': 1, 'M2': 2, 'M3': 3, 'M4': 4}, 'status': {'M1': False, 'M2': False, 'M3': False, 'M4': False}, 'error_msg': {'M1': 'æ²¡æœ‰æä¾›æè´¨ä»£ç ', 'M2': 'æ²¡æœ‰æä¾›æè´¨ä»£ç ', 'M3': 'æ²¡æœ‰æä¾›æè´¨ä»£ç ', 'M4': 'æ²¡æœ‰æä¾›æè´¨ä»£ç '}, 'id': {'M1': None, 'M2': None, 'M3': None, 'M4': None}, 'name': {'M1': 'M1', 'M2': 'M2', 'M3': 'M3', 'M4': 'M4'}}\n",
      "error_check{'M1': False, 'M2': False, 'M3': False, 'M4': False}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA driver error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 21\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# åˆ›å»ºè®­ç»ƒå™¨ï¼Œå¹¶ä¸”ä½¿ç”¨ä¸Šé¢ç»™å‡ºçš„ reward function\u001b[39;00m\n\u001b[1;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m GRPOTrainer(\n\u001b[1;32m      3\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m      4\u001b[0m     processing_class \u001b[38;5;241m=\u001b[39m tokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m final_dataset,\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 21\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/transformers/trainer.py:2250\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2248\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2251\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2252\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   2253\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   2254\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2255\u001b[0m     )\n",
      "File \u001b[0;32m<string>:311\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m<string>:73\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/accelerate/accelerator.py:2329\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2329\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    628\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/torch/autograd/function.py:307\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m user_fn(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:1710\u001b[0m, in \u001b[0;36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.backward\u001b[0;34m(ctx, *flat_args)\u001b[0m\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CompiledFunction\u001b[38;5;241m.\u001b[39m_double_backward(ctx, impl_fn, all_args)\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m impl_fn()\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:1700\u001b[0m, in \u001b[0;36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.backward.<locals>.impl_fn\u001b[0;34m(double_ctx)\u001b[0m\n\u001b[1;32m   1699\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimpl_fn\u001b[39m(double_ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1700\u001b[0m     out \u001b[38;5;241m=\u001b[39m CompiledFunction\u001b[38;5;241m.\u001b[39m_backward_impl(ctx, all_args)\n\u001b[1;32m   1701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CompiledFunction\u001b[38;5;241m.\u001b[39m_backward_epilogue(ctx, out)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2065\u001b[0m, in \u001b[0;36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction._backward_impl\u001b[0;34m(ctx, all_args)\u001b[0m\n\u001b[1;32m   2048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2049\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdonated_buffer\n\u001b[1;32m   2050\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m saved_tensors_use_once\n\u001b[1;32m   2051\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m fw_metadata\u001b[38;5;241m.\u001b[39mbw_donated_idxs \u001b[38;5;241m!=\u001b[39m []\n\u001b[1;32m   2052\u001b[0m ):\n\u001b[1;32m   2053\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(\n\u001b[1;32m   2054\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   2055\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2062\u001b[0m         ),\n\u001b[1;32m   2063\u001b[0m     )\n\u001b[0;32m-> 2065\u001b[0m out \u001b[38;5;241m=\u001b[39m call_func_at_runtime_with_args(\n\u001b[1;32m   2066\u001b[0m     CompiledFunction\u001b[38;5;241m.\u001b[39mcompiled_bw,\n\u001b[1;32m   2067\u001b[0m     all_args,\n\u001b[1;32m   2068\u001b[0m     steal_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2069\u001b[0m     disable_amp\u001b[38;5;241m=\u001b[39mdisable_amp,\n\u001b[1;32m   2070\u001b[0m )\n\u001b[1;32m   2071\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[0m, in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 126\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(f(args))\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m prior_skip_guard_eval_unsafe \u001b[38;5;241m=\u001b[39m set_skip_guard_eval_unsafe(\n\u001b[1;32m    742\u001b[0m     _is_skip_guard_eval_unsafe_stance()\n\u001b[1;32m    743\u001b[0m )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/torch/_inductor/output_code.py:466\u001b[0m, in \u001b[0;36mCompiledFxGraph.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_callable(inputs)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    468\u001b[0m     AutotuneCacheBundler\u001b[38;5;241m.\u001b[39mend_compile()\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/torch/_inductor/utils.py:2128\u001b[0m, in \u001b[0;36malign_inputs_from_check_idxs.<locals>.run\u001b[0;34m(new_inputs)\u001b[0m\n\u001b[1;32m   2126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(new_inputs: List[InputType]):\n\u001b[1;32m   2127\u001b[0m     copy_misaligned_inputs(new_inputs, inputs_to_check)\n\u001b[0;32m-> 2128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model(new_inputs)\n",
      "File \u001b[0;32m/tmp/torchinductor_root/ok/cokr3m5cklxcxuqyiruocx5cgmknhw7sze3ucobbkuxivylqm4xw.py:228\u001b[0m, in \u001b[0;36mcall\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_DeviceGuard(\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    227\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 228\u001b[0m     buf1 \u001b[38;5;241m=\u001b[39m empty_strided_cuda((s0, s1, s2), (s1\u001b[38;5;241m*\u001b[39ms2, s2, \u001b[38;5;241m1\u001b[39m), torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [], Original ATen: [aten.new_zeros, aten.scatter_add]\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     triton_poi_fused_new_zeros_scatter_add_0_xnumel \u001b[38;5;241m=\u001b[39m s0\u001b[38;5;241m*\u001b[39ms1\u001b[38;5;241m*\u001b[39ms2\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA driver error: out of memory"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºè®­ç»ƒå™¨ï¼Œå¹¶ä¸”ä½¿ç”¨ä¸Šé¢ç»™å‡ºçš„ reward function\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        # æ ¼å¼å¥–åŠ±\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        \n",
    "        # å›¾åƒæ„ä¹‰å¥–åŠ±\n",
    "        accuracy_reward,\n",
    "        meaning_reward,\n",
    "        \n",
    "        # è¿è¡Œé”™è¯¯æ£€æŸ¥\n",
    "        error_check,\n",
    "        \n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = final_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bbbf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ²¡æœ‰æ•°æ®å¯ä»¥ç»˜å›¾!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "\n",
    "# è®¾ç½®Seabornæ ·å¼ä»¥è·å¾—æ›´å¥½çœ‹çš„å›¾è¡¨\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "def extract_rewards_from_log(log_path):\n",
    "    \"\"\"ä»è®­ç»ƒæ—¥å¿—æ–‡ä»¶ä¸­æå–å¥–åŠ±æ•°æ®\n",
    "    \n",
    "    å‚æ•°:\n",
    "        log_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„\n",
    "        \n",
    "    è¿”å›:\n",
    "        åŒ…å«æ­¥éª¤å’Œå¯¹åº”å¥–åŠ±çš„pandas DataFrame\n",
    "    \"\"\"\n",
    "    # å­˜å‚¨æ•°æ®çš„å­—å…¸\n",
    "    data = defaultdict(list)\n",
    "    step_pattern = re.compile(r'Step\\s+(\\d+)')\n",
    "    reward_pattern = re.compile(r'Reward_(\\d+):\\s+([-\\d.]+)')\n",
    "    mean_reward_pattern = re.compile(r'Mean Reward:\\s+([-\\d.]+)')\n",
    "    \n",
    "    if not os.path.exists(log_path):\n",
    "        print(f\"æ—¥å¿—æ–‡ä»¶ {log_path} ä¸å­˜åœ¨!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    with open(log_path, 'r') as f:\n",
    "        for line in f:\n",
    "            # æå–æ­¥éª¤\n",
    "            step_match = step_pattern.search(line)\n",
    "            if step_match:\n",
    "                current_step = int(step_match.group(1))\n",
    "                data['step'].append(current_step)\n",
    "                \n",
    "                # æå–å„ä¸ªå¥–åŠ±å‡½æ•°çš„å€¼\n",
    "                rewards = reward_pattern.findall(line)\n",
    "                for idx, value in rewards:\n",
    "                    data[f'reward_{idx}'].append(float(value))\n",
    "                \n",
    "                # æå–å¹³å‡å¥–åŠ±\n",
    "                mean_match = mean_reward_pattern.search(line)\n",
    "                if mean_match:\n",
    "                    data['mean_reward'].append(float(mean_match.group(1)))\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def extract_rewards_from_trainer(trainer):\n",
    "    \"\"\"ä»trainerå¯¹è±¡ä¸­ç›´æ¥æå–å¥–åŠ±æ•°æ®\n",
    "    \n",
    "    å‚æ•°:\n",
    "        trainer: GRPOTrainerå¯¹è±¡\n",
    "        \n",
    "    è¿”å›:\n",
    "        åŒ…å«æ­¥éª¤å’Œå¯¹åº”å¥–åŠ±çš„pandas DataFrame\n",
    "    \"\"\"\n",
    "    if hasattr(trainer, 'state') and hasattr(trainer.state, 'log_history'):\n",
    "        data = defaultdict(list)\n",
    "        for entry in trainer.state.log_history:\n",
    "            if 'step' in entry:\n",
    "                data['step'].append(entry['step'])\n",
    "                \n",
    "                # æå–å„ä¸ªå¥–åŠ±\n",
    "                for key, value in entry.items():\n",
    "                    if key.startswith('reward_'):\n",
    "                        data[key].append(value)\n",
    "                \n",
    "                # æå–å¹³å‡å¥–åŠ±\n",
    "                if 'mean_reward' in entry:\n",
    "                    data['mean_reward'].append(entry['mean_reward'])\n",
    "                \n",
    "        return pd.DataFrame(data)\n",
    "    else:\n",
    "        print(\"è®­ç»ƒå™¨æ²¡æœ‰æ—¥å¿—å†å²æˆ–è€…ç»“æ„ä¸ç¬¦åˆé¢„æœŸ!\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def plot_rewards(data, title=\"GRPOè®­ç»ƒå¥–åŠ±æ›²çº¿\", save_path=None, moving_avg_window=5):\n",
    "    \"\"\"ç»˜åˆ¶å¥–åŠ±æŠ˜çº¿å›¾\n",
    "    \n",
    "    å‚æ•°:\n",
    "        data: åŒ…å«å¥–åŠ±æ•°æ®çš„DataFrame\n",
    "        title: å›¾è¡¨æ ‡é¢˜\n",
    "        save_path: ä¿å­˜å›¾è¡¨çš„è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™æ˜¾ç¤ºå›¾è¡¨\n",
    "        moving_avg_window: ç§»åŠ¨å¹³å‡çª—å£å¤§å°\n",
    "    \"\"\"\n",
    "    if data.empty:\n",
    "        print(\"æ²¡æœ‰æ•°æ®å¯ä»¥ç»˜å›¾!\")\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # å®šä¹‰ä¸€ç»„ä¸“ä¸šçš„é¢œè‰²\n",
    "    colors = sns.color_palette('viridis', n_colors=len(data.columns)-1)\n",
    "    \n",
    "    # ç»˜åˆ¶æ¯ä¸ªå¥–åŠ±å‡½æ•°çš„æ›²çº¿\n",
    "    for i, col in enumerate([col for col in data.columns if col != 'step']):\n",
    "        # åŸå§‹æ•°æ®ç‚¹ï¼ˆé€æ˜åº¦é™ä½ï¼‰\n",
    "        ax.plot(data['step'], data[col], alpha=0.3, color=colors[i], label=f\"{col} (raw)\")\n",
    "        \n",
    "        # æ·»åŠ ç§»åŠ¨å¹³å‡çº¿\n",
    "        if len(data) >= moving_avg_window:\n",
    "            moving_avg = data[col].rolling(window=moving_avg_window).mean()\n",
    "            ax.plot(data['step'], moving_avg, linewidth=2, color=colors[i], label=f\"{col} ({moving_avg_window}-point avg)\")\n",
    "    \n",
    "    # æ·»åŠ æ ‡é¢˜å’Œæ ‡ç­¾\n",
    "    ax.set_title(title, fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Training Steps', fontsize=14)\n",
    "    ax.set_ylabel('Reward', fontsize=14)\n",
    "    \n",
    "    # æ·»åŠ ç½‘æ ¼çº¿å’Œå›¾ä¾‹\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax.legend(loc='best', fontsize=12)\n",
    "    \n",
    "    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯\n",
    "    if 'mean_reward' in data.columns:\n",
    "        final_mean = data['mean_reward'].iloc[-1]\n",
    "        max_mean = data['mean_reward'].max()\n",
    "        min_mean = data['mean_reward'].min()\n",
    "        stats_text = f\"Final mean reward: {final_mean:.4f}\\nMax mean reward: {max_mean:.4f}\\nMin mean reward: {min_mean:.4f}\"\n",
    "        plt.figtext(0.02, 0.02, stats_text, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # ä¿å­˜æˆ–æ˜¾ç¤ºå›¾è¡¨\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"å›¾è¡¨å·²ä¿å­˜åˆ° {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "# ç¤ºä¾‹ç”¨æ³•\n",
    "def visualize_rewards(trainer=None, log_file=None, output_path=None):\n",
    "    \"\"\"å¯è§†åŒ–è®­ç»ƒå¥–åŠ±\n",
    "    \n",
    "    å‚æ•°:\n",
    "        trainer: GRPOTrainerå¯¹è±¡ï¼Œå¦‚æœæä¾›åˆ™ç›´æ¥ä»è®­ç»ƒå™¨ä¸­æå–æ•°æ®\n",
    "        log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœtrainerä¸å¯ç”¨åˆ™ä»æ—¥å¿—æ–‡ä»¶ä¸­æå–æ•°æ®\n",
    "        output_path: å›¾è¡¨ä¿å­˜è·¯å¾„ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•ä¸‹çš„'reward_plot.png'\n",
    "    \"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = 'reward_plot.png'\n",
    "    \n",
    "    if trainer is not None:\n",
    "        data = extract_rewards_from_trainer(trainer)\n",
    "    elif log_file is not None:\n",
    "        data = extract_rewards_from_log(log_file)\n",
    "    else:\n",
    "        print(\"è¯·æä¾›trainerå¯¹è±¡æˆ–æ—¥å¿—æ–‡ä»¶è·¯å¾„!\")\n",
    "        return\n",
    "    \n",
    "    plot_rewards(data, save_path=output_path)\n",
    "    \n",
    "    # è¾“å‡ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯\n",
    "    if not data.empty and 'mean_reward' in data.columns:\n",
    "        print(\"\\n--- å¥–åŠ±ç»Ÿè®¡ä¿¡æ¯ ---\")\n",
    "        print(f\"æœ€ç»ˆå¹³å‡å¥–åŠ±: {data['mean_reward'].iloc[-1]:.4f}\")\n",
    "        print(f\"æœ€å¤§å¹³å‡å¥–åŠ±: {data['mean_reward'].max():.4f}\")\n",
    "        print(f\"æœ€å°å¹³å‡å¥–åŠ±: {data['mean_reward'].min():.4f}\")\n",
    "        \n",
    "        # è®¡ç®—å¥–åŠ±å¢é•¿ç‡\n",
    "        if len(data) > 1:\n",
    "            first_reward = data['mean_reward'].iloc[0]\n",
    "            last_reward = data['mean_reward'].iloc[-1]\n",
    "            growth = ((last_reward - first_reward) / abs(first_reward)) * 100 if first_reward != 0 else float('inf')\n",
    "            print(f\"å¥–åŠ±å¢é•¿ç‡: {growth:.2f}%\")\n",
    "\n",
    "# ç”¨æ³•ç¤ºä¾‹\n",
    "# 1. ä½¿ç”¨è®­ç»ƒå™¨å¯¹è±¡\n",
    "# visualize_rewards(trainer=trainer)\n",
    "\n",
    "# 2. æˆ–è€…ä½¿ç”¨æ—¥å¿—æ–‡ä»¶\n",
    "# visualize_rewards(log_file=\"./outputs_gemma-3_grpo_lora/opt_gemm3_2.log\")\n",
    "\n",
    "# ä»è®­ç»ƒåç›´æ¥å¯è§†åŒ–\n",
    "# åœ¨è®­ç»ƒåè°ƒç”¨ä»¥ä¸‹ä»£ç å³å¯ç›´æ¥å¯è§†åŒ–\n",
    "visualize_rewards(trainer=trainer, output_path=\"reward_trends.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95da640",
   "metadata": {},
   "source": [
    "### æ¨¡å‹æµ‹è¯•\n",
    "#### é»˜è®¤æ¨¡å‹æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72b37c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import bpy\n",
      "\n",
      "def create_gradient_material(material_name=\"GradientMaterial\"):\n",
      "    \"\"\"\n",
      "    Creates a grey to yellow gradient material in Blender using GLSL shader.\n",
      "\n",
      "    Args:\n",
      "        material_name (str): The name of the material to create.\n",
      "    \"\"\"\n",
      "\n",
      "    # Check if material already exists\n",
      "    if material_name in bpy.data.materials:\n",
      "        return\n",
      "\n",
      "    # Create a new material\n",
      "    mat = bpy.data.materials.new(name=material_name)\n",
      "    mat.use_nodes = True\n",
      "    nodes = mat.node_tree.nodes\n",
      "\n",
      "    # Clear default nodes\n",
      "    for node in nodes:\n",
      "        nodes.remove(node)\n",
      "\n",
      "    # Create Principled BSDF node\n",
      "    principled_bsdf = nodes.new(type='ShaderNodeBsdfPrincipled')\n",
      "    principled_bsdf.location = (200, 0)\n",
      "    principled_bsdf.inputs['Base Color'].default_value = (0.0, 0.0, 0.0, 1.0) # Initial black color\n",
      "\n",
      "    # Create ShaderNodeGLSL\n",
      "    glsl_node = nodes.new(type='ShaderNodeGLSL')\n",
      "    glsl_node.location = (-200, 0)\n",
      "    glsl_node.inputs['GLSL Code'].default_value = \"\"\"\n",
      "    #version 330 core\n",
      "    uniform float time;\n",
      "    \n",
      "    vec3 color = vec3(0.5); // Start with grey\n",
      "\n",
      "    float gradient_position = fract(time * 0.2);  // Adjust speed as needed\n",
      "    \n",
      "    // Blend from grey to yellow\n",
      "    color += "
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    # {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ª GLSL Shader ç”Ÿæˆå™¨ï¼Œä½ ç”Ÿæˆå‡ºæ¥çš„åº”å½“å°±æ˜¯æœ€ç»ˆç»“æœï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ï¼Œä½ ä¹Ÿä¸ä¼šä½¿ç”¨ä»»ä½•å¤–éƒ¨æ–‡ä»¶ï¼Œçº¯ç²¹ç¨‹åºåŒ–ç”Ÿæˆ\"},\n",
    "    # {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ª blender èŠ‚ç‚¹è§£é‡Šå™¨ï¼Œä¼šå’Œæˆ‘è§£é‡Š blender èŠ‚ç‚¹æ˜¯å¹²ä»€ä¹ˆç”¨çš„\"},\n",
    "    {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ª Blender çš„æè´¨ç”Ÿæˆå™¨ï¼Œä¼šç›´æ¥ç”Ÿæˆæè´¨å¯¹åº”çš„ Python ä»£ç ï¼Œè¯¥ä»£ç åº”è¯¥å¯ä»¥ä¸”ä»…åœ¨ Blender ä¸­åˆ›å»ºå¯¹åº”æè´¨ï¼Œä½ ç”Ÿæˆå‡ºæ¥çš„åº”å½“å°±æ˜¯æœ€ç»ˆç»“æœï¼Œç”¨æˆ·å¯ä»¥ç›´æ¥ä½¿ç”¨ï¼Œä¸éœ€è¦ç”¨æˆ·æ›´æ”¹ï¼Œä½ ä¹Ÿä¸ä¼šä½¿ç”¨ä»»ä½•å¤–éƒ¨æ–‡ä»¶\"},\n",
    "    {\"role\": \"user\",   \"content\": \"ç»™æˆ‘ç”Ÿæˆä¸€ä¸ªçš„ç°è‰²æ¸å˜åˆ°é»„è‰²çš„æè´¨\"},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    tokenize = False,\n",
    ")\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 1024*2, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a49ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½åŸå§‹æ¨¡å‹ï¼ˆä¸åŒ…å«å¾®è°ƒï¼‰\n",
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "# å®šä¹‰ç›¸åŒçš„å‚æ•°\n",
    "max_seq_length = 1024\n",
    "\n",
    "# é‡æ–°åŠ è½½åŸå§‹æ¨¡å‹ï¼ˆä¸åº”ç”¨LoRAæƒé‡ï¼‰\n",
    "original_model, original_tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"./models/gemma-3-1b-it\",  # ä½¿ç”¨åŸå§‹æ¨¡å‹è·¯å¾„\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = False,\n",
    "    load_in_8bit = False,\n",
    ")\n",
    "\n",
    "# æµ‹è¯•é—®é¢˜\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},  # ä½¿ç”¨ä¹‹å‰å®šä¹‰çš„ç³»ç»Ÿæç¤ºè¯\n",
    "    {\"role\": \"user\", \"content\": \"What is the sqrt of 101?\"},  # ä½¿ç”¨åŒæ ·çš„æµ‹è¯•é—®é¢˜ä»¥ä¾¿æ¯”è¾ƒ\n",
    "]\n",
    "\n",
    "# å‡†å¤‡è¾“å…¥\n",
    "test_text = original_tokenizer.apply_chat_template(\n",
    "    test_messages,\n",
    "    add_generation_prompt = True,\n",
    "    tokenize = False,\n",
    ")\n",
    "\n",
    "# ä½¿ç”¨TextStreamerç›´æ¥æŸ¥çœ‹è¾“å‡º\n",
    "from transformers import TextStreamer\n",
    "print(\"\\nåŸå§‹æ¨¡å‹è¾“å‡ºï¼š\")\n",
    "_ = original_model.generate(\n",
    "    **original_tokenizer(test_text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 1024,\n",
    "    temperature = 0.8,  # ä½¿ç”¨ä¸å¾®è°ƒæ¨¡å‹ç›¸åŒçš„æ¸©åº¦\n",
    "    top_p = 0.95,\n",
    "    top_k = 64,\n",
    "    streamer = TextStreamer(original_tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7426953b",
   "metadata": {},
   "source": [
    "#### finetuning æ¨¡å‹æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfea2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜ Lora\n",
    "model.save_lora(\"grpo_saved_lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c833dc17",
   "metadata": {},
   "source": [
    "#### ä¿å­˜ Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855619d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gemma-3/tokenizer_config.json',\n",
       " 'gemma-3/special_tokens_map.json',\n",
       " 'gemma-3/tokenizer.model',\n",
       " 'gemma-3/added_tokens.json',\n",
       " 'gemma-3/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"gemma-3\")  # Local saving\n",
    "tokenizer.save_pretrained(\"gemma-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "790cbde8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'startswith'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:125\u001b[0m, in \u001b[0;36mHfFileSystem._repo_and_revision_exist\u001b[0;34m(self, repo_type, repo_id, revision)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api\u001b[38;5;241m.\u001b[39mrepo_info(\n\u001b[1;32m    126\u001b[0m         repo_id, revision\u001b[38;5;241m=\u001b[39mrevision, repo_type\u001b[38;5;241m=\u001b[39mrepo_type, timeout\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mHF_HUB_ETAG_TIMEOUT\n\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (RepositoryNotFoundError, HFValidationError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     validate_repo_id(arg_value)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:160\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './models'.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/unsloth_zoo/saving_utils.py:545\u001b[0m, in \u001b[0;36mmerge_and_overwrite_lora\u001b[0;34m(get_model_name, model, tokenizer, save_directory, push_to_hub, private, token, output_dtype, low_disk_space_usage, use_temp_file, cleanup_temp_file)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 545\u001b[0m     file_list \u001b[38;5;241m=\u001b[39m HfFileSystem(token \u001b[38;5;241m=\u001b[39m token)\u001b[38;5;241m.\u001b[39mls(model_name, detail \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:368\u001b[0m, in \u001b[0;36mHfFileSystem.ls\u001b[0;34m(self, path, detail, refresh, revision, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03mList the contents of a directory.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    dictionaries (if detail=True).\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m resolved_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_path(path, revision\u001b[38;5;241m=\u001b[39mrevision)\n\u001b[1;32m    369\u001b[0m path \u001b[38;5;241m=\u001b[39m resolved_path\u001b[38;5;241m.\u001b[39munresolve()\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:216\u001b[0m, in \u001b[0;36mHfFileSystem.resolve_path\u001b[0;34m(self, path, revision)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m repo_and_revision_exist:\n\u001b[0;32m--> 216\u001b[0m         _raise_file_not_found(path, err)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:1136\u001b[0m, in \u001b[0;36m_raise_file_not_found\u001b[0;34m(path, err)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (invalid repository id)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: ./models/gemma-3-1b-it (invalid repository id)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m: \u001b[38;5;66;03m# Change to True to save finetune!\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m     model\u001b[38;5;241m.\u001b[39msave_pretrained_merged(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma-3-finetune\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/unsloth/save.py:2357\u001b[0m, in \u001b[0;36munsloth_generic_save_pretrained_merged\u001b[0;34m(self, save_directory, tokenizer, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   2355\u001b[0m arguments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   2356\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m arguments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m-> 2357\u001b[0m unsloth_generic_save(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39marguments)\n\u001b[1;32m   2358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m   2359\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/unsloth/save.py:2304\u001b[0m, in \u001b[0;36munsloth_generic_save\u001b[0;34m(model, tokenizer, save_directory, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, use_temp_dir, commit_message, private, create_pr, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   2274\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39minference_mode\n\u001b[1;32m   2275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munsloth_generic_save\u001b[39m(\n\u001b[1;32m   2276\u001b[0m     model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2301\u001b[0m     maximum_memory_usage : \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m   2302\u001b[0m ):\n\u001b[1;32m   2303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m push_to_hub: token \u001b[38;5;241m=\u001b[39m get_token()\n\u001b[0;32m-> 2304\u001b[0m     merge_and_overwrite_lora(\n\u001b[1;32m   2305\u001b[0m         get_model_name,\n\u001b[1;32m   2306\u001b[0m         model                \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m   2307\u001b[0m         tokenizer            \u001b[38;5;241m=\u001b[39m tokenizer,\n\u001b[1;32m   2308\u001b[0m         save_directory       \u001b[38;5;241m=\u001b[39m save_directory,\n\u001b[1;32m   2309\u001b[0m         push_to_hub          \u001b[38;5;241m=\u001b[39m push_to_hub,\n\u001b[1;32m   2310\u001b[0m         private              \u001b[38;5;241m=\u001b[39m private,\n\u001b[1;32m   2311\u001b[0m         token                \u001b[38;5;241m=\u001b[39m token,\n\u001b[1;32m   2312\u001b[0m         output_dtype         \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2313\u001b[0m         low_disk_space_usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2314\u001b[0m         use_temp_file        \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   2315\u001b[0m     )\n\u001b[1;32m   2316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/unsloth_zoo/saving_utils.py:549\u001b[0m, in \u001b[0;36mmerge_and_overwrite_lora\u001b[0;34m(get_model_name, model, tokenizer, save_directory, push_to_hub, private, token, output_dtype, low_disk_space_usage, use_temp_file, cleanup_temp_file)\u001b[0m\n\u001b[1;32m    547\u001b[0m     original_model_id \u001b[38;5;241m=\u001b[39m get_original_model_id(model_name)\n\u001b[1;32m    548\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m original_model_id\n\u001b[0;32m--> 549\u001b[0m     file_list \u001b[38;5;241m=\u001b[39m HfFileSystem(token \u001b[38;5;241m=\u001b[39m token)\u001b[38;5;241m.\u001b[39mls(model_name, detail \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    551\u001b[0m safetensors_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    552\u001b[0m max_size_in_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:368\u001b[0m, in \u001b[0;36mHfFileSystem.ls\u001b[0;34m(self, path, detail, refresh, revision, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mls\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m, detail: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, refresh: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, revision: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    341\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Union[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]]:\n\u001b[1;32m    342\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    List the contents of a directory.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m        dictionaries (if detail=True).\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     resolved_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_path(path, revision\u001b[38;5;241m=\u001b[39mrevision)\n\u001b[1;32m    369\u001b[0m     path \u001b[38;5;241m=\u001b[39m resolved_path\u001b[38;5;241m.\u001b[39munresolve()\n\u001b[1;32m    370\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpand_info\u001b[39m\u001b[38;5;124m\"\u001b[39m: detail, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:172\u001b[0m, in \u001b[0;36mHfFileSystem.resolve_path\u001b[0;34m(self, path, revision)\u001b[0m\n\u001b[1;32m    169\u001b[0m         revision \u001b[38;5;241m=\u001b[39m revision_in_path\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m revision\n\u001b[0;32m--> 172\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strip_protocol(path)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# can't list repositories at root\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccess to repositories lists is not implemented.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/fsspec/spec.py:198\u001b[0m, in \u001b[0;36mAbstractFileSystem._strip_protocol\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    196\u001b[0m protos \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mprotocol,) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mprotocol, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mprotocol\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m protocol \u001b[38;5;129;01min\u001b[39;00m protos:\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mstartswith(protocol \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m://\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    199\u001b[0m         path \u001b[38;5;241m=\u001b[39m path[\u001b[38;5;28mlen\u001b[39m(protocol) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m :]\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mstartswith(protocol \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m::\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'startswith'"
     ]
    }
   ],
   "source": [
    "if True: # Change to True to save finetune!\n",
    "    model.save_pretrained_merged(\"gemma-3-finetune\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1a0885",
   "metadata": {},
   "source": [
    "### ä¿å­˜ä¸ºå®Œæ•´æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27072ac2",
   "metadata": {},
   "source": [
    "##### ä¿å­˜ä¸º bf16 æ ¼å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f4381df",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'startswith'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:125\u001b[0m, in \u001b[0;36mHfFileSystem._repo_and_revision_exist\u001b[0;34m(self, repo_type, repo_id, revision)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api\u001b[38;5;241m.\u001b[39mrepo_info(\n\u001b[1;32m    126\u001b[0m         repo_id, revision\u001b[38;5;241m=\u001b[39mrevision, repo_type\u001b[38;5;241m=\u001b[39mrepo_type, timeout\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mHF_HUB_ETAG_TIMEOUT\n\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (RepositoryNotFoundError, HFValidationError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     validate_repo_id(arg_value)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:160\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './models'.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/unsloth_zoo/saving_utils.py:545\u001b[0m, in \u001b[0;36mmerge_and_overwrite_lora\u001b[0;34m(get_model_name, model, tokenizer, save_directory, push_to_hub, private, token, output_dtype, low_disk_space_usage, use_temp_file, cleanup_temp_file)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 545\u001b[0m     file_list \u001b[38;5;241m=\u001b[39m HfFileSystem(token \u001b[38;5;241m=\u001b[39m token)\u001b[38;5;241m.\u001b[39mls(model_name, detail \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:368\u001b[0m, in \u001b[0;36mHfFileSystem.ls\u001b[0;34m(self, path, detail, refresh, revision, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03mList the contents of a directory.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    dictionaries (if detail=True).\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m resolved_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_path(path, revision\u001b[38;5;241m=\u001b[39mrevision)\n\u001b[1;32m    369\u001b[0m path \u001b[38;5;241m=\u001b[39m resolved_path\u001b[38;5;241m.\u001b[39munresolve()\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:216\u001b[0m, in \u001b[0;36mHfFileSystem.resolve_path\u001b[0;34m(self, path, revision)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m repo_and_revision_exist:\n\u001b[0;32m--> 216\u001b[0m         _raise_file_not_found(path, err)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:1136\u001b[0m, in \u001b[0;36m_raise_file_not_found\u001b[0;34m(path, err)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (invalid repository id)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: ./models/gemma-3-1b-it (invalid repository id)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Merge to 16bit\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m: model\u001b[38;5;241m.\u001b[39msave_pretrained_merged(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer, save_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerged_16bit\u001b[39m\u001b[38;5;124m\"\u001b[39m,)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m: model\u001b[38;5;241m.\u001b[39mpush_to_hub_merged(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf/model\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer, save_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerged_16bit\u001b[39m\u001b[38;5;124m\"\u001b[39m, token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Merge to 4bit\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/unsloth/save.py:2357\u001b[0m, in \u001b[0;36munsloth_generic_save_pretrained_merged\u001b[0;34m(self, save_directory, tokenizer, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   2355\u001b[0m arguments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   2356\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m arguments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m-> 2357\u001b[0m unsloth_generic_save(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39marguments)\n\u001b[1;32m   2358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m   2359\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/unsloth/save.py:2304\u001b[0m, in \u001b[0;36munsloth_generic_save\u001b[0;34m(model, tokenizer, save_directory, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, use_temp_dir, commit_message, private, create_pr, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   2274\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39minference_mode\n\u001b[1;32m   2275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munsloth_generic_save\u001b[39m(\n\u001b[1;32m   2276\u001b[0m     model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2301\u001b[0m     maximum_memory_usage : \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m   2302\u001b[0m ):\n\u001b[1;32m   2303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m push_to_hub: token \u001b[38;5;241m=\u001b[39m get_token()\n\u001b[0;32m-> 2304\u001b[0m     merge_and_overwrite_lora(\n\u001b[1;32m   2305\u001b[0m         get_model_name,\n\u001b[1;32m   2306\u001b[0m         model                \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m   2307\u001b[0m         tokenizer            \u001b[38;5;241m=\u001b[39m tokenizer,\n\u001b[1;32m   2308\u001b[0m         save_directory       \u001b[38;5;241m=\u001b[39m save_directory,\n\u001b[1;32m   2309\u001b[0m         push_to_hub          \u001b[38;5;241m=\u001b[39m push_to_hub,\n\u001b[1;32m   2310\u001b[0m         private              \u001b[38;5;241m=\u001b[39m private,\n\u001b[1;32m   2311\u001b[0m         token                \u001b[38;5;241m=\u001b[39m token,\n\u001b[1;32m   2312\u001b[0m         output_dtype         \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2313\u001b[0m         low_disk_space_usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2314\u001b[0m         use_temp_file        \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   2315\u001b[0m     )\n\u001b[1;32m   2316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/unsloth_zoo/saving_utils.py:549\u001b[0m, in \u001b[0;36mmerge_and_overwrite_lora\u001b[0;34m(get_model_name, model, tokenizer, save_directory, push_to_hub, private, token, output_dtype, low_disk_space_usage, use_temp_file, cleanup_temp_file)\u001b[0m\n\u001b[1;32m    547\u001b[0m     original_model_id \u001b[38;5;241m=\u001b[39m get_original_model_id(model_name)\n\u001b[1;32m    548\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m original_model_id\n\u001b[0;32m--> 549\u001b[0m     file_list \u001b[38;5;241m=\u001b[39m HfFileSystem(token \u001b[38;5;241m=\u001b[39m token)\u001b[38;5;241m.\u001b[39mls(model_name, detail \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    551\u001b[0m safetensors_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    552\u001b[0m max_size_in_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:368\u001b[0m, in \u001b[0;36mHfFileSystem.ls\u001b[0;34m(self, path, detail, refresh, revision, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mls\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m, detail: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, refresh: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, revision: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    341\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Union[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]]:\n\u001b[1;32m    342\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    List the contents of a directory.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m        dictionaries (if detail=True).\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     resolved_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_path(path, revision\u001b[38;5;241m=\u001b[39mrevision)\n\u001b[1;32m    369\u001b[0m     path \u001b[38;5;241m=\u001b[39m resolved_path\u001b[38;5;241m.\u001b[39munresolve()\n\u001b[1;32m    370\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpand_info\u001b[39m\u001b[38;5;124m\"\u001b[39m: detail, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:172\u001b[0m, in \u001b[0;36mHfFileSystem.resolve_path\u001b[0;34m(self, path, revision)\u001b[0m\n\u001b[1;32m    169\u001b[0m         revision \u001b[38;5;241m=\u001b[39m revision_in_path\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m revision\n\u001b[0;32m--> 172\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strip_protocol(path)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# can't list repositories at root\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccess to repositories lists is not implemented.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth/lib/python3.11/site-packages/fsspec/spec.py:198\u001b[0m, in \u001b[0;36mAbstractFileSystem._strip_protocol\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    196\u001b[0m protos \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mprotocol,) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mprotocol, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mprotocol\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m protocol \u001b[38;5;129;01min\u001b[39;00m protos:\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mstartswith(protocol \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m://\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    199\u001b[0m         path \u001b[38;5;241m=\u001b[39m path[\u001b[38;5;28mlen\u001b[39m(protocol) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m :]\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mstartswith(protocol \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m::\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'startswith'"
     ]
    }
   ],
   "source": [
    "# Merge to 16bit\n",
    "if True: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if True: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde13d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # Change to True to upload finetune\n",
    "    model.push_to_hub_merged(\n",
    "        \"HF_ACCOUNT/gemma-3-finetune\", tokenizer,\n",
    "        token = \"hf_...\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8bbb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜ä¸º GGUF æ ¼å¼\n",
    "# if False:\n",
    "#     model.save_pretrained_gguf(\n",
    "#         \"gemma-3-finetune\",\n",
    "#         quantization_type = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a89623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if False: # Change to True to upload GGUF\n",
    "#     model.push_to_hub_gguf(\n",
    "#         \"gemma-3-finetune\",\n",
    "#         quantization_type = \"Q8_0\", # Only Q8_0, BF16, F16 supported\n",
    "#         repo_id = \"HF_ACCOUNT/gemma-finetune-gguf\",\n",
    "#         token = \"hf_...\",\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
