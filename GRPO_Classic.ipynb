{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4whfu8wzPjQO"
   },
   "source": [
    "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + â­ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> â­\n",
    "</div>\n",
    "\n",
    "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgjdVJflPjQO"
   },
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biwbaQvsPjQO"
   },
   "source": [
    "**Read our [blog post](https://unsloth.ai/blog/r1-reasoning) for guidance on how to train reasoning models.**\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdjNKWvSPjQP"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaBUw4wJ2WlP"
   },
   "source": [
    "Use `PatchFastRL` before all functions to patch GRPO and other RL algorithms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59DIs5BMcvjN",
    "outputId": "5d03d8e6-d961-4101-8cd5-669944f91a5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, PatchFastRL\n",
    "PatchFastRL(\"GRPO\", FastLanguageModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 805,
     "referenced_widgets": [
      "f57d844b2efa469e8aadd48175ce70ab",
      "47d2fd7f76754d9fa156576bc0c58abb",
      "81a0791760de4dcebd543c40d2c1e322",
      "a729c5fc5c764c85885cac7a2d4d95d0",
      "d6d5a7d96a034247b38d25d8a9cc979c",
      "4c9248100f89400d9e1407dbb168d5d6",
      "96cea0d773c8426b8be72dd7f72e5a82",
      "1b9f8a2a793640d689abc10f5f39c54b",
      "627f68389cf64e2a915a72ab147ee8a7",
      "9eed940f3815428583b4ddefc1a81469",
      "0b9230e976b34a9ea85978cf22857012",
      "22e0933485c14d94b0c1cfe198d6758f",
      "43462d5de24b4e55871b3f579798b374",
      "99577e7cbed74c89afb3d44d4fd956c5",
      "d034c840e7f74177a7b07a188d666b8d",
      "0f8ead1775934dc3a10533b67b3dd905",
      "e43ad27d5d304d1ebf9b374016409a97",
      "51948945111f437c9ed6ccab22072dd3",
      "4089236deafd4fa2be86d8dc0a29d469",
      "78cc90a50c0c4636b0f41436a820ecd3",
      "129dc789722b43439574390bba63b36a",
      "8991360910ef417db03499f76f5fe323"
     ]
    },
    "id": "DkIvEkIIkEyB",
    "outputId": "c5a32856-2166-4485-fdb3-16241d0e6316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-11 00:49:29 __init__.py:207] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.3.8: Fast Qwen2 patching. Transformers: 4.49.0. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 Ti SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading /home/projects/unsloth-training/models/Qwen2.5-3B-Instruct with actual GPU utilization = 82.75%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 15.99 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 1024. Num Sequences = 192.\n",
      "Unsloth: vLLM's KV Cache can use up to 7.37 GB. Also swap space = 2 GB.\n",
      "INFO 03-11 00:50:13 config.py:549] This model supports multiple tasks: {'classify', 'generate', 'score', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 03-11 00:50:13 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/home/projects/unsloth-training/models/Qwen2.5-3B-Instruct', speculative_config=None, tokenizer='/home/projects/unsloth-training/models/Qwen2.5-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/projects/unsloth-training/models/Qwen2.5-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":192}, use_cached_outputs=False, \n",
      "WARNING 03-11 00:50:13 interface.py:304] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 03-11 00:50:13 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 03-11 00:50:24 model_runner.py:1110] Starting to load model /home/projects/unsloth-training/models/Qwen2.5-3B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W311 00:50:23.882242727 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7bad27dd45e42afbde3d5d3c7d974d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-11 00:50:29 model_runner.py:1115] Loading model weights took 5.7701 GB\n",
      "INFO 03-11 00:50:29 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 03-11 00:50:31 worker.py:267] Memory profiling takes 1.45 seconds\n",
      "INFO 03-11 00:50:31 worker.py:267] the current vLLM instance can use total_gpu_memory (15.99GiB) x gpu_memory_utilization (0.83) = 13.23GiB\n",
      "INFO 03-11 00:50:31 worker.py:267] model weights take 5.77GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.05GiB; the rest of the memory reserved for KV Cache is 6.37GiB.\n",
      "INFO 03-11 00:50:31 executor_base.py:111] # cuda blocks: 11595, # CPU blocks: 3640\n",
      "INFO 03-11 00:50:31 executor_base.py:116] Maximum concurrency for 1024 tokens per request: 181.17x\n",
      "INFO 03-11 00:50:31 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:14<00:00,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-11 00:50:46 model_runner.py:1562] Graph capturing finished in 15 secs, took 0.32 GiB\n",
      "INFO 03-11 00:50:46 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 17.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n",
      "Not an error, but Unsloth cannot patch O projection layer with our manual autograd engine since either LoRA adapters\n",
      "are not enabled or a bias term (like in Qwen) is used.\n",
      "Unsloth 2025.3.8 patched 36 layers with 36 QKV layers, 0 O layers and 36 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import is_bfloat16_supported\n",
    "import torch\n",
    "max_seq_length = 1024 # æœ€å¤§åºåˆ—é•¿åº¦ï¼Œå¯ä»¥å¢åŠ ä»¥æ”¯æŒæ›´é•¿çš„æ¨ç†æ–‡æœ¬\n",
    "lora_rank = 32 # LoRAçš„ç§©ï¼Œæ•°å€¼è¶Šå¤§æ¨¡å‹è¶Šæ™ºèƒ½ä½†è®­ç»ƒé€Ÿåº¦è¶Šæ…¢ # 16\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"/home/projects/unsloth-training/models/Qwen2.5-3B-Instruct\", # é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„ï¼Œæˆ–è€…hfåç§°\n",
    "    max_seq_length = max_seq_length, # è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦\n",
    "    load_in_4bit = False, # ä½¿ç”¨4ä½é‡åŒ–åŠ è½½æ¨¡å‹ï¼Œå¯ä»¥èŠ‚çœæ˜¾å­˜ #True\n",
    "    fast_inference = True,# å¯ç”¨vLLMåŠ é€Ÿæ¨ç†\n",
    "    max_lora_rank = lora_rank, # è®¾ç½®LoRAçš„æœ€å¤§ç§©\n",
    "    gpu_memory_utilization = 0.9, # GPUå†…å­˜ä½¿ç”¨ç‡ï¼Œå¦‚æœå‡ºç°OOMå¯ä»¥é™ä½æ­¤å€¼\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model, # é¢„å…ˆåŠ è½½æ¨¡å‹\n",
    "    r = lora_rank, # LoRAçš„ç§©ï¼Œå»ºè®®å€¼ä¸º8, 16, 32, 64æˆ–128\n",
    "    target_modules = [\"gate_proj\", \"up_proj\", \"down_proj\",], # éœ€è¦åº”ç”¨LoRAçš„ç›®æ ‡æ¨¡å—\n",
    "    lora_alpha = lora_rank, # LoRAç¼©æ”¾å‚æ•°ï¼Œé€šå¸¸è®¾ä¸ºä¸rç›¸åŒ\n",
    "    use_gradient_checkpointing = \"unsloth\", # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥æ”¯æŒé•¿æ–‡æœ¬å¾®è°ƒ\n",
    "    random_state = 3407,  # éšæœºæ•°ç§å­ï¼Œç¡®ä¿ç»“æœå¯é‡ç°\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSlkA49z2xZB"
   },
   "source": [
    "### Data Prep\n",
    "<a name=\"Data\"></a>\n",
    "\n",
    "We directly leverage [@willccbb](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb) for data prep and all reward functions. You are free to create your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXk993X6C2ZZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset keys: dict_keys(['instruction', 'input', 'output'])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd4c1b6ec13405cbb407c4ca2a5a30d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import re  # å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼åº“ï¼Œç”¨äºå­—ç¬¦ä¸²åŒ¹é…å’Œæå–\n",
    "from datasets import load_dataset, Dataset  # å¯¼å…¥æ•°æ®é›†å¤„ç†ç›¸å…³åº“\n",
    "\n",
    "# å®šä¹‰ç³»ç»Ÿæç¤ºï¼ŒæŒ‡å®šå“åº”æ ¼å¼\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "å›ç­”éµå¾ªä»¥ä¸‹æ ¼å¼ï¼š\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "# å®šä¹‰XMLæ ¼å¼çš„æ€ç»´é“¾(Chain of Thought)æ ¼å¼æ¨¡æ¿\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    \"\"\"\n",
    "    ä»åŒ…å«XMLæ ‡ç­¾çš„æ–‡æœ¬ä¸­æå–<answer>æ ‡ç­¾å†…çš„ç­”æ¡ˆ\n",
    "    \n",
    "    å‚æ•°:\n",
    "        text: åŒ…å«XMLæ ‡ç­¾çš„æ–‡æœ¬\n",
    "        \n",
    "    è¿”å›:\n",
    "        str: æå–å‡ºçš„ç­”æ¡ˆæ–‡æœ¬ï¼Œå»é™¤é¦–å°¾ç©ºæ ¼\n",
    "    \"\"\"\n",
    "    answer = text.split(\"<answer>\")[-1]  # æå–<answer>æ ‡ç­¾åçš„å†…å®¹\n",
    "    answer = answer.split(\"</answer>\")[0]  # æå–</answer>æ ‡ç­¾å‰çš„å†…å®¹\n",
    "    return answer.strip()  # å»é™¤é¦–å°¾ç©ºæ ¼\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    \"\"\"\n",
    "    ä»æ–‡æœ¬ä¸­æå–####æ ‡è®°åçš„ç­”æ¡ˆï¼ˆç”¨äºå¤„ç†æŸäº›ç‰¹å®šæ ¼å¼çš„æ•°æ®ï¼‰\n",
    "    \n",
    "    å‚æ•°:\n",
    "        text: åŒ…å«####æ ‡è®°çš„æ–‡æœ¬\n",
    "        \n",
    "    è¿”å›:\n",
    "        str | None: æå–å‡ºçš„ç­”æ¡ˆæ–‡æœ¬æˆ–Noneï¼ˆå¦‚æœæ²¡æœ‰####æ ‡è®°ï¼‰\n",
    "    \"\"\"\n",
    "    if \"####\" not in text:  # æ£€æŸ¥æ–‡æœ¬ä¸­æ˜¯å¦æœ‰####æ ‡è®°\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()  # æå–####æ ‡è®°åçš„å†…å®¹å¹¶å»é™¤é¦–å°¾ç©ºæ ¼\n",
    "\n",
    "# åŠ è½½æ•°æ®é›†çš„å‡½æ•°\n",
    "def get_gsm8k_questions(split = \"train\", local_path=\"/home/projects/unsloth-training/datasets/ruozhiba_R1/alpaca_output.jsonl\") -> Dataset:\n",
    "    \"\"\"\n",
    "    ä»æœ¬åœ°è·¯å¾„åŠ è½½æ•°æ®é›†å¹¶è¿›è¡Œå¤„ç†\n",
    "    \n",
    "    å‚æ•°:\n",
    "        split: æ•°æ®é›†åˆ†å‰²ï¼Œé»˜è®¤ä¸º\"train\"\n",
    "        local_path: æœ¬åœ°æ•°æ®é›†è·¯å¾„\n",
    "        \n",
    "    è¿”å›:\n",
    "        Dataset: å¤„ç†åçš„æ•°æ®é›†å¯¹è±¡\n",
    "    \"\"\"\n",
    "    # ä»æœ¬åœ°è·¯å¾„åŠ è½½æ•°æ®é›†\n",
    "    data = load_dataset('json', data_files=local_path, split=split)\n",
    "    \n",
    "    # æ£€æŸ¥æ•°æ®é›†ç»“æ„ï¼Œæ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬çš„é”®\n",
    "    example = data[0]\n",
    "    print(\"Dataset keys:\", example.keys())\n",
    "    \n",
    "    # å¯¹æ•°æ®é›†è¿›è¡Œæ˜ å°„å¤„ç†ï¼Œæ„å»ºé€‚åˆè®­ç»ƒçš„æ ¼å¼\n",
    "    data = data.map(lambda x: {\n",
    "        'prompt': [\n",
    "            # æ·»åŠ ç³»ç»Ÿæç¤ºä½œä¸ºç¬¬ä¸€æ¡æ¶ˆæ¯\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            # æ·»åŠ ç”¨æˆ·é—®é¢˜ï¼Œä¼˜å…ˆä½¿ç”¨'instruction'å­—æ®µï¼Œå¦‚ä¸å­˜åœ¨åˆ™å°è¯•å…¶ä»–å­—æ®µ\n",
    "            {'role': 'user', 'content': x['instruction'] if 'instruction' in x else x.get('input', '')}\n",
    "        ],\n",
    "        # æå–ç­”æ¡ˆï¼Œä¼˜å…ˆä½¿ç”¨'output'å­—æ®µï¼Œå¦‚ä¸å­˜åœ¨åˆ™å°è¯•å…¶ä»–å­—æ®µ\n",
    "        # 'answer': extract_hash_answer(x['output'] if 'output' in x else x.get('response', x.get('answer', '')))\n",
    "    })\n",
    "    return data\n",
    "\n",
    "# åŠ è½½å¹¶å¤„ç†æ•°æ®é›†\n",
    "dataset = get_gsm8k_questions(local_path=\"/home/projects/unsloth-training/datasets/ruozhiba_R1/alpaca_output.jsonl\")\n",
    "\n",
    "# ä»¥ä¸‹æ˜¯å„ç§å¥–åŠ±å‡½æ•°çš„å®šä¹‰ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹ç”Ÿæˆçš„å›ç­”è´¨é‡\n",
    "\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    è¯„ä¼°æ¨¡å‹å›ç­”çš„æ­£ç¡®æ€§ï¼Œä¸æ ‡å‡†ç­”æ¡ˆè¿›è¡Œæ¯”è¾ƒ\n",
    "    \n",
    "    å‚æ•°:\n",
    "        prompts: æä¾›ç»™æ¨¡å‹çš„é—®é¢˜åˆ—è¡¨\n",
    "        completions: æ¨¡å‹ç”Ÿæˆçš„å®Œæˆå†…å®¹åˆ—è¡¨\n",
    "        answer: æ ‡å‡†ç­”æ¡ˆåˆ—è¡¨\n",
    "        **kwargs: é¢å¤–çš„å…³é”®å­—å‚æ•°\n",
    "        \n",
    "    è¿”å›:\n",
    "        list[float]: æ­£ç¡®å›ç­”å¾—2.0åˆ†ï¼Œä¸æ­£ç¡®å¾—0.0åˆ†\n",
    "    \"\"\"\n",
    "    # ä»completionsä¸­æå–å‡ºæ¨¡å‹çš„å®é™…å›ç­”æ–‡æœ¬\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    # è·å–å½“å‰é—®é¢˜æ–‡æœ¬\n",
    "    q = prompts[0][-1]['content']\n",
    "    # ä»å›ç­”ä¸­æå–XMLæ ‡è®°çš„ç­”æ¡ˆå†…å®¹\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    # æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼Œæ˜¾ç¤ºé—®é¢˜ã€æ­£ç¡®ç­”æ¡ˆã€æ¨¡å‹å›ç­”å’Œæå–çš„ç­”æ¡ˆ\n",
    "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "    # æ¯”è¾ƒæå–çš„ç­”æ¡ˆä¸æ ‡å‡†ç­”æ¡ˆï¼Œæ­£ç¡®åˆ™è¿”å›2.0ï¼Œé”™è¯¯åˆ™è¿”å›0.0\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
    "\n",
    "def int_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    æ£€æŸ¥æ¨¡å‹å›ç­”æ˜¯å¦ä¸ºæ•´æ•°\n",
    "    \n",
    "    å‚æ•°:\n",
    "        completions: æ¨¡å‹ç”Ÿæˆçš„å®Œæˆå†…å®¹åˆ—è¡¨\n",
    "        **kwargs: é¢å¤–çš„å…³é”®å­—å‚æ•°\n",
    "        \n",
    "    è¿”å›:\n",
    "        list[float]: å›ç­”ä¸ºæ•´æ•°å¾—0.5åˆ†ï¼Œå¦åˆ™å¾—0.0åˆ†\n",
    "    \"\"\"\n",
    "    # ä»completionsä¸­æå–å‡ºæ¨¡å‹çš„å®é™…å›ç­”æ–‡æœ¬\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    # ä»å›ç­”ä¸­æå–XMLæ ‡è®°çš„ç­”æ¡ˆå†…å®¹\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    # æ£€æŸ¥æå–çš„ç­”æ¡ˆæ˜¯å¦ä¸ºæ•°å­—å­—ç¬¦ä¸²ï¼Œæ˜¯åˆ™è¿”å›0.5ï¼Œå¦åˆ™è¿”å›0.0\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
    "\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    ä¸¥æ ¼æ£€æŸ¥å›ç­”æ˜¯å¦ç¬¦åˆæŒ‡å®šçš„XMLæ ¼å¼\n",
    "    \n",
    "    æ ¼å¼è¦æ±‚: å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹æ ¼å¼\n",
    "    <reasoning>\n",
    "    [æ¨ç†å†…å®¹ï¼Œå¯å¤šè¡Œ]\n",
    "    </reasoning>\n",
    "    <answer>\n",
    "    [ç­”æ¡ˆå†…å®¹ï¼Œå¯å¤šè¡Œ]\n",
    "    </answer>\n",
    "    \n",
    "    å‚æ•°:\n",
    "        completions: æ¨¡å‹ç”Ÿæˆçš„å®Œæˆå†…å®¹åˆ—è¡¨\n",
    "        **kwargs: é¢å¤–çš„å…³é”®å­—å‚æ•°\n",
    "        \n",
    "    è¿”å›:\n",
    "        list[float]: æ ¼å¼æ­£ç¡®å¾—0.5åˆ†ï¼Œå¦åˆ™å¾—0.0åˆ†\n",
    "    \"\"\"\n",
    "    # å®šä¹‰ä¸¥æ ¼çš„XMLæ ¼å¼æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼Œè¦æ±‚ç²¾ç¡®åŒ¹é…å¼€å§‹å’Œç»“æŸæ ‡ç­¾ä»¥åŠæ¢è¡Œ\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    # ä»completionsä¸­æå–å‡ºæ¨¡å‹çš„å®é™…å›ç­”æ–‡æœ¬\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ£€æŸ¥æ ¼å¼æ˜¯å¦åŒ¹é…\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    # åŒ¹é…æˆåŠŸè¿”å›0.5ï¼Œå¦åˆ™è¿”å›0.0\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    å®½æ¾æ£€æŸ¥å›ç­”æ˜¯å¦ç¬¦åˆXMLæ ¼å¼\n",
    "    \n",
    "    æ ¼å¼è¦æ±‚: åªè¦åŒ…å«<reasoning>æ ‡ç­¾å’Œ<answer>æ ‡ç­¾å³å¯ï¼Œä¸ä¸¥æ ¼è¦æ±‚æ¢è¡Œå’Œé¡ºåº\n",
    "    \n",
    "    å‚æ•°:\n",
    "        completions: æ¨¡å‹ç”Ÿæˆçš„å®Œæˆå†…å®¹åˆ—è¡¨\n",
    "        **kwargs: é¢å¤–çš„å…³é”®å­—å‚æ•°\n",
    "        \n",
    "    è¿”å›:\n",
    "        list[float]: æ ¼å¼æ­£ç¡®å¾—0.5åˆ†ï¼Œå¦åˆ™å¾—0.0åˆ†\n",
    "    \"\"\"\n",
    "    # å®šä¹‰å®½æ¾çš„XMLæ ¼å¼æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼Œåªè¦æ±‚åŒ…å«æ ‡ç­¾ï¼Œä¸é™åˆ¶æ¢è¡Œæ ¼å¼\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    # ä»completionsä¸­æå–å‡ºæ¨¡å‹çš„å®é™…å›ç­”æ–‡æœ¬\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ£€æŸ¥æ ¼å¼æ˜¯å¦åŒ¹é…\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    # åŒ¹é…æˆåŠŸè¿”å›0.5ï¼Œå¦åˆ™è¿”å›0.0\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def count_xml(text) -> float:\n",
    "    \"\"\"\n",
    "    è®¡ç®—XMLæ ‡ç­¾çš„æ­£ç¡®ä½¿ç”¨æƒ…å†µï¼Œå¹¶ç»™äºˆåˆ†æ•°å¥–åŠ±\n",
    "    \n",
    "    å‚æ•°:\n",
    "        text: éœ€è¦è¯„ä¼°çš„æ–‡æœ¬\n",
    "        \n",
    "    è¿”å›:\n",
    "        float: æ ¹æ®XMLæ ‡ç­¾çš„æ­£ç¡®ä½¿ç”¨æƒ…å†µè®¡ç®—çš„åˆ†æ•°(æœ€é«˜0.5åˆ†)\n",
    "    \"\"\"\n",
    "    count = 0.0\n",
    "    # æ£€æŸ¥æ˜¯å¦æ­£ç¡®ä½¿ç”¨<reasoning>æ ‡ç­¾ï¼Œæ­£ç¡®å¾—0.125åˆ†\n",
    "    if text.count(\"<reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    # æ£€æŸ¥æ˜¯å¦æ­£ç¡®ä½¿ç”¨</reasoning>æ ‡ç­¾ï¼Œæ­£ç¡®å¾—0.125åˆ†\n",
    "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    # æ£€æŸ¥æ˜¯å¦æ­£ç¡®ä½¿ç”¨<answer>æ ‡ç­¾ï¼Œæ­£ç¡®å¾—0.125åˆ†\n",
    "    # åŒæ—¶å‡å»</answer>åå¤šä½™å†…å®¹çš„æƒ©ç½šåˆ†\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001  # å¯¹å¤šä½™å†…å®¹è¿›è¡Œæƒ©ç½š\n",
    "    # æ£€æŸ¥æ˜¯å¦æ­£ç¡®ä½¿ç”¨</answer>æ ‡ç­¾ï¼Œæ­£ç¡®å¾—0.125åˆ†\n",
    "    # åŒæ—¶å‡å»</answer>åå¤šä½™å†…å®¹çš„æƒ©ç½šåˆ†\n",
    "    if text.count(\"\\n</answer>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001  # å¯¹å¤šä½™å†…å®¹è¿›è¡Œæƒ©ç½š\n",
    "    return count\n",
    "\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    è¯„ä¼°å›ç­”ä¸­XMLæ ‡ç­¾çš„æ­£ç¡®ä½¿ç”¨æƒ…å†µ\n",
    "    \n",
    "    å‚æ•°:\n",
    "        completions: æ¨¡å‹ç”Ÿæˆçš„å®Œæˆå†…å®¹åˆ—è¡¨\n",
    "        **kwargs: é¢å¤–çš„å…³é”®å­—å‚æ•°\n",
    "        \n",
    "    è¿”å›:\n",
    "        list[float]: æ¯ä¸ªå›ç­”çš„XMLæ ¼å¼è¯„åˆ†(0-0.5ä¹‹é—´)\n",
    "    \"\"\"\n",
    "    # ä»completionsä¸­æå–å‡ºæ¨¡å‹çš„å®é™…å›ç­”æ–‡æœ¬\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    # å¯¹æ¯ä¸ªå›ç­”æ–‡æœ¬è¯„ä¼°XMLæ ‡ç­¾ä½¿ç”¨æƒ…å†µ\n",
    "    return [count_xml(c) for c in contents]\n",
    "\n",
    "# æ·»åŠ ä¸€ä¸ªæ£€æŸ¥ æ€è€ƒè¿‡ç¨‹çš„æ–‡æœ¬å’Œæœ€åçš„æ–‡æœ¬ç›¸ä¼¼åº¦çš„å‡½æ•°ï¼Œç¡®ä¿ç»“æœä¸ä¼šå’Œæ€è€ƒè¿‡ç¨‹ç›¸åŒ\n",
    "\n",
    "def reasoning_length_reward_func(completions, max_length=1024, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    å¥–åŠ±æ¨ç†æ–‡æœ¬é•¿åº¦ï¼Œæ–‡æœ¬è¶Šé•¿å¥–åŠ±è¶Šé«˜ï¼Œæœ€é«˜5åˆ†\n",
    "    \n",
    "    å¥–åŠ±ä¸æ¨ç†æ–‡æœ¬çš„é•¿åº¦å‘ˆçº¿æ€§å…³ç³»ï¼Œç›´åˆ°è¾¾åˆ°max_lengthä¸ªå­—ç¬¦ï¼Œ\n",
    "    ä¹‹åå°†ç»™äºˆæ»¡åˆ†5.0åˆ†ã€‚\n",
    "    \n",
    "    å‚æ•°:\n",
    "        completions: æ¨¡å‹ç”Ÿæˆçš„å®Œæˆå†…å®¹åˆ—è¡¨\n",
    "        max_length: è·å¾—æœ€é«˜å¥–åŠ±çš„å­—ç¬¦æ•°ï¼ˆé»˜è®¤ï¼š500ï¼‰\n",
    "        **kwargs: é¢å¤–çš„å…³é”®å­—å‚æ•°\n",
    "        \n",
    "    è¿”å›:\n",
    "        list[float]: åŸºäºæ¨ç†æ–‡æœ¬é•¿åº¦çš„å¥–åŠ±ï¼ˆ0.0åˆ°5.0ä¹‹é—´ï¼‰\n",
    "    \"\"\"\n",
    "    # ä»completionsä¸­æå–å‡ºæ¨¡å‹çš„å®é™…å›ç­”æ–‡æœ¬\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    rewards = []\n",
    "    \n",
    "    for response in responses:\n",
    "        try:\n",
    "            # æå–<reasoning>å’Œ</reasoning>æ ‡ç­¾ä¹‹é—´çš„æ–‡æœ¬\n",
    "            reasoning_match = re.search(r\"<reasoning>(.*?)</reasoning>\", response, re.DOTALL)\n",
    "            \n",
    "            if reasoning_match:\n",
    "                reasoning_text = reasoning_match.group(1).strip()\n",
    "                text_length = len(reasoning_text)\n",
    "                \n",
    "                # çº¿æ€§ç¼©æ”¾ï¼šreward = 5.0 * min(1.0, text_length / max_length)\n",
    "                # åœ¨max_lengthå­—ç¬¦æˆ–æ›´å¤šæ—¶ç»™äºˆæ»¡åˆ†5.0åˆ†\n",
    "                reward = 2 * min(0.0, text_length / max_length)\n",
    "                \n",
    "                # æ‰“å°è°ƒè¯•ä¿¡æ¯\n",
    "                # print(f\"æ¨ç†é•¿åº¦: {text_length} å­—ç¬¦, å¥–åŠ±: {reward:.2f}\")\n",
    "                \n",
    "                rewards.append(reward)\n",
    "            else:\n",
    "                # æœªæ‰¾åˆ°reasoningæ ‡ç­¾\n",
    "                rewards.append(0.0)\n",
    "        except Exception as e:\n",
    "            # å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™\n",
    "            rewards.append(0.0)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tze5NF5523DB"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "\n",
    "Now set up GRPO Trainer and all configurations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptqkXK2D4d6p",
    "outputId": "344b54e8-5a9c-4676-bfc0-23f8b5cb7426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 1 to the `num_generations` of 6\n"
     ]
    }
   ],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    use_vllm = True, # ä½¿ç”¨vLLMè¿›è¡Œæ¨ç†åŠ é€Ÿï¼Œæ˜¾è‘—æé«˜ç”Ÿæˆå’Œè¯„ä¼°é€Ÿåº¦\n",
    "    learning_rate = 5e-6, # å­¦ä¹ ç‡è®¾ç½®ä¸º5e-6ï¼Œé€‚åˆLoRAå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹\n",
    "    adam_beta1 = 0.9, # Adamä¼˜åŒ–å™¨çš„beta1å‚æ•°ï¼Œæ§åˆ¶ä¸€é˜¶çŸ©ä¼°è®¡çš„æŒ‡æ•°è¡°å‡ç‡\n",
    "    adam_beta2 = 0.99, # Adamä¼˜åŒ–å™¨çš„beta2å‚æ•°ï¼Œæ§åˆ¶äºŒé˜¶çŸ©ä¼°è®¡çš„æŒ‡æ•°è¡°å‡ç‡ï¼Œ\n",
    "    weight_decay = 0.1, # æƒé‡è¡°å‡ç³»æ•°ï¼Œç”¨äºL2æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    "    warmup_ratio = 0.1, # å­¦ä¹ ç‡é¢„çƒ­æ¯”ä¾‹ï¼Œåœ¨è®­ç»ƒåˆæœŸé€æ¸å¢åŠ å­¦ä¹ ç‡ï¼Œå æ€»è®­ç»ƒæ­¥æ•°çš„10%\n",
    "    lr_scheduler_type = \"cosine\", # å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹ï¼Œä½™å¼¦é€€ç«å¯ä»¥å¹³æ»‘åœ°é™ä½å­¦ä¹ ç‡\n",
    "    optim = \"paged_adamw_8bit\", # ä¼˜åŒ–å™¨ç±»å‹ï¼Œä½¿ç”¨8ä½é‡åŒ–çš„Adamä¼˜åŒ–å™¨å‡å°‘å†…å­˜å ç”¨\n",
    "    logging_steps = 1, # æ¯æ­¥è®­ç»ƒåè®°å½•æ—¥å¿—ï¼Œä¾¿äºå®æ—¶ç›‘æ§è®­ç»ƒçŠ¶æ€\n",
    "    bf16 = is_bfloat16_supported(), # å¦‚æœæ”¯æŒbfloat16åˆ™å¯ç”¨ï¼Œæé«˜è®­ç»ƒé€Ÿåº¦å¹¶å‡å°‘å†…å­˜ä½¿ç”¨\n",
    "    fp16 = not is_bfloat16_supported(), # å½“ä¸æ”¯æŒbfloat16æ—¶ï¼Œä½¿ç”¨fp16æ··åˆç²¾åº¦è®­ç»ƒ\n",
    "    per_device_train_batch_size = 1, # æ¯ä¸ªè®¾å¤‡çš„è®­ç»ƒæ‰¹é‡å¤§å°ï¼ŒGRPOä¼šè‡ªåŠ¨è°ƒæ•´ä¸ºåŒ¹é…num_generations\n",
    "    gradient_accumulation_steps = 4, # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œ1è¡¨ç¤ºæ¯æ­¥æ›´æ–°ä¸€æ¬¡æ¨¡å‹å‚æ•°ï¼ˆå¯å¢åŠ åˆ°4ä»¥ç¨³å®šè®­ç»ƒï¼‰ #1\n",
    "    num_generations = 6, # æ¯æ¬¡è¯„ä¼°ç”Ÿæˆçš„æ ·æœ¬æ•°é‡ï¼Œå½±å“å¤šæ ·æ€§å’Œå†…å­˜ä½¿ç”¨\n",
    "    max_prompt_length = 1024, # è¾“å…¥æç¤ºçš„æœ€å¤§é•¿åº¦ï¼ˆtokenæ•°ï¼‰ï¼Œè¶…è¿‡ä¼šè¢«æˆªæ–­\n",
    "    max_completion_length = 1024,  # ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦ï¼ˆtokenæ•°ï¼‰ï¼Œé™åˆ¶æ¨¡å‹è¾“å‡ºé•¿åº¦\n",
    "    # num_train_epochs = 1, # å®Œæ•´è®­ç»ƒçš„è½®æ•°ï¼Œå½“å‰è¢«æ³¨é‡Šï¼Œä½¿ç”¨max_stepsæ§åˆ¶è®­ç»ƒé•¿åº¦\n",
    "    max_steps = 100, # è®­ç»ƒçš„æœ€å¤§æ­¥æ•°ï¼Œ100æ­¥ä¸ºå¿«é€Ÿå®éªŒè®¾ç½®\n",
    "    save_steps = 250, # æ¯250æ­¥ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹ï¼Œç”¨äºæ¢å¤è®­ç»ƒæˆ–è¯„ä¼°\n",
    "    max_grad_norm = 0.1, # æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸\n",
    "    report_to = \"none\", # è®­ç»ƒè¿‡ç¨‹æŠ¥å‘Šå·¥å…·ï¼Œ\"none\"è¡¨ç¤ºä¸ä½¿ç”¨å¤–éƒ¨å·¥å…·ï¼Œå¯é€‰ç”¨W&Bç­‰\n",
    "    output_dir = \"outputs\", # è¾“å‡ºç›®å½•ï¼Œç”¨äºä¿å­˜æ¨¡å‹ã€æ—¥å¿—å’Œæ£€æŸ¥ç‚¹\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8egDqHG3GH0"
   },
   "source": [
    "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
    "\n",
    "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
    "\n",
    "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
    "|------|---------------|-----------|------------|-------------------|----------|\n",
    "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
    "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
    "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vzOuSVCL_GA9",
    "outputId": "0fe20ec2-ea69-486a-e2df-4685bd390413"
   },
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model, ## ä¼ å…¥é¢„åŠ è½½çš„æ¨¡å‹ï¼Œä¹‹å‰å·²ä½¿ç”¨LoRAæ–¹æ³•å‡†å¤‡å¥½\n",
    "    processing_class = tokenizer, # ä¼ å…¥åˆ†è¯å™¨ï¼Œç”¨äºæ–‡æœ¬å¤„ç†å’Œç¼–ç \n",
    "    reward_funcs = [\n",
    "        xmlcount_reward_func, # æ£€æŸ¥XMLæ ‡ç­¾çš„æ­£ç¡®ä½¿ç”¨ï¼ˆ<reasoning>å’Œ<answer>æ ‡ç­¾ï¼‰å¹¶ç»™äºˆå¥–åŠ±\n",
    "        soft_format_reward_func, # å®½æ¾åœ°æ£€æŸ¥å›ç­”æ˜¯å¦ç¬¦åˆXMLæ ¼å¼ï¼Œåªè¦åŒ…å«æ ‡ç­¾å³å¯\n",
    "        strict_format_reward_func, # ä¸¥æ ¼æ£€æŸ¥å›ç­”æ˜¯å¦ç¬¦åˆXMLæ ¼å¼ï¼ŒåŒ…æ‹¬æ¢è¡Œå’Œé¡ºåº\n",
    "        int_reward_func, # æ£€æŸ¥å›ç­”ä¸­çš„ç­”æ¡ˆæ˜¯å¦ä¸ºæ•´æ•°å¹¶ç»™äºˆå¥–åŠ±\n",
    "        # correctness_reward_func, # ä¸æ ‡å‡†ç­”æ¡ˆè¿›è¡Œæ¯”è¾ƒï¼Œè¯„ä¼°å›ç­”çš„æ­£ç¡®æ€§\n",
    "        reasoning_length_reward_func,\n",
    "    ],\n",
    "    # è®­ç»ƒå‚æ•°é…ç½®ï¼Œä¹‹å‰å·²å®šä¹‰\n",
    "    args = training_args,\n",
    "    # è®­ç»ƒæ•°æ®é›†ï¼Œå·²é¢„å¤„ç†æˆåŒ…å«promptå’Œanswerçš„æ ¼å¼\n",
    "    train_dataset = dataset,\n",
    ")\n",
    "\n",
    "# å¯åŠ¨è®­ç»ƒè¿‡ç¨‹\n",
    "# æ¨¡å‹ä¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œæ ¹æ®ä¸Šè¿°å¥–åŠ±å‡½æ•°åé¦ˆä¸æ–­è°ƒæ•´ç”Ÿæˆç­–ç•¥\n",
    "# ç›®æ ‡æ˜¯å­¦ä¹ ç”Ÿæˆç¬¦åˆXMLæ ¼å¼çš„å›ç­”ï¼ŒåŒ…å«æ¨ç†è¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆ\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbdvvDCbLrLe"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "\n",
    "Now let's try the model we just trained! First, let's first try the model without any GRPO trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "id": "urQZvMTaLrrQ",
    "outputId": "b2ab3c22-cfd8-43b1-b173-4b780cd3fed0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template([\n\u001b[1;32m      2\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mç»™æˆ‘åˆ›å»ºä¸€ä¸ªæœ‰å…³äºé¸Ÿçš„ glsl ä»£ç \u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      3\u001b[0m ], tokenize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SamplingParams\n\u001b[1;32m      6\u001b[0m sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams(\n\u001b[1;32m      7\u001b[0m     temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m,\n\u001b[1;32m      8\u001b[0m     top_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.95\u001b[39m,\n\u001b[1;32m      9\u001b[0m     max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"user\", \"content\" : \"ç»™æˆ‘åˆ›å»ºä¸€ä¸ªæœ‰å…³äºé¸Ÿçš„ glsl ä»£ç \"},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    [text],\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = None,\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é¦–å…ˆï¼Œæˆ‘ä»¬æ¥åˆ†æç»™å®šæ–¹ç¨‹ \\(\\sqrt{a} - \\sqrt{a} + x = x\\)ã€‚\n",
      "\n",
      "è¿™ä¸ªæ–¹ç¨‹å¯ä»¥ç®€åŒ–ä¸ºï¼š\\(0 + x = x\\) æˆ–è€…ç®€å†™ä¸º \\(x = x\\)ã€‚è¿™çœ‹èµ·æ¥åƒæ˜¯ä¸€ä¸ªæ’ç­‰å¼ï¼Œå®ƒå¯¹äºæ‰€æœ‰å®šä¹‰åŸŸå†…çš„ \\(x\\) éƒ½æˆç«‹ï¼Œå› æ­¤åœ¨æŸç§æ„ä¹‰ä¸Šï¼Œè¿™è¡¨ç¤ºæ–¹ç¨‹å¯¹äºæ‰€æœ‰ \\(x\\) éƒ½æ˜¯æ­£ç¡®çš„ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘åˆ°åŸå§‹æ–¹ç¨‹ä¸­ \\(a > 1\\) çš„æ¡ä»¶ã€‚å®é™…ä¸Šï¼ŒåŸå§‹æ–¹ç¨‹ç®€åŒ–åçš„ç­‰å¼ \\(x = x\\) å¹¶æ²¡æœ‰æä¾›å…³äº \\(x\\) çš„é¢å¤–é™åˆ¶ï¼Œæ‰€ä»¥å®ƒåœ¨ \\(x\\) çš„ä»»ä½•å€¼ä¸Šéƒ½æˆç«‹ã€‚\n",
      "\n",
      "ç»™å®š \\(a > 1\\) å¹¶ä¸ä¼šå½±å“åˆ° \\(x = x\\) çš„ç»“è®ºï¼Œå› ä¸ºåœ¨ \\(x = x\\) çš„æƒ…å†µä¸‹ï¼Œ\\(x\\) å¯ä»¥æ˜¯ä»»ä½•å®æ•°ã€‚å› æ­¤ï¼Œæ²¡æœ‰ç‰¹å®šçš„ \\(x\\) å€¼è¢«æ’é™¤åœ¨å¯èƒ½çš„è§£ä¹‹å¤–ï¼Œè¯´æ˜è¿™ä¸ªæ–¹ç¨‹çš„å®æ•°è§£çš„é›†åˆæ˜¯æ— é™çš„ï¼Œå®ƒåŒ…å«äº†æ‰€æœ‰çš„å®æ•°ã€‚\n",
      "\n",
      "æ‰€ä»¥ï¼Œå¦‚æœæ–¹ç¨‹ \\(\\sqrt{a} - \\sqrt{a} + x = x\\) çš„è§£æ˜¯æ‰€æœ‰ \\(x\\) çš„å®æ•°ï¼Œé‚£ä¹ˆè§£çš„å’Œä¾ç„¶æ˜¯æ‰€æœ‰å®æ•°çš„å’Œï¼Œè€Œåœ¨æ•°å­¦ä¸­ï¼Œæ‰€æœ‰å®æ•°çš„å’Œå¹¶ä¸å­˜åœ¨ä¸€ä¸ªå…·ä½“çš„æ•°å€¼ï¼Œå®ƒæ˜¯æœªå®šä¹‰çš„ã€‚\n",
      "\n",
      "æ€»ç»“æ¥è¯´ï¼Œå½“ \\(a > 1\\) æ—¶ï¼Œæ–¹ç¨‹ \\(\\sqrt{a} - \\sqrt{a} + x = x\\) çš„å®æ•°è§£ä¹‹å’Œä¸ºæœªå®šä¹‰ã€‚\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXfSTmXFLyIE"
   },
   "source": [
    "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XOed9DauLydR"
   },
   "outputs": [],
   "source": [
    "model.save_lora(\"grpo_saved_lora\") # ä¿å­˜LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45U-8F0nL1Uf"
   },
   "source": [
    "Now we load the LoRA and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "id": "__w_7GamL1m1",
    "outputId": "2402a0e9-6ec0-4f65-9921-311888040df9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.81s/it, est. speed input: 12.49 toks/s, output: 67.43 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<reasoning>\\nè¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé¦–å…ˆéœ€è¦ç†è§£å’Œç®€åŒ–ç»™å®šçš„æ–¹ç¨‹ \\\\(\\\\sqrt{a} - \\\\sqrt{a} + x = x\\\\)ã€‚æˆ‘ä»¬å¯ä»¥æ³¨æ„åˆ° \\\\(\\\\sqrt{a} - \\\\sqrt{a} = 0\\\\)ï¼Œè¿™æ„å‘³ç€åŸæ–¹ç¨‹ç®€åŒ–ä¸º \\\\(0 + x = x\\\\)ï¼Œå³ \\\\(x = x\\\\)ã€‚è¿™æ„å‘³ç€åŸæ–¹ç¨‹å¯¹äºæ‰€æœ‰å®æ•° \\\\(x\\\\) éƒ½æˆç«‹ï¼Œè¿™æ„å‘³ç€ä»»ä½•å®æ•°éƒ½æ˜¯è¿™ä¸ªæ–¹ç¨‹çš„è§£ã€‚ç”±äºè¯¥æ–¹ç¨‹å¯¹äºæ‰€æœ‰ \\\\(x\\\\) éƒ½æ˜¯æˆç«‹çš„ï¼Œæ‰€ä»¥å®æ•°è§£çš„é›†åˆåŒ…å«äº†æ‰€æœ‰å®æ•°ã€‚å¦‚æœè¦æ±‚è§£çš„å®æ•°è§£ä¹‹å’Œï¼Œç”±äºå®æ•°é›†åˆåŒ…å«æ‰€æœ‰çš„å®æ•°ï¼Œè€Œå®æ•°é›†åˆæ²¡æœ‰ä¸€ä¸ªç¡®å®šçš„å’Œï¼Œå› æ­¤è§£çš„å’Œå°†æ²¡æœ‰ä¸€ä¸ªæ˜ç¡®çš„æ•°å€¼ã€‚ä½†è‹¥ä¸¥æ ¼æŒ‰ç…§é¢˜æ„æ±‚æ‰€æœ‰å¯èƒ½çš„å®æ•°è§£ä¹‹å’Œï¼Œç»“æœå°†ä¸ºæ‰€æœ‰å®æ•°çš„å¹³å‡å€¼ï¼Œè¿™åœ¨ç°å®ä¸­æ˜¯ä¸å­˜åœ¨çš„ã€‚ä½†æ˜¯ï¼Œæ ¹æ®é¢˜ç›®çš„é€»è¾‘ï¼Œå®é™…ä¸Šæ¯ä¸ªå®æ•°è§£ç›¸åŠ çš„ç»“æœè¿˜æ˜¯ä¿æŒä¸å˜ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼ŒåŸæ–¹ç¨‹ç»™å®šæ¡ä»¶ä¸å½±å“ç»“æœï¼Œè§£ä»ç„¶æ˜¯æ‰€æœ‰å®æ•°ï¼Œä¸”æ²¡æœ‰ä¸€ä¸ªå…·ä½“çš„å®æ•°å’Œã€‚è€ƒè™‘åˆ°ä»¥ä¸Šæƒ…å†µï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºç»“è®ºå®æ•°è§£ä¹‹å’Œä¸º0ï¼Œç”±äºé¢˜ç›®æ²¡æœ‰æ˜ç¡®é™å®š \\\\(x\\\\) çš„èŒƒå›´ï¼Œå‡è®¾ \\\\(x\\\\) çš„å–å€¼ä»è´Ÿæ— ç©·å¤§åˆ°æ­£æ— ç©·å¤§ï¼Œå®æ•°è§£ä¹‹å’Œå¯ä»¥ç†è§£ä¸ºæ‰€æœ‰ \\\\(x\\\\) çš„å–å€¼ç›¸åŠ ä¸º0ï¼ˆå³ä¸­æ€§å€¼ï¼‰ã€‚\\n</reasoning>\\n<answer>\\n0\\n</answer>\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
    "    {\"role\" : \"user\", \"content\" : \"å¦‚æœ a > 1ï¼Œåˆ™ âˆšï¸ aâˆ’âˆš a + x = x çš„å®æ•°è§£ä¹‹å’Œç­‰äº?\"},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SP998x4tMRFE",
    "outputId": "13ea89c4-8b26-4ee7-9fec-9ed3441eaa53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<reasoning>\n",
      "è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé¦–å…ˆéœ€è¦ç†è§£å’Œç®€åŒ–ç»™å®šçš„æ–¹ç¨‹ \\(\\sqrt{a} - \\sqrt{a} + x = x\\)ã€‚æˆ‘ä»¬å¯ä»¥æ³¨æ„åˆ° \\(\\sqrt{a} - \\sqrt{a} = 0\\)ï¼Œè¿™æ„å‘³ç€åŸæ–¹ç¨‹ç®€åŒ–ä¸º \\(0 + x = x\\)ï¼Œå³ \\(x = x\\)ã€‚è¿™æ„å‘³ç€åŸæ–¹ç¨‹å¯¹äºæ‰€æœ‰å®æ•° \\(x\\) éƒ½æˆç«‹ï¼Œè¿™æ„å‘³ç€ä»»ä½•å®æ•°éƒ½æ˜¯è¿™ä¸ªæ–¹ç¨‹çš„è§£ã€‚ç”±äºè¯¥æ–¹ç¨‹å¯¹äºæ‰€æœ‰ \\(x\\) éƒ½æ˜¯æˆç«‹çš„ï¼Œæ‰€ä»¥å®æ•°è§£çš„é›†åˆåŒ…å«äº†æ‰€æœ‰å®æ•°ã€‚å¦‚æœè¦æ±‚è§£çš„å®æ•°è§£ä¹‹å’Œï¼Œç”±äºå®æ•°é›†åˆåŒ…å«æ‰€æœ‰çš„å®æ•°ï¼Œè€Œå®æ•°é›†åˆæ²¡æœ‰ä¸€ä¸ªç¡®å®šçš„å’Œï¼Œå› æ­¤è§£çš„å’Œå°†æ²¡æœ‰ä¸€ä¸ªæ˜ç¡®çš„æ•°å€¼ã€‚ä½†è‹¥ä¸¥æ ¼æŒ‰ç…§é¢˜æ„æ±‚æ‰€æœ‰å¯èƒ½çš„å®æ•°è§£ä¹‹å’Œï¼Œç»“æœå°†ä¸ºæ‰€æœ‰å®æ•°çš„å¹³å‡å€¼ï¼Œè¿™åœ¨ç°å®ä¸­æ˜¯ä¸å­˜åœ¨çš„ã€‚ä½†æ˜¯ï¼Œæ ¹æ®é¢˜ç›®çš„é€»è¾‘ï¼Œå®é™…ä¸Šæ¯ä¸ªå®æ•°è§£ç›¸åŠ çš„ç»“æœè¿˜æ˜¯ä¿æŒä¸å˜ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼ŒåŸæ–¹ç¨‹ç»™å®šæ¡ä»¶ä¸å½±å“ç»“æœï¼Œè§£ä»ç„¶æ˜¯æ‰€æœ‰å®æ•°ï¼Œä¸”æ²¡æœ‰ä¸€ä¸ªå…·ä½“çš„å®æ•°å’Œã€‚è€ƒè™‘åˆ°ä»¥ä¸Šæƒ…å†µï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºç»“è®ºå®æ•°è§£ä¹‹å’Œä¸º0ï¼Œç”±äºé¢˜ç›®æ²¡æœ‰æ˜ç¡®é™å®š \\(x\\) çš„èŒƒå›´ï¼Œå‡è®¾ \\(x\\) çš„å–å€¼ä»è´Ÿæ— ç©·å¤§åˆ°æ­£æ— ç©·å¤§ï¼Œå®æ•°è§£ä¹‹å’Œå¯ä»¥ç†è§£ä¸ºæ‰€æœ‰ \\(x\\) çš„å–å€¼ç›¸åŠ ä¸º0ï¼ˆå³ä¸­æ€§å€¼ï¼‰ã€‚\n",
      "</reasoning>\n",
      "<answer>\n",
      "0\n",
      "</answer>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gzZDHijL_3l"
   },
   "source": [
    "Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTTciyNnMCI2"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYVi3GLfMCg4"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fT7HEOzDMDcI"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
    "\n",
    "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-p9BiitMF63"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvsXeMAQPjQR"
   },
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n",
    "\n",
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "Some other links:\n",
    "1. Llama 3.2 Conversational notebook. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)\n",
    "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
    "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
    "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
    "\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
    "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
    "\n",
    "  Join Discord if you need help + â­ï¸ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> â­ï¸\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "https://github.com/unslothai/notebooks/blob/main/nb/Phi_4_(14B)-GRPO.ipynb",
     "timestamp": 1741194453492
    }
   ]
  },
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0b9230e976b34a9ea85978cf22857012": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f8ead1775934dc3a10533b67b3dd905": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "129dc789722b43439574390bba63b36a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b9f8a2a793640d689abc10f5f39c54b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22e0933485c14d94b0c1cfe198d6758f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_43462d5de24b4e55871b3f579798b374",
       "IPY_MODEL_99577e7cbed74c89afb3d44d4fd956c5",
       "IPY_MODEL_d034c840e7f74177a7b07a188d666b8d"
      ],
      "layout": "IPY_MODEL_0f8ead1775934dc3a10533b67b3dd905"
     }
    },
    "4089236deafd4fa2be86d8dc0a29d469": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43462d5de24b4e55871b3f579798b374": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e43ad27d5d304d1ebf9b374016409a97",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_51948945111f437c9ed6ccab22072dd3",
      "value": ""
     }
    },
    "47d2fd7f76754d9fa156576bc0c58abb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c9248100f89400d9e1407dbb168d5d6",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_96cea0d773c8426b8be72dd7f72e5a82",
      "value": ""
     }
    },
    "4c9248100f89400d9e1407dbb168d5d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51948945111f437c9ed6ccab22072dd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "627f68389cf64e2a915a72ab147ee8a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "78cc90a50c0c4636b0f41436a820ecd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "81a0791760de4dcebd543c40d2c1e322": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b9f8a2a793640d689abc10f5f39c54b",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_627f68389cf64e2a915a72ab147ee8a7",
      "value": 2
     }
    },
    "8991360910ef417db03499f76f5fe323": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "96cea0d773c8426b8be72dd7f72e5a82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "99577e7cbed74c89afb3d44d4fd956c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4089236deafd4fa2be86d8dc0a29d469",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_78cc90a50c0c4636b0f41436a820ecd3",
      "value": 2
     }
    },
    "9eed940f3815428583b4ddefc1a81469": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a729c5fc5c764c85885cac7a2d4d95d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9eed940f3815428583b4ddefc1a81469",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0b9230e976b34a9ea85978cf22857012",
      "value": "Loadingâ€‡safetensorsâ€‡checkpointâ€‡shards:â€‡100%â€‡Completedâ€‡|â€‡2/2â€‡[00:51&lt;00:00,â€‡25.46s/it]\n"
     }
    },
    "d034c840e7f74177a7b07a188d666b8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_129dc789722b43439574390bba63b36a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8991360910ef417db03499f76f5fe323",
      "value": "Loadingâ€‡safetensorsâ€‡checkpointâ€‡shards:â€‡100%â€‡Completedâ€‡|â€‡2/2â€‡[00:47&lt;00:00,â€‡23.63s/it]\n"
     }
    },
    "d6d5a7d96a034247b38d25d8a9cc979c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e43ad27d5d304d1ebf9b374016409a97": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f57d844b2efa469e8aadd48175ce70ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_47d2fd7f76754d9fa156576bc0c58abb",
       "IPY_MODEL_81a0791760de4dcebd543c40d2c1e322",
       "IPY_MODEL_a729c5fc5c764c85885cac7a2d4d95d0"
      ],
      "layout": "IPY_MODEL_d6d5a7d96a034247b38d25d8a9cc979c"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
