{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4whfu8wzPjQO"
   },
   "source": [
    "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgjdVJflPjQO"
   },
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biwbaQvsPjQO"
   },
   "source": [
    "**Read our [blog post](https://unsloth.ai/blog/r1-reasoning) for guidance on how to train reasoning models.**\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdjNKWvSPjQP"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaBUw4wJ2WlP"
   },
   "source": [
    "Use `PatchFastRL` before all functions to patch GRPO and other RL algorithms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59DIs5BMcvjN",
    "outputId": "5d03d8e6-d961-4101-8cd5-669944f91a5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, PatchFastRL\n",
    "PatchFastRL(\"GRPO\", FastLanguageModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 805,
     "referenced_widgets": [
      "f57d844b2efa469e8aadd48175ce70ab",
      "47d2fd7f76754d9fa156576bc0c58abb",
      "81a0791760de4dcebd543c40d2c1e322",
      "a729c5fc5c764c85885cac7a2d4d95d0",
      "d6d5a7d96a034247b38d25d8a9cc979c",
      "4c9248100f89400d9e1407dbb168d5d6",
      "96cea0d773c8426b8be72dd7f72e5a82",
      "1b9f8a2a793640d689abc10f5f39c54b",
      "627f68389cf64e2a915a72ab147ee8a7",
      "9eed940f3815428583b4ddefc1a81469",
      "0b9230e976b34a9ea85978cf22857012",
      "22e0933485c14d94b0c1cfe198d6758f",
      "43462d5de24b4e55871b3f579798b374",
      "99577e7cbed74c89afb3d44d4fd956c5",
      "d034c840e7f74177a7b07a188d666b8d",
      "0f8ead1775934dc3a10533b67b3dd905",
      "e43ad27d5d304d1ebf9b374016409a97",
      "51948945111f437c9ed6ccab22072dd3",
      "4089236deafd4fa2be86d8dc0a29d469",
      "78cc90a50c0c4636b0f41436a820ecd3",
      "129dc789722b43439574390bba63b36a",
      "8991360910ef417db03499f76f5fe323"
     ]
    },
    "id": "DkIvEkIIkEyB",
    "outputId": "c5a32856-2166-4485-fdb3-16241d0e6316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-11 00:49:29 __init__.py:207] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.3.8: Fast Qwen2 patching. Transformers: 4.49.0. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 Ti SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading /home/projects/unsloth-training/models/Qwen2.5-3B-Instruct with actual GPU utilization = 82.75%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 15.99 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 1024. Num Sequences = 192.\n",
      "Unsloth: vLLM's KV Cache can use up to 7.37 GB. Also swap space = 2 GB.\n",
      "INFO 03-11 00:50:13 config.py:549] This model supports multiple tasks: {'classify', 'generate', 'score', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 03-11 00:50:13 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/home/projects/unsloth-training/models/Qwen2.5-3B-Instruct', speculative_config=None, tokenizer='/home/projects/unsloth-training/models/Qwen2.5-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/projects/unsloth-training/models/Qwen2.5-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":192}, use_cached_outputs=False, \n",
      "WARNING 03-11 00:50:13 interface.py:304] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 03-11 00:50:13 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 03-11 00:50:24 model_runner.py:1110] Starting to load model /home/projects/unsloth-training/models/Qwen2.5-3B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W311 00:50:23.882242727 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7bad27dd45e42afbde3d5d3c7d974d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-11 00:50:29 model_runner.py:1115] Loading model weights took 5.7701 GB\n",
      "INFO 03-11 00:50:29 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 03-11 00:50:31 worker.py:267] Memory profiling takes 1.45 seconds\n",
      "INFO 03-11 00:50:31 worker.py:267] the current vLLM instance can use total_gpu_memory (15.99GiB) x gpu_memory_utilization (0.83) = 13.23GiB\n",
      "INFO 03-11 00:50:31 worker.py:267] model weights take 5.77GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.05GiB; the rest of the memory reserved for KV Cache is 6.37GiB.\n",
      "INFO 03-11 00:50:31 executor_base.py:111] # cuda blocks: 11595, # CPU blocks: 3640\n",
      "INFO 03-11 00:50:31 executor_base.py:116] Maximum concurrency for 1024 tokens per request: 181.17x\n",
      "INFO 03-11 00:50:31 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:14<00:00,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-11 00:50:46 model_runner.py:1562] Graph capturing finished in 15 secs, took 0.32 GiB\n",
      "INFO 03-11 00:50:46 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 17.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n",
      "Not an error, but Unsloth cannot patch O projection layer with our manual autograd engine since either LoRA adapters\n",
      "are not enabled or a bias term (like in Qwen) is used.\n",
      "Unsloth 2025.3.8 patched 36 layers with 36 QKV layers, 0 O layers and 36 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import is_bfloat16_supported\n",
    "import torch\n",
    "max_seq_length = 1024 # ÊúÄÂ§ßÂ∫èÂàóÈïøÂ∫¶ÔºåÂèØ‰ª•Â¢ûÂä†‰ª•ÊîØÊåÅÊõ¥ÈïøÁöÑÊé®ÁêÜÊñáÊú¨\n",
    "lora_rank = 32 # LoRAÁöÑÁß©ÔºåÊï∞ÂÄºË∂äÂ§ßÊ®°ÂûãË∂äÊô∫ËÉΩ‰ΩÜËÆ≠ÁªÉÈÄüÂ∫¶Ë∂äÊÖ¢ # 16\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"/home/projects/unsloth-training/models/Qwen2.5-3B-Instruct\", # È¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑË∑ØÂæÑÔºåÊàñËÄÖhfÂêçÁß∞\n",
    "    max_seq_length = max_seq_length, # ËÆæÁΩÆÊúÄÂ§ßÂ∫èÂàóÈïøÂ∫¶\n",
    "    load_in_4bit = False, # ‰ΩøÁî®4‰ΩçÈáèÂåñÂä†ËΩΩÊ®°ÂûãÔºåÂèØ‰ª•ËäÇÁúÅÊòæÂ≠ò #True\n",
    "    fast_inference = True,# ÂêØÁî®vLLMÂä†ÈÄüÊé®ÁêÜ\n",
    "    max_lora_rank = lora_rank, # ËÆæÁΩÆLoRAÁöÑÊúÄÂ§ßÁß©\n",
    "    gpu_memory_utilization = 0.9, # GPUÂÜÖÂ≠ò‰ΩøÁî®ÁéáÔºåÂ¶ÇÊûúÂá∫Áé∞OOMÂèØ‰ª•Èôç‰ΩéÊ≠§ÂÄº\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model, # È¢ÑÂÖàÂä†ËΩΩÊ®°Âûã\n",
    "    r = lora_rank, # LoRAÁöÑÁß©ÔºåÂª∫ËÆÆÂÄº‰∏∫8, 16, 32, 64Êàñ128\n",
    "    target_modules = [\"gate_proj\", \"up_proj\", \"down_proj\",], # ÈúÄË¶ÅÂ∫îÁî®LoRAÁöÑÁõÆÊ†áÊ®°Âùó\n",
    "    lora_alpha = lora_rank, # LoRAÁº©ÊîæÂèÇÊï∞ÔºåÈÄöÂ∏∏ËÆæ‰∏∫‰∏érÁõ∏Âêå\n",
    "    use_gradient_checkpointing = \"unsloth\", # ÂêØÁî®Ê¢ØÂ∫¶Ê£ÄÊü•ÁÇπ‰ª•ÊîØÊåÅÈïøÊñáÊú¨ÂæÆË∞É\n",
    "    random_state = 3407,  # ÈöèÊú∫Êï∞ÁßçÂ≠êÔºåÁ°Æ‰øùÁªìÊûúÂèØÈáçÁé∞\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSlkA49z2xZB"
   },
   "source": [
    "### Data Prep\n",
    "<a name=\"Data\"></a>\n",
    "\n",
    "We directly leverage [@willccbb](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb) for data prep and all reward functions. You are free to create your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXk993X6C2ZZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset keys: dict_keys(['instruction', 'input', 'output'])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd4c1b6ec13405cbb407c4ca2a5a30d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ÂØºÂÖ•ÂøÖË¶ÅÁöÑÂ∫ì\n",
    "import re  # ÂØºÂÖ•Ê≠£ÂàôË°®ËææÂºèÂ∫ìÔºåÁî®‰∫éÂ≠óÁ¨¶‰∏≤ÂåπÈÖçÂíåÊèêÂèñ\n",
    "from datasets import load_dataset, Dataset  # ÂØºÂÖ•Êï∞ÊçÆÈõÜÂ§ÑÁêÜÁõ∏ÂÖ≥Â∫ì\n",
    "\n",
    "# ÂÆö‰πâÁ≥ªÁªüÊèêÁ§∫ÔºåÊåáÂÆöÂìçÂ∫îÊ†ºÂºè\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "ÂõûÁ≠îÈÅµÂæ™‰ª•‰∏ãÊ†ºÂºèÔºö\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "# ÂÆö‰πâXMLÊ†ºÂºèÁöÑÊÄùÁª¥Èìæ(Chain of Thought)Ê†ºÂºèÊ®°Êùø\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    \"\"\"\n",
    "    ‰ªéÂåÖÂê´XMLÊ†áÁ≠æÁöÑÊñáÊú¨‰∏≠ÊèêÂèñ<answer>Ê†áÁ≠æÂÜÖÁöÑÁ≠îÊ°à\n",
    "    \n",
    "    ÂèÇÊï∞:\n",
    "        text: ÂåÖÂê´XMLÊ†áÁ≠æÁöÑÊñáÊú¨\n",
    "        \n",
    "    ËøîÂõû:\n",
    "        str: ÊèêÂèñÂá∫ÁöÑÁ≠îÊ°àÊñáÊú¨ÔºåÂéªÈô§È¶ñÂ∞æÁ©∫Ê†º\n",
    "    \"\"\"\n",
    "    answer = text.split(\"<answer>\")[-1]  # ÊèêÂèñ<answer>Ê†áÁ≠æÂêéÁöÑÂÜÖÂÆπ\n",
    "    answer = answer.split(\"</answer>\")[0]  # ÊèêÂèñ</answer>Ê†áÁ≠æÂâçÁöÑÂÜÖÂÆπ\n",
    "    return answer.strip()  # ÂéªÈô§È¶ñÂ∞æÁ©∫Ê†º\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    \"\"\"\n",
    "    ‰ªéÊñáÊú¨‰∏≠ÊèêÂèñ####Ê†áËÆ∞ÂêéÁöÑÁ≠îÊ°àÔºàÁî®‰∫éÂ§ÑÁêÜÊüê‰∫õÁâπÂÆöÊ†ºÂºèÁöÑÊï∞ÊçÆÔºâ\n",
    "    \n",
    "    ÂèÇÊï∞:\n",
    "        text: ÂåÖÂê´####Ê†áËÆ∞ÁöÑÊñáÊú¨\n",
    "        \n",
    "    ËøîÂõû:\n",
    "        str | None: ÊèêÂèñÂá∫ÁöÑÁ≠îÊ°àÊñáÊú¨ÊàñNoneÔºàÂ¶ÇÊûúÊ≤°Êúâ####Ê†áËÆ∞Ôºâ\n",
    "    \"\"\"\n",
    "    if \"####\" not in text:  # Ê£ÄÊü•ÊñáÊú¨‰∏≠ÊòØÂê¶Êúâ####Ê†áËÆ∞\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()  # ÊèêÂèñ####Ê†áËÆ∞ÂêéÁöÑÂÜÖÂÆπÂπ∂ÂéªÈô§È¶ñÂ∞æÁ©∫Ê†º\n",
    "\n",
    "# Âä†ËΩΩÊï∞ÊçÆÈõÜÁöÑÂáΩÊï∞\n",
    "def get_gsm8k_questions(split = \"train\", local_path=\"/home/projects/unsloth-training/datasets/ruozhiba_R1/alpaca_output.jsonl\") -> Dataset:\n",
    "    \"\"\"\n",
    "    ‰ªéÊú¨Âú∞Ë∑ØÂæÑÂä†ËΩΩÊï∞ÊçÆÈõÜÂπ∂ËøõË°åÂ§ÑÁêÜ\n",
    "    \n",
    "    ÂèÇÊï∞:\n",
    "        split: Êï∞ÊçÆÈõÜÂàÜÂâ≤ÔºåÈªòËÆ§‰∏∫\"train\"\n",
    "        local_path: Êú¨Âú∞Êï∞ÊçÆÈõÜË∑ØÂæÑ\n",
    "        \n",
    "    ËøîÂõû:\n",
    "        Dataset: Â§ÑÁêÜÂêéÁöÑÊï∞ÊçÆÈõÜÂØπË±°\n",
    "    \"\"\"\n",
    "    # ‰ªéÊú¨Âú∞Ë∑ØÂæÑÂä†ËΩΩÊï∞ÊçÆÈõÜ\n",
    "    data = load_dataset('json', data_files=local_path, split=split)\n",
    "    \n",
    "    # Ê£ÄÊü•Êï∞ÊçÆÈõÜÁªìÊûÑÔºåÊâìÂç∞Á¨¨‰∏Ä‰∏™Ê†∑Êú¨ÁöÑÈîÆ\n",
    "    example = data[0]\n",
    "    print(\"Dataset keys:\", example.keys())\n",
    "    \n",
    "    # ÂØπÊï∞ÊçÆÈõÜËøõË°åÊò†Â∞ÑÂ§ÑÁêÜÔºåÊûÑÂª∫ÈÄÇÂêàËÆ≠ÁªÉÁöÑÊ†ºÂºè\n",
    "    data = data.map(lambda x: {\n",
    "        'prompt': [\n",
    "            # Ê∑ªÂä†Á≥ªÁªüÊèêÁ§∫‰Ωú‰∏∫Á¨¨‰∏ÄÊù°Ê∂àÊÅØ\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            # Ê∑ªÂä†Áî®Êà∑ÈóÆÈ¢òÔºå‰ºòÂÖà‰ΩøÁî®'instruction'Â≠óÊÆµÔºåÂ¶Ç‰∏çÂ≠òÂú®ÂàôÂ∞ùËØïÂÖ∂‰ªñÂ≠óÊÆµ\n",
    "            {'role': 'user', 'content': x['instruction'] if 'instruction' in x else x.get('input', '')}\n",
    "        ],\n",
    "        # ÊèêÂèñÁ≠îÊ°àÔºå‰ºòÂÖà‰ΩøÁî®'output'Â≠óÊÆµÔºåÂ¶Ç‰∏çÂ≠òÂú®ÂàôÂ∞ùËØïÂÖ∂‰ªñÂ≠óÊÆµ\n",
    "        # 'answer': extract_hash_answer(x['output'] if 'output' in x else x.get('response', x.get('answer', '')))\n",
    "    })\n",
    "    return data\n",
    "\n",
    "# Âä†ËΩΩÂπ∂Â§ÑÁêÜÊï∞ÊçÆÈõÜ\n",
    "dataset = get_gsm8k_questions(local_path=\"/home/projects/unsloth-training/datasets/ruozhiba_R1/alpaca_output.jsonl\")\n",
    "\n",
    "# ‰ª•‰∏ãÊòØÂêÑÁßçÂ•ñÂä±ÂáΩÊï∞ÁöÑÂÆö‰πâÔºåÁî®‰∫éËØÑ‰º∞Ê®°ÂûãÁîüÊàêÁöÑÂõûÁ≠îË¥®Èáè\n",
    "\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    ËØÑ‰º∞Ê®°ÂûãÂõûÁ≠îÁöÑÊ≠£Á°ÆÊÄßÔºå‰∏éÊ†áÂáÜÁ≠îÊ°àËøõË°åÊØîËæÉ\n",
    "    \n",
    "    ÂèÇÊï∞:\n",
    "        prompts: Êèê‰æõÁªôÊ®°ÂûãÁöÑÈóÆÈ¢òÂàóË°®\n",
    "        completions: Ê®°ÂûãÁîüÊàêÁöÑÂÆåÊàêÂÜÖÂÆπÂàóË°®\n",
    "        answer: Ê†áÂáÜÁ≠îÊ°àÂàóË°®\n",
    "        **kwargs: È¢ùÂ§ñÁöÑÂÖ≥ÈîÆÂ≠óÂèÇÊï∞\n",
    "        \n",
    "    ËøîÂõû:\n",
    "        list[float]: Ê≠£Á°ÆÂõûÁ≠îÂæó2.0ÂàÜÔºå‰∏çÊ≠£Á°ÆÂæó0.0ÂàÜ\n",
    "    \"\"\"\n",
    "    # ‰ªécompletions‰∏≠ÊèêÂèñÂá∫Ê®°ÂûãÁöÑÂÆûÈôÖÂõûÁ≠îÊñáÊú¨\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    # Ëé∑ÂèñÂΩìÂâçÈóÆÈ¢òÊñáÊú¨\n",
    "    q = prompts[0][-1]['content']\n",
    "    # ‰ªéÂõûÁ≠î‰∏≠ÊèêÂèñXMLÊ†áËÆ∞ÁöÑÁ≠îÊ°àÂÜÖÂÆπ\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    # ÊâìÂç∞Ë∞ÉËØï‰ø°ÊÅØÔºåÊòæÁ§∫ÈóÆÈ¢ò„ÄÅÊ≠£Á°ÆÁ≠îÊ°à„ÄÅÊ®°ÂûãÂõûÁ≠îÂíåÊèêÂèñÁöÑÁ≠îÊ°à\n",
    "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "    # ÊØîËæÉÊèêÂèñÁöÑÁ≠îÊ°à‰∏éÊ†áÂáÜÁ≠îÊ°àÔºåÊ≠£Á°ÆÂàôËøîÂõû2.0ÔºåÈîôËØØÂàôËøîÂõû0.0\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
    "\n",
    "def int_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    Ê£ÄÊü•Ê®°ÂûãÂõûÁ≠îÊòØÂê¶‰∏∫Êï¥Êï∞\n",
    "    \n",
    "    ÂèÇÊï∞:\n",
    "        completions: Ê®°ÂûãÁîüÊàêÁöÑÂÆåÊàêÂÜÖÂÆπÂàóË°®\n",
    "        **kwargs: È¢ùÂ§ñÁöÑÂÖ≥ÈîÆÂ≠óÂèÇÊï∞\n",
    "        \n",
    "    ËøîÂõû:\n",
    "        list[float]: ÂõûÁ≠î‰∏∫Êï¥Êï∞Âæó0.5ÂàÜÔºåÂê¶ÂàôÂæó0.0ÂàÜ\n",
    "    \"\"\"\n",
    "    # ‰ªécompletions‰∏≠ÊèêÂèñÂá∫Ê®°ÂûãÁöÑÂÆûÈôÖÂõûÁ≠îÊñáÊú¨\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    # ‰ªéÂõûÁ≠î‰∏≠ÊèêÂèñXMLÊ†áËÆ∞ÁöÑÁ≠îÊ°àÂÜÖÂÆπ\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    # Ê£ÄÊü•ÊèêÂèñÁöÑÁ≠îÊ°àÊòØÂê¶‰∏∫Êï∞Â≠óÂ≠óÁ¨¶‰∏≤ÔºåÊòØÂàôËøîÂõû0.5ÔºåÂê¶ÂàôËøîÂõû0.0\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
    "\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    ‰∏•Ê†ºÊ£ÄÊü•ÂõûÁ≠îÊòØÂê¶Á¨¶ÂêàÊåáÂÆöÁöÑXMLÊ†ºÂºè\n",
    "    \n",
    "    Ê†ºÂºèË¶ÅÊ±Ç: ÂøÖÈ°ª‰∏•Ê†ºÈÅµÂæ™‰ª•‰∏ãÊ†ºÂºè\n",
    "    <reasoning>\n",
    "    [Êé®ÁêÜÂÜÖÂÆπÔºåÂèØÂ§öË°å]\n",
    "    </reasoning>\n",
    "    <answer>\n",
    "    [Á≠îÊ°àÂÜÖÂÆπÔºåÂèØÂ§öË°å]\n",
    "    </answer>\n",
    "    \n",
    "    ÂèÇÊï∞:\n",
    "        completions: Ê®°ÂûãÁîüÊàêÁöÑÂÆåÊàêÂÜÖÂÆπÂàóË°®\n",
    "        **kwargs: È¢ùÂ§ñÁöÑÂÖ≥ÈîÆÂ≠óÂèÇÊï∞\n",
    "        \n",
    "    ËøîÂõû:\n",
    "        list[float]: Ê†ºÂºèÊ≠£Á°ÆÂæó0.5ÂàÜÔºåÂê¶ÂàôÂæó0.0ÂàÜ\n",
    "    \"\"\"\n",
    "    # ÂÆö‰πâ‰∏•Ê†ºÁöÑXMLÊ†ºÂºèÊ≠£ÂàôË°®ËææÂºèÊ®°ÂºèÔºåË¶ÅÊ±ÇÁ≤æÁ°ÆÂåπÈÖçÂºÄÂßãÂíåÁªìÊùüÊ†áÁ≠æ‰ª•ÂèäÊç¢Ë°å\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    # ‰ªécompletions‰∏≠ÊèêÂèñÂá∫Ê®°ÂûãÁöÑÂÆûÈôÖÂõûÁ≠îÊñáÊú¨\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    # ‰ΩøÁî®Ê≠£ÂàôË°®ËææÂºèÊ£ÄÊü•Ê†ºÂºèÊòØÂê¶ÂåπÈÖç\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    # ÂåπÈÖçÊàêÂäüËøîÂõû0.5ÔºåÂê¶ÂàôËøîÂõû0.0\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    ÂÆΩÊùæÊ£ÄÊü•ÂõûÁ≠îÊòØÂê¶Á¨¶ÂêàXMLÊ†ºÂºè\n",
    "    \n",
    "    Ê†ºÂºèË¶ÅÊ±Ç: Âè™Ë¶ÅÂåÖÂê´<reasoning>Ê†áÁ≠æÂíå<answer>Ê†áÁ≠æÂç≥ÂèØÔºå‰∏ç‰∏•Ê†ºË¶ÅÊ±ÇÊç¢Ë°åÂíåÈ°∫Â∫è\n",
    "    \n",
    "    ÂèÇÊï∞:\n",
    "        completions: Ê®°ÂûãÁîüÊàêÁöÑÂÆåÊàêÂÜÖÂÆπÂàóË°®\n",
    "        **kwargs: È¢ùÂ§ñÁöÑÂÖ≥ÈîÆÂ≠óÂèÇÊï∞\n",
    "        \n",
    "    ËøîÂõû:\n",
    "        list[float]: Ê†ºÂºèÊ≠£Á°ÆÂæó0.5ÂàÜÔºåÂê¶ÂàôÂæó0.0ÂàÜ\n",
    "    \"\"\"\n",
    "    # ÂÆö‰πâÂÆΩÊùæÁöÑXMLÊ†ºÂºèÊ≠£ÂàôË°®ËææÂºèÊ®°ÂºèÔºåÂè™Ë¶ÅÊ±ÇÂåÖÂê´Ê†áÁ≠æÔºå‰∏çÈôêÂà∂Êç¢Ë°åÊ†ºÂºè\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    # ‰ªécompletions‰∏≠ÊèêÂèñÂá∫Ê®°ÂûãÁöÑÂÆûÈôÖÂõûÁ≠îÊñáÊú¨\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    # ‰ΩøÁî®Ê≠£ÂàôË°®ËææÂºèÊ£ÄÊü•Ê†ºÂºèÊòØÂê¶ÂåπÈÖç\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    # ÂåπÈÖçÊàêÂäüËøîÂõû0.5ÔºåÂê¶ÂàôËøîÂõû0.0\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def count_xml(text) -> float:\n",
    "    \"\"\"\n",
    "    ËÆ°ÁÆóXMLÊ†áÁ≠æÁöÑÊ≠£Á°Æ‰ΩøÁî®ÊÉÖÂÜµÔºåÂπ∂Áªô‰∫àÂàÜÊï∞Â•ñÂä±\n",
    "    \n",
    "    ÂèÇÊï∞:\n",
    "        text: ÈúÄË¶ÅËØÑ‰º∞ÁöÑÊñáÊú¨\n",
    "        \n",
    "    ËøîÂõû:\n",
    "        float: Ê†πÊçÆXMLÊ†áÁ≠æÁöÑÊ≠£Á°Æ‰ΩøÁî®ÊÉÖÂÜµËÆ°ÁÆóÁöÑÂàÜÊï∞(ÊúÄÈ´ò0.5ÂàÜ)\n",
    "    \"\"\"\n",
    "    count = 0.0\n",
    "    # Ê£ÄÊü•ÊòØÂê¶Ê≠£Á°Æ‰ΩøÁî®<reasoning>Ê†áÁ≠æÔºåÊ≠£Á°ÆÂæó0.125ÂàÜ\n",
    "    if text.count(\"<reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    # Ê£ÄÊü•ÊòØÂê¶Ê≠£Á°Æ‰ΩøÁî®</reasoning>Ê†áÁ≠æÔºåÊ≠£Á°ÆÂæó0.125ÂàÜ\n",
    "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    # Ê£ÄÊü•ÊòØÂê¶Ê≠£Á°Æ‰ΩøÁî®<answer>Ê†áÁ≠æÔºåÊ≠£Á°ÆÂæó0.125ÂàÜ\n",
    "    # ÂêåÊó∂ÂáèÂéª</answer>ÂêéÂ§ö‰ΩôÂÜÖÂÆπÁöÑÊÉ©ÁΩöÂàÜ\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001  # ÂØπÂ§ö‰ΩôÂÜÖÂÆπËøõË°åÊÉ©ÁΩö\n",
    "    # Ê£ÄÊü•ÊòØÂê¶Ê≠£Á°Æ‰ΩøÁî®</answer>Ê†áÁ≠æÔºåÊ≠£Á°ÆÂæó0.125ÂàÜ\n",
    "    # ÂêåÊó∂ÂáèÂéª</answer>ÂêéÂ§ö‰ΩôÂÜÖÂÆπÁöÑÊÉ©ÁΩöÂàÜ\n",
    "    if text.count(\"\\n</answer>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001  # ÂØπÂ§ö‰ΩôÂÜÖÂÆπËøõË°åÊÉ©ÁΩö\n",
    "    return count\n",
    "\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    ËØÑ‰º∞ÂõûÁ≠î‰∏≠XMLÊ†áÁ≠æÁöÑÊ≠£Á°Æ‰ΩøÁî®ÊÉÖÂÜµ\n",
    "    \n",
    "    ÂèÇÊï∞:\n",
    "        completions: Ê®°ÂûãÁîüÊàêÁöÑÂÆåÊàêÂÜÖÂÆπÂàóË°®\n",
    "        **kwargs: È¢ùÂ§ñÁöÑÂÖ≥ÈîÆÂ≠óÂèÇÊï∞\n",
    "        \n",
    "    ËøîÂõû:\n",
    "        list[float]: ÊØè‰∏™ÂõûÁ≠îÁöÑXMLÊ†ºÂºèËØÑÂàÜ(0-0.5‰πãÈó¥)\n",
    "    \"\"\"\n",
    "    # ‰ªécompletions‰∏≠ÊèêÂèñÂá∫Ê®°ÂûãÁöÑÂÆûÈôÖÂõûÁ≠îÊñáÊú¨\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    # ÂØπÊØè‰∏™ÂõûÁ≠îÊñáÊú¨ËØÑ‰º∞XMLÊ†áÁ≠æ‰ΩøÁî®ÊÉÖÂÜµ\n",
    "    return [count_xml(c) for c in contents]\n",
    "\n",
    "# Ê∑ªÂä†‰∏Ä‰∏™Ê£ÄÊü• ÊÄùËÄÉËøáÁ®ãÁöÑÊñáÊú¨ÂíåÊúÄÂêéÁöÑÊñáÊú¨Áõ∏‰ººÂ∫¶ÁöÑÂáΩÊï∞ÔºåÁ°Æ‰øùÁªìÊûú‰∏ç‰ºöÂíåÊÄùËÄÉËøáÁ®ãÁõ∏Âêå\n",
    "\n",
    "def reasoning_length_reward_func(completions, max_length=1024, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    Â•ñÂä±Êé®ÁêÜÊñáÊú¨ÈïøÂ∫¶ÔºåÊñáÊú¨Ë∂äÈïøÂ•ñÂä±Ë∂äÈ´òÔºåÊúÄÈ´ò5ÂàÜ\n",
    "    \n",
    "    Â•ñÂä±‰∏éÊé®ÁêÜÊñáÊú¨ÁöÑÈïøÂ∫¶ÂëàÁ∫øÊÄßÂÖ≥Á≥ªÔºåÁõ¥Âà∞ËææÂà∞max_length‰∏™Â≠óÁ¨¶Ôºå\n",
    "    ‰πãÂêéÂ∞ÜÁªô‰∫àÊª°ÂàÜ5.0ÂàÜ„ÄÇ\n",
    "    \n",
    "    ÂèÇÊï∞:\n",
    "        completions: Ê®°ÂûãÁîüÊàêÁöÑÂÆåÊàêÂÜÖÂÆπÂàóË°®\n",
    "        max_length: Ëé∑ÂæóÊúÄÈ´òÂ•ñÂä±ÁöÑÂ≠óÁ¨¶Êï∞ÔºàÈªòËÆ§Ôºö500Ôºâ\n",
    "        **kwargs: È¢ùÂ§ñÁöÑÂÖ≥ÈîÆÂ≠óÂèÇÊï∞\n",
    "        \n",
    "    ËøîÂõû:\n",
    "        list[float]: Âü∫‰∫éÊé®ÁêÜÊñáÊú¨ÈïøÂ∫¶ÁöÑÂ•ñÂä±Ôºà0.0Âà∞5.0‰πãÈó¥Ôºâ\n",
    "    \"\"\"\n",
    "    # ‰ªécompletions‰∏≠ÊèêÂèñÂá∫Ê®°ÂûãÁöÑÂÆûÈôÖÂõûÁ≠îÊñáÊú¨\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    rewards = []\n",
    "    \n",
    "    for response in responses:\n",
    "        try:\n",
    "            # ÊèêÂèñ<reasoning>Âíå</reasoning>Ê†áÁ≠æ‰πãÈó¥ÁöÑÊñáÊú¨\n",
    "            reasoning_match = re.search(r\"<reasoning>(.*?)</reasoning>\", response, re.DOTALL)\n",
    "            \n",
    "            if reasoning_match:\n",
    "                reasoning_text = reasoning_match.group(1).strip()\n",
    "                text_length = len(reasoning_text)\n",
    "                \n",
    "                # Á∫øÊÄßÁº©ÊîæÔºöreward = 5.0 * min(1.0, text_length / max_length)\n",
    "                # Âú®max_lengthÂ≠óÁ¨¶ÊàñÊõ¥Â§öÊó∂Áªô‰∫àÊª°ÂàÜ5.0ÂàÜ\n",
    "                reward = 2 * min(0.0, text_length / max_length)\n",
    "                \n",
    "                # ÊâìÂç∞Ë∞ÉËØï‰ø°ÊÅØ\n",
    "                # print(f\"Êé®ÁêÜÈïøÂ∫¶: {text_length} Â≠óÁ¨¶, Â•ñÂä±: {reward:.2f}\")\n",
    "                \n",
    "                rewards.append(reward)\n",
    "            else:\n",
    "                # Êú™ÊâæÂà∞reasoningÊ†áÁ≠æ\n",
    "                rewards.append(0.0)\n",
    "        except Exception as e:\n",
    "            # Â§ÑÁêÜËøáÁ®ã‰∏≠Âá∫Èîô\n",
    "            rewards.append(0.0)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tze5NF5523DB"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "\n",
    "Now set up GRPO Trainer and all configurations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptqkXK2D4d6p",
    "outputId": "344b54e8-5a9c-4676-bfc0-23f8b5cb7426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 1 to the `num_generations` of 6\n"
     ]
    }
   ],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    use_vllm = True, # ‰ΩøÁî®vLLMËøõË°åÊé®ÁêÜÂä†ÈÄüÔºåÊòæËëóÊèêÈ´òÁîüÊàêÂíåËØÑ‰º∞ÈÄüÂ∫¶\n",
    "    learning_rate = 5e-6, # Â≠¶‰π†ÁéáËÆæÁΩÆ‰∏∫5e-6ÔºåÈÄÇÂêàLoRAÂæÆË∞ÉÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã\n",
    "    adam_beta1 = 0.9, # Adam‰ºòÂåñÂô®ÁöÑbeta1ÂèÇÊï∞ÔºåÊéßÂà∂‰∏ÄÈò∂Áü©‰º∞ËÆ°ÁöÑÊåáÊï∞Ë°∞ÂáèÁéá\n",
    "    adam_beta2 = 0.99, # Adam‰ºòÂåñÂô®ÁöÑbeta2ÂèÇÊï∞ÔºåÊéßÂà∂‰∫åÈò∂Áü©‰º∞ËÆ°ÁöÑÊåáÊï∞Ë°∞ÂáèÁéáÔºå\n",
    "    weight_decay = 0.1, # ÊùÉÈáçË°∞ÂáèÁ≥ªÊï∞ÔºåÁî®‰∫éL2Ê≠£ÂàôÂåñÔºåÈò≤Ê≠¢ËøáÊãüÂêà\n",
    "    warmup_ratio = 0.1, # Â≠¶‰π†ÁéáÈ¢ÑÁÉ≠ÊØî‰æãÔºåÂú®ËÆ≠ÁªÉÂàùÊúüÈÄêÊ∏êÂ¢ûÂä†Â≠¶‰π†ÁéáÔºåÂç†ÊÄªËÆ≠ÁªÉÊ≠•Êï∞ÁöÑ10%\n",
    "    lr_scheduler_type = \"cosine\", # Â≠¶‰π†ÁéáË∞ÉÂ∫¶Âô®Á±ªÂûãÔºå‰ΩôÂº¶ÈÄÄÁÅ´ÂèØ‰ª•Âπ≥ÊªëÂú∞Èôç‰ΩéÂ≠¶‰π†Áéá\n",
    "    optim = \"paged_adamw_8bit\", # ‰ºòÂåñÂô®Á±ªÂûãÔºå‰ΩøÁî®8‰ΩçÈáèÂåñÁöÑAdam‰ºòÂåñÂô®ÂáèÂ∞ëÂÜÖÂ≠òÂç†Áî®\n",
    "    logging_steps = 1, # ÊØèÊ≠•ËÆ≠ÁªÉÂêéËÆ∞ÂΩïÊó•ÂøóÔºå‰æø‰∫éÂÆûÊó∂ÁõëÊéßËÆ≠ÁªÉÁä∂ÊÄÅ\n",
    "    bf16 = is_bfloat16_supported(), # Â¶ÇÊûúÊîØÊåÅbfloat16ÂàôÂêØÁî®ÔºåÊèêÈ´òËÆ≠ÁªÉÈÄüÂ∫¶Âπ∂ÂáèÂ∞ëÂÜÖÂ≠ò‰ΩøÁî®\n",
    "    fp16 = not is_bfloat16_supported(), # ÂΩì‰∏çÊîØÊåÅbfloat16Êó∂Ôºå‰ΩøÁî®fp16Ê∑∑ÂêàÁ≤æÂ∫¶ËÆ≠ÁªÉ\n",
    "    per_device_train_batch_size = 1, # ÊØè‰∏™ËÆæÂ§áÁöÑËÆ≠ÁªÉÊâπÈáèÂ§ßÂ∞èÔºåGRPO‰ºöËá™Âä®Ë∞ÉÊï¥‰∏∫ÂåπÈÖçnum_generations\n",
    "    gradient_accumulation_steps = 4, # Ê¢ØÂ∫¶Á¥ØÁßØÊ≠•Êï∞Ôºå1Ë°®Á§∫ÊØèÊ≠•Êõ¥Êñ∞‰∏ÄÊ¨°Ê®°ÂûãÂèÇÊï∞ÔºàÂèØÂ¢ûÂä†Âà∞4‰ª•Á®≥ÂÆöËÆ≠ÁªÉÔºâ #1\n",
    "    num_generations = 6, # ÊØèÊ¨°ËØÑ‰º∞ÁîüÊàêÁöÑÊ†∑Êú¨Êï∞ÈáèÔºåÂΩ±ÂìçÂ§öÊ†∑ÊÄßÂíåÂÜÖÂ≠ò‰ΩøÁî®\n",
    "    max_prompt_length = 1024, # ËæìÂÖ•ÊèêÁ§∫ÁöÑÊúÄÂ§ßÈïøÂ∫¶ÔºàtokenÊï∞ÔºâÔºåË∂ÖËøá‰ºöË¢´Êà™Êñ≠\n",
    "    max_completion_length = 1024,  # ÁîüÊàêÊñáÊú¨ÁöÑÊúÄÂ§ßÈïøÂ∫¶ÔºàtokenÊï∞ÔºâÔºåÈôêÂà∂Ê®°ÂûãËæìÂá∫ÈïøÂ∫¶\n",
    "    # num_train_epochs = 1, # ÂÆåÊï¥ËÆ≠ÁªÉÁöÑËΩÆÊï∞ÔºåÂΩìÂâçË¢´Ê≥®ÈáäÔºå‰ΩøÁî®max_stepsÊéßÂà∂ËÆ≠ÁªÉÈïøÂ∫¶\n",
    "    max_steps = 100, # ËÆ≠ÁªÉÁöÑÊúÄÂ§ßÊ≠•Êï∞Ôºå100Ê≠•‰∏∫Âø´ÈÄüÂÆûÈ™åËÆæÁΩÆ\n",
    "    save_steps = 250, # ÊØè250Ê≠•‰øùÂ≠ò‰∏ÄÊ¨°Ê£ÄÊü•ÁÇπÔºåÁî®‰∫éÊÅ¢Â§çËÆ≠ÁªÉÊàñËØÑ‰º∞\n",
    "    max_grad_norm = 0.1, # Ê¢ØÂ∫¶Ë£ÅÂâ™ÈòàÂÄºÔºåÈò≤Ê≠¢Ê¢ØÂ∫¶ÁàÜÁÇ∏\n",
    "    report_to = \"none\", # ËÆ≠ÁªÉËøáÁ®ãÊä•ÂëäÂ∑•ÂÖ∑Ôºå\"none\"Ë°®Á§∫‰∏ç‰ΩøÁî®Â§ñÈÉ®Â∑•ÂÖ∑ÔºåÂèØÈÄâÁî®W&BÁ≠â\n",
    "    output_dir = \"outputs\", # ËæìÂá∫ÁõÆÂΩïÔºåÁî®‰∫é‰øùÂ≠òÊ®°Âûã„ÄÅÊó•ÂøóÂíåÊ£ÄÊü•ÁÇπ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8egDqHG3GH0"
   },
   "source": [
    "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
    "\n",
    "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
    "\n",
    "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
    "|------|---------------|-----------|------------|-------------------|----------|\n",
    "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
    "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
    "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vzOuSVCL_GA9",
    "outputId": "0fe20ec2-ea69-486a-e2df-4685bd390413"
   },
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model, ## ‰º†ÂÖ•È¢ÑÂä†ËΩΩÁöÑÊ®°ÂûãÔºå‰πãÂâçÂ∑≤‰ΩøÁî®LoRAÊñπÊ≥ïÂáÜÂ§áÂ•Ω\n",
    "    processing_class = tokenizer, # ‰º†ÂÖ•ÂàÜËØçÂô®ÔºåÁî®‰∫éÊñáÊú¨Â§ÑÁêÜÂíåÁºñÁ†Å\n",
    "    reward_funcs = [\n",
    "        xmlcount_reward_func, # Ê£ÄÊü•XMLÊ†áÁ≠æÁöÑÊ≠£Á°Æ‰ΩøÁî®Ôºà<reasoning>Âíå<answer>Ê†áÁ≠æÔºâÂπ∂Áªô‰∫àÂ•ñÂä±\n",
    "        soft_format_reward_func, # ÂÆΩÊùæÂú∞Ê£ÄÊü•ÂõûÁ≠îÊòØÂê¶Á¨¶ÂêàXMLÊ†ºÂºèÔºåÂè™Ë¶ÅÂåÖÂê´Ê†áÁ≠æÂç≥ÂèØ\n",
    "        strict_format_reward_func, # ‰∏•Ê†ºÊ£ÄÊü•ÂõûÁ≠îÊòØÂê¶Á¨¶ÂêàXMLÊ†ºÂºèÔºåÂåÖÊã¨Êç¢Ë°åÂíåÈ°∫Â∫è\n",
    "        int_reward_func, # Ê£ÄÊü•ÂõûÁ≠î‰∏≠ÁöÑÁ≠îÊ°àÊòØÂê¶‰∏∫Êï¥Êï∞Âπ∂Áªô‰∫àÂ•ñÂä±\n",
    "        # correctness_reward_func, # ‰∏éÊ†áÂáÜÁ≠îÊ°àËøõË°åÊØîËæÉÔºåËØÑ‰º∞ÂõûÁ≠îÁöÑÊ≠£Á°ÆÊÄß\n",
    "        reasoning_length_reward_func,\n",
    "    ],\n",
    "    # ËÆ≠ÁªÉÂèÇÊï∞ÈÖçÁΩÆÔºå‰πãÂâçÂ∑≤ÂÆö‰πâ\n",
    "    args = training_args,\n",
    "    # ËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºåÂ∑≤È¢ÑÂ§ÑÁêÜÊàêÂåÖÂê´promptÂíåanswerÁöÑÊ†ºÂºè\n",
    "    train_dataset = dataset,\n",
    ")\n",
    "\n",
    "# ÂêØÂä®ËÆ≠ÁªÉËøáÁ®ã\n",
    "# Ê®°Âûã‰ºöÈÄöËøáÂº∫ÂåñÂ≠¶‰π†Á≠ñÁï•ÔºåÊ†πÊçÆ‰∏äËø∞Â•ñÂä±ÂáΩÊï∞ÂèçÈ¶à‰∏çÊñ≠Ë∞ÉÊï¥ÁîüÊàêÁ≠ñÁï•\n",
    "# ÁõÆÊ†áÊòØÂ≠¶‰π†ÁîüÊàêÁ¨¶ÂêàXMLÊ†ºÂºèÁöÑÂõûÁ≠îÔºåÂåÖÂê´Êé®ÁêÜËøáÁ®ãÂíåÊúÄÁªàÁ≠îÊ°à\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbdvvDCbLrLe"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "\n",
    "Now let's try the model we just trained! First, let's first try the model without any GRPO trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "id": "urQZvMTaLrrQ",
    "outputId": "b2ab3c22-cfd8-43b1-b173-4b780cd3fed0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template([\n\u001b[1;32m      2\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mÁªôÊàëÂàõÂª∫‰∏Ä‰∏™ÊúâÂÖ≥‰∫éÈ∏üÁöÑ glsl ‰ª£Á†Å\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      3\u001b[0m ], tokenize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SamplingParams\n\u001b[1;32m      6\u001b[0m sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams(\n\u001b[1;32m      7\u001b[0m     temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m,\n\u001b[1;32m      8\u001b[0m     top_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.95\u001b[39m,\n\u001b[1;32m      9\u001b[0m     max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"user\", \"content\" : \"ÁªôÊàëÂàõÂª∫‰∏Ä‰∏™ÊúâÂÖ≥‰∫éÈ∏üÁöÑ glsl ‰ª£Á†Å\"},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    [text],\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = None,\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "È¶ñÂÖàÔºåÊàë‰ª¨Êù•ÂàÜÊûêÁªôÂÆöÊñπÁ®ã \\(\\sqrt{a} - \\sqrt{a} + x = x\\)„ÄÇ\n",
      "\n",
      "Ëøô‰∏™ÊñπÁ®ãÂèØ‰ª•ÁÆÄÂåñ‰∏∫Ôºö\\(0 + x = x\\) ÊàñËÄÖÁÆÄÂÜô‰∏∫ \\(x = x\\)„ÄÇËøôÁúãËµ∑Êù•ÂÉèÊòØ‰∏Ä‰∏™ÊÅíÁ≠âÂºèÔºåÂÆÉÂØπ‰∫éÊâÄÊúâÂÆö‰πâÂüüÂÜÖÁöÑ \\(x\\) ÈÉΩÊàêÁ´ãÔºåÂõ†Ê≠§Âú®ÊüêÁßçÊÑè‰πâ‰∏äÔºåËøôË°®Á§∫ÊñπÁ®ãÂØπ‰∫éÊâÄÊúâ \\(x\\) ÈÉΩÊòØÊ≠£Á°ÆÁöÑ„ÄÇ‰ΩÜÊòØÔºåÊàë‰ª¨ÈúÄË¶ÅËÄÉËôëÂà∞ÂéüÂßãÊñπÁ®ã‰∏≠ \\(a > 1\\) ÁöÑÊù°‰ª∂„ÄÇÂÆûÈôÖ‰∏äÔºåÂéüÂßãÊñπÁ®ãÁÆÄÂåñÂêéÁöÑÁ≠âÂºè \\(x = x\\) Âπ∂Ê≤°ÊúâÊèê‰æõÂÖ≥‰∫é \\(x\\) ÁöÑÈ¢ùÂ§ñÈôêÂà∂ÔºåÊâÄ‰ª•ÂÆÉÂú® \\(x\\) ÁöÑ‰ªª‰ΩïÂÄº‰∏äÈÉΩÊàêÁ´ã„ÄÇ\n",
      "\n",
      "ÁªôÂÆö \\(a > 1\\) Âπ∂‰∏ç‰ºöÂΩ±ÂìçÂà∞ \\(x = x\\) ÁöÑÁªìËÆ∫ÔºåÂõ†‰∏∫Âú® \\(x = x\\) ÁöÑÊÉÖÂÜµ‰∏ãÔºå\\(x\\) ÂèØ‰ª•ÊòØ‰ªª‰ΩïÂÆûÊï∞„ÄÇÂõ†Ê≠§ÔºåÊ≤°ÊúâÁâπÂÆöÁöÑ \\(x\\) ÂÄºË¢´ÊéíÈô§Âú®ÂèØËÉΩÁöÑËß£‰πãÂ§ñÔºåËØ¥ÊòéËøô‰∏™ÊñπÁ®ãÁöÑÂÆûÊï∞Ëß£ÁöÑÈõÜÂêàÊòØÊó†ÈôêÁöÑÔºåÂÆÉÂåÖÂê´‰∫ÜÊâÄÊúâÁöÑÂÆûÊï∞„ÄÇ\n",
      "\n",
      "ÊâÄ‰ª•ÔºåÂ¶ÇÊûúÊñπÁ®ã \\(\\sqrt{a} - \\sqrt{a} + x = x\\) ÁöÑËß£ÊòØÊâÄÊúâ \\(x\\) ÁöÑÂÆûÊï∞ÔºåÈÇ£‰πàËß£ÁöÑÂíå‰æùÁÑ∂ÊòØÊâÄÊúâÂÆûÊï∞ÁöÑÂíåÔºåËÄåÂú®Êï∞Â≠¶‰∏≠ÔºåÊâÄÊúâÂÆûÊï∞ÁöÑÂíåÂπ∂‰∏çÂ≠òÂú®‰∏Ä‰∏™ÂÖ∑‰ΩìÁöÑÊï∞ÂÄºÔºåÂÆÉÊòØÊú™ÂÆö‰πâÁöÑ„ÄÇ\n",
      "\n",
      "ÊÄªÁªìÊù•ËØ¥ÔºåÂΩì \\(a > 1\\) Êó∂ÔºåÊñπÁ®ã \\(\\sqrt{a} - \\sqrt{a} + x = x\\) ÁöÑÂÆûÊï∞Ëß£‰πãÂíå‰∏∫Êú™ÂÆö‰πâ„ÄÇ\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXfSTmXFLyIE"
   },
   "source": [
    "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XOed9DauLydR"
   },
   "outputs": [],
   "source": [
    "model.save_lora(\"grpo_saved_lora\") # ‰øùÂ≠òLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45U-8F0nL1Uf"
   },
   "source": [
    "Now we load the LoRA and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "id": "__w_7GamL1m1",
    "outputId": "2402a0e9-6ec0-4f65-9921-311888040df9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.81s/it, est. speed input: 12.49 toks/s, output: 67.43 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<reasoning>\\nË¶ÅËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÈ¶ñÂÖàÈúÄË¶ÅÁêÜËß£ÂíåÁÆÄÂåñÁªôÂÆöÁöÑÊñπÁ®ã \\\\(\\\\sqrt{a} - \\\\sqrt{a} + x = x\\\\)„ÄÇÊàë‰ª¨ÂèØ‰ª•Ê≥®ÊÑèÂà∞ \\\\(\\\\sqrt{a} - \\\\sqrt{a} = 0\\\\)ÔºåËøôÊÑèÂë≥ÁùÄÂéüÊñπÁ®ãÁÆÄÂåñ‰∏∫ \\\\(0 + x = x\\\\)ÔºåÂç≥ \\\\(x = x\\\\)„ÄÇËøôÊÑèÂë≥ÁùÄÂéüÊñπÁ®ãÂØπ‰∫éÊâÄÊúâÂÆûÊï∞ \\\\(x\\\\) ÈÉΩÊàêÁ´ãÔºåËøôÊÑèÂë≥ÁùÄ‰ªª‰ΩïÂÆûÊï∞ÈÉΩÊòØËøô‰∏™ÊñπÁ®ãÁöÑËß£„ÄÇÁî±‰∫éËØ•ÊñπÁ®ãÂØπ‰∫éÊâÄÊúâ \\\\(x\\\\) ÈÉΩÊòØÊàêÁ´ãÁöÑÔºåÊâÄ‰ª•ÂÆûÊï∞Ëß£ÁöÑÈõÜÂêàÂåÖÂê´‰∫ÜÊâÄÊúâÂÆûÊï∞„ÄÇÂ¶ÇÊûúË¶ÅÊ±ÇËß£ÁöÑÂÆûÊï∞Ëß£‰πãÂíåÔºåÁî±‰∫éÂÆûÊï∞ÈõÜÂêàÂåÖÂê´ÊâÄÊúâÁöÑÂÆûÊï∞ÔºåËÄåÂÆûÊï∞ÈõÜÂêàÊ≤°Êúâ‰∏Ä‰∏™Á°ÆÂÆöÁöÑÂíåÔºåÂõ†Ê≠§Ëß£ÁöÑÂíåÂ∞ÜÊ≤°Êúâ‰∏Ä‰∏™ÊòéÁ°ÆÁöÑÊï∞ÂÄº„ÄÇ‰ΩÜËã•‰∏•Ê†ºÊåâÁÖßÈ¢òÊÑèÊ±ÇÊâÄÊúâÂèØËÉΩÁöÑÂÆûÊï∞Ëß£‰πãÂíåÔºåÁªìÊûúÂ∞Ü‰∏∫ÊâÄÊúâÂÆûÊï∞ÁöÑÂπ≥ÂùáÂÄºÔºåËøôÂú®Áé∞ÂÆû‰∏≠ÊòØ‰∏çÂ≠òÂú®ÁöÑ„ÄÇ‰ΩÜÊòØÔºåÊ†πÊçÆÈ¢òÁõÆÁöÑÈÄªËæëÔºåÂÆûÈôÖ‰∏äÊØè‰∏™ÂÆûÊï∞Ëß£Áõ∏Âä†ÁöÑÁªìÊûúËøòÊòØ‰øùÊåÅ‰∏çÂèòÔºå‰πüÂ∞±ÊòØËØ¥ÔºåÂéüÊñπÁ®ãÁªôÂÆöÊù°‰ª∂‰∏çÂΩ±ÂìçÁªìÊûúÔºåËß£‰ªçÁÑ∂ÊòØÊâÄÊúâÂÆûÊï∞Ôºå‰∏îÊ≤°Êúâ‰∏Ä‰∏™ÂÖ∑‰ΩìÁöÑÂÆûÊï∞Âíå„ÄÇËÄÉËôëÂà∞‰ª•‰∏äÊÉÖÂÜµÔºåÊàë‰ª¨ÂèØ‰ª•ÂæóÂá∫ÁªìËÆ∫ÂÆûÊï∞Ëß£‰πãÂíå‰∏∫0ÔºåÁî±‰∫éÈ¢òÁõÆÊ≤°ÊúâÊòéÁ°ÆÈôêÂÆö \\\\(x\\\\) ÁöÑËåÉÂõ¥ÔºåÂÅáËÆæ \\\\(x\\\\) ÁöÑÂèñÂÄº‰ªéË¥üÊó†Á©∑Â§ßÂà∞Ê≠£Êó†Á©∑Â§ßÔºåÂÆûÊï∞Ëß£‰πãÂíåÂèØ‰ª•ÁêÜËß£‰∏∫ÊâÄÊúâ \\\\(x\\\\) ÁöÑÂèñÂÄºÁõ∏Âä†‰∏∫0ÔºàÂç≥‰∏≠ÊÄßÂÄºÔºâ„ÄÇ\\n</reasoning>\\n<answer>\\n0\\n</answer>\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
    "    {\"role\" : \"user\", \"content\" : \"Â¶ÇÊûú a > 1ÔºåÂàô ‚àöÔ∏Å a‚àí‚àö a + x = x ÁöÑÂÆûÊï∞Ëß£‰πãÂíåÁ≠â‰∫é?\"},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SP998x4tMRFE",
    "outputId": "13ea89c4-8b26-4ee7-9fec-9ed3441eaa53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<reasoning>\n",
      "Ë¶ÅËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÈ¶ñÂÖàÈúÄË¶ÅÁêÜËß£ÂíåÁÆÄÂåñÁªôÂÆöÁöÑÊñπÁ®ã \\(\\sqrt{a} - \\sqrt{a} + x = x\\)„ÄÇÊàë‰ª¨ÂèØ‰ª•Ê≥®ÊÑèÂà∞ \\(\\sqrt{a} - \\sqrt{a} = 0\\)ÔºåËøôÊÑèÂë≥ÁùÄÂéüÊñπÁ®ãÁÆÄÂåñ‰∏∫ \\(0 + x = x\\)ÔºåÂç≥ \\(x = x\\)„ÄÇËøôÊÑèÂë≥ÁùÄÂéüÊñπÁ®ãÂØπ‰∫éÊâÄÊúâÂÆûÊï∞ \\(x\\) ÈÉΩÊàêÁ´ãÔºåËøôÊÑèÂë≥ÁùÄ‰ªª‰ΩïÂÆûÊï∞ÈÉΩÊòØËøô‰∏™ÊñπÁ®ãÁöÑËß£„ÄÇÁî±‰∫éËØ•ÊñπÁ®ãÂØπ‰∫éÊâÄÊúâ \\(x\\) ÈÉΩÊòØÊàêÁ´ãÁöÑÔºåÊâÄ‰ª•ÂÆûÊï∞Ëß£ÁöÑÈõÜÂêàÂåÖÂê´‰∫ÜÊâÄÊúâÂÆûÊï∞„ÄÇÂ¶ÇÊûúË¶ÅÊ±ÇËß£ÁöÑÂÆûÊï∞Ëß£‰πãÂíåÔºåÁî±‰∫éÂÆûÊï∞ÈõÜÂêàÂåÖÂê´ÊâÄÊúâÁöÑÂÆûÊï∞ÔºåËÄåÂÆûÊï∞ÈõÜÂêàÊ≤°Êúâ‰∏Ä‰∏™Á°ÆÂÆöÁöÑÂíåÔºåÂõ†Ê≠§Ëß£ÁöÑÂíåÂ∞ÜÊ≤°Êúâ‰∏Ä‰∏™ÊòéÁ°ÆÁöÑÊï∞ÂÄº„ÄÇ‰ΩÜËã•‰∏•Ê†ºÊåâÁÖßÈ¢òÊÑèÊ±ÇÊâÄÊúâÂèØËÉΩÁöÑÂÆûÊï∞Ëß£‰πãÂíåÔºåÁªìÊûúÂ∞Ü‰∏∫ÊâÄÊúâÂÆûÊï∞ÁöÑÂπ≥ÂùáÂÄºÔºåËøôÂú®Áé∞ÂÆû‰∏≠ÊòØ‰∏çÂ≠òÂú®ÁöÑ„ÄÇ‰ΩÜÊòØÔºåÊ†πÊçÆÈ¢òÁõÆÁöÑÈÄªËæëÔºåÂÆûÈôÖ‰∏äÊØè‰∏™ÂÆûÊï∞Ëß£Áõ∏Âä†ÁöÑÁªìÊûúËøòÊòØ‰øùÊåÅ‰∏çÂèòÔºå‰πüÂ∞±ÊòØËØ¥ÔºåÂéüÊñπÁ®ãÁªôÂÆöÊù°‰ª∂‰∏çÂΩ±ÂìçÁªìÊûúÔºåËß£‰ªçÁÑ∂ÊòØÊâÄÊúâÂÆûÊï∞Ôºå‰∏îÊ≤°Êúâ‰∏Ä‰∏™ÂÖ∑‰ΩìÁöÑÂÆûÊï∞Âíå„ÄÇËÄÉËôëÂà∞‰ª•‰∏äÊÉÖÂÜµÔºåÊàë‰ª¨ÂèØ‰ª•ÂæóÂá∫ÁªìËÆ∫ÂÆûÊï∞Ëß£‰πãÂíå‰∏∫0ÔºåÁî±‰∫éÈ¢òÁõÆÊ≤°ÊúâÊòéÁ°ÆÈôêÂÆö \\(x\\) ÁöÑËåÉÂõ¥ÔºåÂÅáËÆæ \\(x\\) ÁöÑÂèñÂÄº‰ªéË¥üÊó†Á©∑Â§ßÂà∞Ê≠£Êó†Á©∑Â§ßÔºåÂÆûÊï∞Ëß£‰πãÂíåÂèØ‰ª•ÁêÜËß£‰∏∫ÊâÄÊúâ \\(x\\) ÁöÑÂèñÂÄºÁõ∏Âä†‰∏∫0ÔºàÂç≥‰∏≠ÊÄßÂÄºÔºâ„ÄÇ\n",
      "</reasoning>\n",
      "<answer>\n",
      "0\n",
      "</answer>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gzZDHijL_3l"
   },
   "source": [
    "Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTTciyNnMCI2"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYVi3GLfMCg4"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fT7HEOzDMDcI"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
    "\n",
    "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-p9BiitMF63"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvsXeMAQPjQR"
   },
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n",
    "\n",
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "Some other links:\n",
    "1. Llama 3.2 Conversational notebook. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)\n",
    "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
    "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
    "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
    "\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
    "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
    "\n",
    "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "https://github.com/unslothai/notebooks/blob/main/nb/Phi_4_(14B)-GRPO.ipynb",
     "timestamp": 1741194453492
    }
   ]
  },
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0b9230e976b34a9ea85978cf22857012": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f8ead1775934dc3a10533b67b3dd905": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "129dc789722b43439574390bba63b36a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b9f8a2a793640d689abc10f5f39c54b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22e0933485c14d94b0c1cfe198d6758f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_43462d5de24b4e55871b3f579798b374",
       "IPY_MODEL_99577e7cbed74c89afb3d44d4fd956c5",
       "IPY_MODEL_d034c840e7f74177a7b07a188d666b8d"
      ],
      "layout": "IPY_MODEL_0f8ead1775934dc3a10533b67b3dd905"
     }
    },
    "4089236deafd4fa2be86d8dc0a29d469": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43462d5de24b4e55871b3f579798b374": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e43ad27d5d304d1ebf9b374016409a97",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_51948945111f437c9ed6ccab22072dd3",
      "value": ""
     }
    },
    "47d2fd7f76754d9fa156576bc0c58abb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c9248100f89400d9e1407dbb168d5d6",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_96cea0d773c8426b8be72dd7f72e5a82",
      "value": ""
     }
    },
    "4c9248100f89400d9e1407dbb168d5d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51948945111f437c9ed6ccab22072dd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "627f68389cf64e2a915a72ab147ee8a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "78cc90a50c0c4636b0f41436a820ecd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "81a0791760de4dcebd543c40d2c1e322": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b9f8a2a793640d689abc10f5f39c54b",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_627f68389cf64e2a915a72ab147ee8a7",
      "value": 2
     }
    },
    "8991360910ef417db03499f76f5fe323": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "96cea0d773c8426b8be72dd7f72e5a82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "99577e7cbed74c89afb3d44d4fd956c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4089236deafd4fa2be86d8dc0a29d469",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_78cc90a50c0c4636b0f41436a820ecd3",
      "value": 2
     }
    },
    "9eed940f3815428583b4ddefc1a81469": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a729c5fc5c764c85885cac7a2d4d95d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9eed940f3815428583b4ddefc1a81469",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_0b9230e976b34a9ea85978cf22857012",
      "value": "Loading‚Äásafetensors‚Äácheckpoint‚Äáshards:‚Äá100%‚ÄáCompleted‚Äá|‚Äá2/2‚Äá[00:51&lt;00:00,‚Äá25.46s/it]\n"
     }
    },
    "d034c840e7f74177a7b07a188d666b8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_129dc789722b43439574390bba63b36a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_8991360910ef417db03499f76f5fe323",
      "value": "Loading‚Äásafetensors‚Äácheckpoint‚Äáshards:‚Äá100%‚ÄáCompleted‚Äá|‚Äá2/2‚Äá[00:47&lt;00:00,‚Äá23.63s/it]\n"
     }
    },
    "d6d5a7d96a034247b38d25d8a9cc979c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e43ad27d5d304d1ebf9b374016409a97": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f57d844b2efa469e8aadd48175ce70ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_47d2fd7f76754d9fa156576bc0c58abb",
       "IPY_MODEL_81a0791760de4dcebd543c40d2c1e322",
       "IPY_MODEL_a729c5fc5c764c85885cac7a2d4d95d0"
      ],
      "layout": "IPY_MODEL_d6d5a7d96a034247b38d25d8a9cc979c"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
